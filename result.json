{"cupti_version": 20, "cuda_runtime_version": 12020, "cuda_driver_version": 12020, "trace_id": "11BF71FFEE0D4AEBAFC693D8645955B9", "traceEvents": [{"ph": "X", "cat": "cuda_runtime", "name": "cudaGetDeviceCount", "pid": 1867968, "tid": 245634880, "ts": 2254831886652.81, "dur": 2386.988, "args": {"cbid": 3, "correlation": 1}}, {"ph": "f", "id": 1, "pid": 1867968, "tid": 245634880, "ts": 2254831886652.81, "cat": "ac2g", "name": "ac2g", "bp": "e"}, {"ph": "X", "cat": "cuda_runtime", "name": "cudaGetDeviceCount", "pid": 1867968, "tid": 245634880, "ts": 2254831889087.119, "dur": 0.191, "args": {"cbid": 3, "correlation": 3}}, {"ph": "f", "id": 3, "pid": 1867968, "tid": 245634880, "ts": 2254831889087.119, "cat": "ac2g", "name": "ac2g", "bp": "e"}, {"ph": "X", "cat": "cuda_runtime", "name": "cudaGetDeviceProperties_v2", "pid": 1867968, "tid": 245634880, "ts": 2254831942196.673, "dur": 253.722, "args": {"cbid": 440, "correlation": 15}}, {"ph": "f", "id": 15, "pid": 1867968, "tid": 245634880, "ts": 2254831942196.673, "cat": "ac2g", "name": "ac2g", "bp": "e"}, {"ph": "X", "cat": "cuda_runtime", "name": "cudaGetDeviceProperties_v2", "pid": 1867968, "tid": 245634880, "ts": 2254831942606.353, "dur": 224.222, "args": {"cbid": 440, "correlation": 16}}, {"ph": "f", "id": 16, "pid": 1867968, "tid": 245634880, "ts": 2254831942606.353, "cat": "ac2g", "name": "ac2g", "bp": "e"}, {"ph": "X", "cat": "overhead", "name": "Instrumentation", "pid": -1, "tid": 0, "ts": 2254832039366.64, "dur": 43.332}, {"ph": "X", "cat": "overhead", "name": "Instrumentation", "pid": -1, "tid": 0, "ts": 2254832039457.389, "dur": 31.063}, {"ph": "X", "cat": "overhead", "name": "Resource", "pid": -1, "tid": 0, "ts": 2254832039488.584, "dur": 7.305}, {"ph": "X", "cat": "overhead", "name": "Resource", "pid": -1, "tid": 0, "ts": 2254832039527.564, "dur": 2764.229}, {"ph": "X", "cat": "overhead", "name": "Resource", "pid": -1, "tid": 0, "ts": 2254832042293.857, "dur": 2500.519}, {"ph": "X", "cat": "overhead", "name": "Resource", "pid": -1, "tid": 0, "ts": 2254832044795.984, "dur": 2647.387}, {"ph": "X", "cat": "cuda_runtime", "name": "cudaDeviceSynchronize", "pid": 1867968, "tid": 245634880, "ts": 2254831943397.4, "dur": 117725.517, "args": {"cbid": 165, "correlation": 19}}, {"ph": "s", "id": 19, "pid": 1867968, "tid": 245634880, "ts": 2254831943397.4, "cat": "ac2g", "name": "ac2g"}, {"name": "process_name", "ph": "M", "pid": 1867968, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 1867968, "tid": 0, "args": {"labels": "CPU"}}, {"name": "process_sort_index", "ph": "M", "pid": 1867968, "tid": 0, "args": {"sort_index": 1867968}}, {"name": "process_name", "ph": "M", "pid": 0, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 0, "tid": 0, "args": {"labels": "GPU 0"}}, {"name": "process_sort_index", "ph": "M", "pid": 0, "tid": 0, "args": {"sort_index": 5000000}}, {"name": "process_name", "ph": "M", "pid": 1, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 1, "tid": 0, "args": {"labels": "GPU 1"}}, {"name": "process_sort_index", "ph": "M", "pid": 1, "tid": 0, "args": {"sort_index": 5000001}}, {"name": "process_name", "ph": "M", "pid": 2, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 2, "tid": 0, "args": {"labels": "GPU 2"}}, {"name": "process_sort_index", "ph": "M", "pid": 2, "tid": 0, "args": {"sort_index": 5000002}}, {"name": "process_name", "ph": "M", "pid": 3, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 3, "tid": 0, "args": {"labels": "GPU 3"}}, {"name": "process_sort_index", "ph": "M", "pid": 3, "tid": 0, "args": {"sort_index": 5000003}}, {"name": "process_name", "ph": "M", "pid": 4, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 4, "tid": 0, "args": {"labels": "GPU 4"}}, {"name": "process_sort_index", "ph": "M", "pid": 4, "tid": 0, "args": {"sort_index": 5000004}}, {"name": "process_name", "ph": "M", "pid": 5, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 5, "tid": 0, "args": {"labels": "GPU 5"}}, {"name": "process_sort_index", "ph": "M", "pid": 5, "tid": 0, "args": {"sort_index": 5000005}}, {"name": "process_name", "ph": "M", "pid": 6, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 6, "tid": 0, "args": {"labels": "GPU 6"}}, {"name": "process_sort_index", "ph": "M", "pid": 6, "tid": 0, "args": {"sort_index": 5000006}}, {"name": "process_name", "ph": "M", "pid": 7, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 7, "tid": 0, "args": {"labels": "GPU 7"}}, {"name": "process_sort_index", "ph": "M", "pid": 7, "tid": 0, "args": {"sort_index": 5000007}}, {"name": "process_name", "ph": "M", "pid": 8, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 8, "tid": 0, "args": {"labels": "GPU 8"}}, {"name": "process_sort_index", "ph": "M", "pid": 8, "tid": 0, "args": {"sort_index": 5000008}}, {"name": "process_name", "ph": "M", "pid": 9, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 9, "tid": 0, "args": {"labels": "GPU 9"}}, {"name": "process_sort_index", "ph": "M", "pid": 9, "tid": 0, "args": {"sort_index": 5000009}}, {"name": "process_name", "ph": "M", "pid": 10, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 10, "tid": 0, "args": {"labels": "GPU 10"}}, {"name": "process_sort_index", "ph": "M", "pid": 10, "tid": 0, "args": {"sort_index": 5000010}}, {"name": "process_name", "ph": "M", "pid": 11, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 11, "tid": 0, "args": {"labels": "GPU 11"}}, {"name": "process_sort_index", "ph": "M", "pid": 11, "tid": 0, "args": {"sort_index": 5000011}}, {"name": "process_name", "ph": "M", "pid": 12, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 12, "tid": 0, "args": {"labels": "GPU 12"}}, {"name": "process_sort_index", "ph": "M", "pid": 12, "tid": 0, "args": {"sort_index": 5000012}}, {"name": "process_name", "ph": "M", "pid": 13, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 13, "tid": 0, "args": {"labels": "GPU 13"}}, {"name": "process_sort_index", "ph": "M", "pid": 13, "tid": 0, "args": {"sort_index": 5000013}}, {"name": "process_name", "ph": "M", "pid": 14, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 14, "tid": 0, "args": {"labels": "GPU 14"}}, {"name": "process_sort_index", "ph": "M", "pid": 14, "tid": 0, "args": {"sort_index": 5000014}}, {"name": "process_name", "ph": "M", "pid": 15, "tid": 0, "args": {"name": "python3.12"}}, {"name": "process_labels", "ph": "M", "pid": 15, "tid": 0, "args": {"labels": "GPU 15"}}, {"name": "process_sort_index", "ph": "M", "pid": 15, "tid": 0, "args": {"sort_index": 5000015}}, {"ph": "X", "cat": "Trace", "ts": 2254831875220.162, "dur": 185961.481, "pid": "Spans", "tid": "PyTorch Profiler", "name": "PyTorch Profiler (0)", "args": {"Op count": 0}}, {"name": "process_sort_index", "ph": "M", "pid": "Spans", "tid": 0, "args": {"sort_index": 536870912}}, {"name": "Iteration Start: PyTorch Profiler", "ph": "i", "s": "g", "pid": "Traces", "tid": "Trace PyTorch Profiler", "ts": 2254831875220.162}, {"name": "Record Window End", "ph": "i", "s": "g", "pid": "", "tid": "", "ts": 2254832061291.981}, {"ph": "M", "pid": 1867968, "tid": 1867968, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 1867968, "tid": 1867968, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875353.318, "ph": "X", "dur": 4.586945657705259, "name": "<module> (<string>:1)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875349.674, "ph": "X", "dur": 11.335625325847538, "name": "builtins.exec", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875386.314, "ph": "X", "dur": 1.5895614525659174, "name": "dict.get", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875390.464, "ph": "X", "dur": 0.5730421210289603, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875385.473, "ph": "X", "dur": 6.9504183207655235, "name": "finalize.detach (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:592)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875406.082, "ph": "X", "dur": 1.5565180525672822, "name": "sys.audit", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875408.966, "ph": "X", "dur": 0.5947811999754307, "name": "builtins.isinstance", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875415.471, "ph": "X", "dur": 0.29999928946129184, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875417.983, "ph": "X", "dur": 11.6869288416225, "name": "posix.lstat", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875431.196, "ph": "X", "dur": 10.834756946920857, "name": "posix.open", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875444.591, "ph": "X", "dur": 2.087821142019019, "name": "posix.fstat", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875448.374, "ph": "X", "dur": 1.2947795420517785, "name": "samestat (<frozen genericpath>:99)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875450.666, "ph": "X", "dur": 0.32173836840776227, "name": "list.append", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875451.426, "ph": "X", "dur": 0.14782573683599887, "name": "list.append", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875452.651, "ph": "X", "dur": 6.213028762901247, "name": "posix.scandir", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875492.681, "ph": "X", "dur": 0.23043423683258646, "name": "posix.ScandirIterator.__exit__", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875497.975, "ph": "X", "dur": 0.19999952630752788, "name": "posix.fspath", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875499.713, "ph": "X", "dur": 0.24260812104260993, "name": "builtins.isinstance", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875499.516, "ph": "X", "dur": 0.621737657869054, "name": "_get_sep (<frozen posixpath>:41)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875501.21, "ph": "X", "dur": 0.7904329104936645, "name": "str.startswith", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875502.415, "ph": "X", "dur": 0.45652065787587887, "name": "str.endswith", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875497.677, "ph": "X", "dur": 6.13042026290466, "name": "join (<frozen posixpath>:71)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875505.057, "ph": "X", "dur": 0.8565197104909346, "name": "posix.DirEntry.is_dir", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875506.909, "ph": "X", "dur": 55.484216413497535, "name": "posix.unlink", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875563.995, "ph": "X", "dur": 0.113912773679505, "name": "posix.fspath", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875564.486, "ph": "X", "dur": 0.1556518052567282, "name": "builtins.isinstance", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875564.359, "ph": "X", "dur": 0.39739036314147935, "name": "_get_sep (<frozen posixpath>:41)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875565.289, "ph": "X", "dur": 0.2573906947262098, "name": "str.startswith", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875565.753, "ph": "X", "dur": 0.22173860525399833, "name": "str.endswith", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875563.871, "ph": "X", "dur": 2.7139066156773675, "name": "join (<frozen posixpath>:71)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875567.013, "ph": "X", "dur": 0.2982601631455742, "name": "posix.DirEntry.is_dir", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875567.833, "ph": "X", "dur": 0.14695617367814007, "name": "list.append", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875415.067, "ph": "X", "dur": 153.14833292525267, "name": "_rmtree_safe_fd (/root/miniconda3/envs/gs-lightning/lib/python3.12/shutil.py:642)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875570.78, "ph": "X", "dur": 0.19478214736037497, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875572.48, "ph": "X", "dur": 4.706945373489776, "name": "posix.DirEntry.stat", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875577.94, "ph": "X", "dur": 5.694769120817392, "name": "posix.open", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875584.101, "ph": "X", "dur": 1.3504315841547427, "name": "posix.fstat", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875585.743, "ph": "X", "dur": 0.669563631551289, "name": "samestat (<frozen genericpath>:99)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875587.026, "ph": "X", "dur": 0.09913019999590512, "name": "list.append", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875587.423, "ph": "X", "dur": 0.08782587894374051, "name": "list.append", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875588.109, "ph": "X", "dur": 2.326951010430194, "name": "posix.scandir", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875599.525, "ph": "X", "dur": 0.2156516631489866, "name": "posix.ScandirIterator.__exit__", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875600.663, "ph": "X", "dur": 0.09217369473303459, "name": "posix.fspath", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875601.086, "ph": "X", "dur": 0.11913015262665792, "name": "builtins.isinstance", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875601.007, "ph": "X", "dur": 0.33999919472279744, "name": "_get_sep (<frozen posixpath>:41)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875601.785, "ph": "X", "dur": 0.21391253683326894, "name": "str.startswith", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875602.243, "ph": "X", "dur": 0.17565175788748102, "name": "str.endswith", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875600.584, "ph": "X", "dur": 5.342596041884571, "name": "join (<frozen posixpath>:71)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875606.229, "ph": "X", "dur": 0.24347768420046872, "name": "posix.DirEntry.is_dir", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875606.912, "ph": "X", "dur": 17.827783862421462, "name": "posix.unlink", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875570.525, "ph": "X", "dur": 54.472914460907724, "name": "_rmtree_safe_fd (/root/miniconda3/envs/gs-lightning/lib/python3.12/shutil.py:642)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875626.009, "ph": "X", "dur": 0.18434738946606918, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875626.789, "ph": "X", "dur": 2.8947757525120013, "name": "posix.close", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875625.866, "ph": "X", "dur": 3.940860231416158, "name": "_rmtree_safe_fd (/root/miniconda3/envs/gs-lightning/lib/python3.12/shutil.py:642)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875630.286, "ph": "X", "dur": 0.07478243157585826, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875630.735, "ph": "X", "dur": 26.50950243048389, "name": "posix.rmdir", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875630.219, "ph": "X", "dur": 27.18689213045591, "name": "_rmtree_safe_fd (/root/miniconda3/envs/gs-lightning/lib/python3.12/shutil.py:642)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875658.389, "ph": "X", "dur": 0.14434748420456361, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875659.014, "ph": "X", "dur": 1.4608661052028125, "name": "posix.close", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875658.307, "ph": "X", "dur": 2.2339077525393005, "name": "_rmtree_safe_fd (/root/miniconda3/envs/gs-lightning/lib/python3.12/shutil.py:642)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875660.855, "ph": "X", "dur": 0.2052169052546808, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875661.412, "ph": "X", "dur": 24.93994093054873, "name": "posix.rmdir", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875660.785, "ph": "X", "dur": 25.68863480946517, "name": "_rmtree_safe_fd (/root/miniconda3/envs/gs-lightning/lib/python3.12/shutil.py:642)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875404.993, "ph": "X", "dur": 281.7358544567831, "name": "rmtree (/root/miniconda3/envs/gs-lightning/lib/python3.12/shutil.py:710)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875398.204, "ph": "X", "dur": 289.2219236827897, "name": "TemporaryDirectory._rmtree (/root/miniconda3/envs/gs-lightning/lib/python3.12/tempfile.py:894)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875381.361, "ph": "X", "dur": 307.23492449783504, "name": "TemporaryDirectory.cleanup (/root/miniconda3/envs/gs-lightning/lib/python3.12/tempfile.py:952)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875695.227, "ph": "X", "dur": 0.4252163841929615, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875702.296, "ph": "X", "dur": 0.4373902684029849, "name": "str.splitlines", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875701.938, "ph": "X", "dur": 1.063475742061333, "name": "indent.<locals>.prefixed_lines (/root/miniconda3/envs/gs-lightning/lib/python3.12/textwrap.py:482)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875700.6, "ph": "X", "dur": 3.0469493051372942, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875698.436, "ph": "X", "dur": 5.397378520829677, "name": "indent (/root/miniconda3/envs/gs-lightning/lib/python3.12/textwrap.py:470)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875712.063, "ph": "X", "dur": 0.6226072210269128, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875711.707, "ph": "X", "dur": 1.1356494841636149, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875713.924, "ph": "X", "dur": 0.09565194736446986, "name": "Manager.disable (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1369)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875715.108, "ph": "X", "dur": 1.034780157851992, "name": "Logger.getEffectiveLevel (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1776)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875717.326, "ph": "X", "dur": 0.2008690894653867, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875717.183, "ph": "X", "dur": 0.44173808419227895, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875709.421, "ph": "X", "dur": 8.575631862803652, "name": "Logger.isEnabledFor (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1790)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875707.201, "ph": "X", "dur": 11.145190994276456, "name": "Logger.info (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1529)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875690.904, "ph": "X", "dur": 27.779064640957763, "name": "_log_traced_frames (/data/wangyingqi/code/pytorch/torch/_dynamo/eval_frame.py:551)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875731.775, "ph": "X", "dur": 0.21739078946470422, "name": "dict.get", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875733.663, "ph": "X", "dur": 0.1956517105182338, "name": "_ModuleLockManager.__init__ (<frozen importlib._bootstrap>:412)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875736.624, "ph": "X", "dur": 0.22260816841185713, "name": "_imp.acquire_lock", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875740.222, "ph": "X", "dur": 0.4495641526130083, "name": "_thread.allocate_lock", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875739.386, "ph": "X", "dur": 1.599996210460223, "name": "_ModuleLock.__init__ (<frozen importlib._bootstrap>:232)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875742.718, "ph": "X", "dur": 0.17130394209818692, "name": "_imp.release_lock", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875736.441, "ph": "X", "dur": 6.566071404991926, "name": "_get_module_lock (<frozen importlib._bootstrap>:426)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875744.949, "ph": "X", "dur": 0.14782573683599887, "name": "_thread.get_ident", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875746.152, "ph": "X", "dur": 0.11478233683736383, "name": "_BlockingOnManager.__init__ (<frozen importlib._bootstrap>:158)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875752.164, "ph": "X", "dur": 0.4278250736665379, "name": "type.__new__", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875750.747, "ph": "X", "dur": 2.0460821104417963, "name": "_WeakValueDictionary.__init__.<locals>.KeyedRef.__new__ (<frozen importlib._bootstrap>:74)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875753.509, "ph": "X", "dur": 1.899125936763656, "name": "_WeakValueDictionary.__init__.<locals>.KeyedRef.__init__ (<frozen importlib._bootstrap>:79)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875748.77, "ph": "X", "dur": 7.007809489184206, "name": "_WeakValueDictionary.setdefault (<frozen importlib._bootstrap>:124)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875756.219, "ph": "X", "dur": 0.2156516631489866, "name": "_List.append", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875747.347, "ph": "X", "dur": 12.343449025805905, "name": "_BlockingOnManager.__enter__ (<frozen importlib._bootstrap>:162)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875760.769, "ph": "X", "dur": 0.15217355262529295, "name": "list.append", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875761.099, "ph": "X", "dur": 0.21304297367541014, "name": "_thread.RLock.__exit__", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875762.587, "ph": "X", "dur": 0.36347739998498546, "name": "_List.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875762.31, "ph": "X", "dur": 0.7678242683893353, "name": "_BlockingOnManager.__exit__ (<frozen importlib._bootstrap>:173)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875765.001, "ph": "X", "dur": 0.4660858526123258, "name": "_weakref._remove_dead_weakref", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875764.459, "ph": "X", "dur": 1.1791276420565557, "name": "_WeakValueDictionary.__init__.<locals>.KeyedRef.remove (<frozen importlib._bootstrap>:82)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875744.86, "ph": "X", "dur": 21.405166693852635, "name": "_ModuleLock.acquire (<frozen importlib._bootstrap>:304)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875735.15, "ph": "X", "dur": 31.239056446077996, "name": "_ModuleLockManager.__enter__ (<frozen importlib._bootstrap>:416)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875766.684, "ph": "X", "dur": 0.23999943156903344, "name": "dict.get", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875769.807, "ph": "X", "dur": 0.4260859473508203, "name": "str.rpartition", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875774.883, "ph": "X", "dur": 0.12782578420524607, "name": "_imp.acquire_lock", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875774.8, "ph": "X", "dur": 0.3139122999870329, "name": "_ImportLockContext.__enter__ (<frozen importlib._bootstrap>:1222)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875778.023, "ph": "X", "dur": 1.6739090788782225, "name": "builtins.locals", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875780.07, "ph": "X", "dur": 2.047821236757514, "name": "str.format", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875783.4, "ph": "X", "dur": 0.49825968945310206, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875784.524, "ph": "X", "dur": 0.07043461578656417, "name": "DistutilsMetaFinder.find_spec.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/site-packages/_distutils_hack/__init__.py:108)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875777.348, "ph": "X", "dur": 7.381721647063497, "name": "DistutilsMetaFinder.find_spec (/root/miniconda3/envs/gs-lightning/lib/python3.12/site-packages/_distutils_hack/__init__.py:101)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875785.708, "ph": "X", "dur": 0.1434779210467048, "name": "_imp.release_lock", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875785.635, "ph": "X", "dur": 0.30173841577700944, "name": "_ImportLockContext.__exit__ (<frozen importlib._bootstrap>:1226)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875786.824, "ph": "X", "dur": 0.09652151052232867, "name": "_imp.acquire_lock", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875786.771, "ph": "X", "dur": 0.2113038473596925, "name": "_ImportLockContext.__enter__ (<frozen importlib._bootstrap>:1222)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875788.744, "ph": "X", "dur": 0.8573892736487934, "name": "_imp.is_builtin", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875788.583, "ph": "X", "dur": 1.1704320104779675, "name": "BuiltinImporter.find_spec (<frozen importlib._bootstrap>:982)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875790.136, "ph": "X", "dur": 0.0773911210494347, "name": "_imp.release_lock", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875790.084, "ph": "X", "dur": 0.18260826315035156, "name": "_ImportLockContext.__exit__ (<frozen importlib._bootstrap>:1226)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875791.126, "ph": "X", "dur": 0.0826084999965876, "name": "_imp.acquire_lock", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875791.072, "ph": "X", "dur": 0.19391258420251617, "name": "_ImportLockContext.__enter__ (<frozen importlib._bootstrap>:1222)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875793.842, "ph": "X", "dur": 0.7182591683913828, "name": "_imp.find_frozen", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875793.667, "ph": "X", "dur": 1.0078236999583687, "name": "_call_with_frames_removed (<frozen importlib._bootstrap>:480)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875792.958, "ph": "X", "dur": 1.9626040472873496, "name": "FrozenImporter.find_spec (<frozen importlib._bootstrap>:1128)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875795.236, "ph": "X", "dur": 0.07826068420729353, "name": "_imp.release_lock", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875795.185, "ph": "X", "dur": 0.17739088420319865, "name": "_ImportLockContext.__exit__ (<frozen importlib._bootstrap>:1226)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875795.86, "ph": "X", "dur": 0.08347806315444642, "name": "_imp.acquire_lock", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875795.809, "ph": "X", "dur": 0.18347782630821036, "name": "_ImportLockContext.__enter__ (<frozen importlib._bootstrap>:1222)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875800.152, "ph": "X", "dur": 0.10608670525877567, "name": "builtins.isinstance", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875803.541, "ph": "X", "dur": 0.4539119684023024, "name": "PathFinder._path_importer_cache (<frozen importlib._bootstrap_external>:1473)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875807.342, "ph": "X", "dur": 0.22521685788543358, "name": "str.rpartition", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875809.246, "ph": "X", "dur": 5.2452049682043835, "name": "posix.stat", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875808.512, "ph": "X", "dur": 6.108681183958189, "name": "_path_stat (<frozen importlib._bootstrap_external>:140)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875815.819, "ph": "X", "dur": 0.07391286841799943, "name": "_make_relax_case.<locals>._relax_case (<frozen importlib._bootstrap_external>:71)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875818.827, "ph": "X", "dur": 0.2669558894626568, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875819.397, "ph": "X", "dur": 0.1747821947296222, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875819.794, "ph": "X", "dur": 0.28086889998839787, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875818.349, "ph": "X", "dur": 1.9695605525502204, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875821.569, "ph": "X", "dur": 0.34521657366995034, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875824.812, "ph": "X", "dur": 0.10695626841663447, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875825.055, "ph": "X", "dur": 0.09826063683804631, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875825.289, "ph": "X", "dur": 0.18869520525536326, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875824.58, "ph": "X", "dur": 0.9965193789062041, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875825.941, "ph": "X", "dur": 0.12347796841595199, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875826.873, "ph": "X", "dur": 0.09739107368018748, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875827.124, "ph": "X", "dur": 0.09043456841731695, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875827.34, "ph": "X", "dur": 0.1599996210460223, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875826.709, "ph": "X", "dur": 0.9008674315417343, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875827.858, "ph": "X", "dur": 0.11043452104806974, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875828.732, "ph": "X", "dur": 0.08173893683872879, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875828.96, "ph": "X", "dur": 0.09130413157517578, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875829.189, "ph": "X", "dur": 0.16434743683531639, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875828.59, "ph": "X", "dur": 0.8556501473330758, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875829.668, "ph": "X", "dur": 0.0930432578908934, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875830.495, "ph": "X", "dur": 0.08695631578588169, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875830.727, "ph": "X", "dur": 0.09478238420661105, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875830.947, "ph": "X", "dur": 0.15304311578315177, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875830.382, "ph": "X", "dur": 0.816519805229429, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875831.418, "ph": "X", "dur": 0.09478238420661105, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875807.202, "ph": "X", "dur": 24.588637414773768, "name": "FileFinder.find_spec (<frozen importlib._bootstrap_external>:1597)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875832.28, "ph": "X", "dur": 0.07826068420729353, "name": "builtins.isinstance", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875832.706, "ph": "X", "dur": 0.26521676314693915, "name": "PathFinder._path_importer_cache (<frozen importlib._bootstrap_external>:1473)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875833.332, "ph": "X", "dur": 0.05826073157654073, "name": "builtins.isinstance", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875833.572, "ph": "X", "dur": 0.726085236812112, "name": "PathFinder._path_importer_cache (<frozen importlib._bootstrap_external>:1473)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875834.753, "ph": "X", "dur": 0.18434738946606918, "name": "str.rpartition", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875835.48, "ph": "X", "dur": 3.495643894592444, "name": "posix.stat", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875835.395, "ph": "X", "dur": 3.7017303630049834, "name": "_path_stat (<frozen importlib._bootstrap_external>:140)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875839.615, "ph": "X", "dur": 0.05826073157654073, "name": "_make_relax_case.<locals>._relax_case (<frozen importlib._bootstrap_external>:71)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875840.6, "ph": "X", "dur": 0.09999976315376394, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875840.871, "ph": "X", "dur": 0.1034780157851992, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875841.11, "ph": "X", "dur": 0.19826039999181025, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875840.414, "ph": "X", "dur": 0.9826063683804631, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875841.67, "ph": "X", "dur": 0.11304321052164619, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875842.468, "ph": "X", "dur": 0.08347806315444642, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875842.699, "ph": "X", "dur": 0.09043456841731695, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875842.911, "ph": "X", "dur": 0.14695617367814007, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875842.327, "ph": "X", "dur": 0.8191284947030054, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875843.367, "ph": "X", "dur": 0.08956500525945814, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875844.123, "ph": "X", "dur": 0.08434762631230523, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875844.354, "ph": "X", "dur": 0.09043456841731695, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875844.568, "ph": "X", "dur": 0.15826049473030468, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875843.995, "ph": "X", "dur": 0.8313023789130289, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875845.062, "ph": "X", "dur": 0.0930432578908934, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875845.728, "ph": "X", "dur": 0.08521718947016406, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875845.959, "ph": "X", "dur": 0.0930432578908934, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875846.174, "ph": "X", "dur": 0.14434748420456361, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875845.601, "ph": "X", "dur": 2.6817327788365914, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875848.532, "ph": "X", "dur": 0.09999976315376394, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875849.256, "ph": "X", "dur": 0.08434762631230523, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875849.486, "ph": "X", "dur": 0.09043456841731695, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875849.7, "ph": "X", "dur": 0.12608665788952844, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875849.131, "ph": "X", "dur": 0.7817372789150764, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875850.132, "ph": "X", "dur": 0.09478238420661105, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875834.641, "ph": "X", "dur": 15.852136367766231, "name": "FileFinder.find_spec (<frozen importlib._bootstrap_external>:1597)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875850.795, "ph": "X", "dur": 0.07043461578656417, "name": "builtins.isinstance", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875851.088, "ph": "X", "dur": 0.25999938419978624, "name": "PathFinder._path_importer_cache (<frozen importlib._bootstrap_external>:1473)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875851.774, "ph": "X", "dur": 0.1695648157824693, "name": "str.rpartition", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875852.364, "ph": "X", "dur": 2.3765161104281463, "name": "posix.stat", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875852.259, "ph": "X", "dur": 2.5565156841049217, "name": "_path_stat (<frozen importlib._bootstrap_external>:140)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875855.274, "ph": "X", "dur": 0.06347811052369363, "name": "_make_relax_case.<locals>._relax_case (<frozen importlib._bootstrap_external>:71)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875856.095, "ph": "X", "dur": 0.09217369473303459, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875856.338, "ph": "X", "dur": 0.08869544210159933, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875856.552, "ph": "X", "dur": 0.1339127263102578, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875855.965, "ph": "X", "dur": 0.8208676210187232, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875857.022, "ph": "X", "dur": 0.11739102631094028, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875857.808, "ph": "X", "dur": 0.08608675262802286, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875858.036, "ph": "X", "dur": 0.09043456841731695, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875858.252, "ph": "X", "dur": 0.15043442630957532, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875857.682, "ph": "X", "dur": 0.8086937368086997, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875858.731, "ph": "X", "dur": 0.09739107368018748, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875859.455, "ph": "X", "dur": 0.08521718947016406, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875859.679, "ph": "X", "dur": 0.08782587894374051, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875859.888, "ph": "X", "dur": 0.1556518052567282, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875859.328, "ph": "X", "dur": 0.8017372315458292, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875860.318, "ph": "X", "dur": 0.09913019999590512, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875860.961, "ph": "X", "dur": 0.08608675262802286, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875861.188, "ph": "X", "dur": 0.08956500525945814, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875861.4, "ph": "X", "dur": 0.12347796841595199, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875860.818, "ph": "X", "dur": 0.7921720368093822, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875861.8, "ph": "X", "dur": 0.09739107368018748, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875862.442, "ph": "X", "dur": 0.0826084999965876, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875862.667, "ph": "X", "dur": 0.09130413157517578, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875862.881, "ph": "X", "dur": 0.12695622104738727, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875862.316, "ph": "X", "dur": 0.7799981525993587, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875863.284, "ph": "X", "dur": 0.09826063683804631, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875851.655, "ph": "X", "dur": 11.964319488979461, "name": "FileFinder.find_spec (<frozen importlib._bootstrap_external>:1597)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875863.848, "ph": "X", "dur": 0.06956505262870534, "name": "builtins.isinstance", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875864.115, "ph": "X", "dur": 0.2052169052546808, "name": "PathFinder._path_importer_cache (<frozen importlib._bootstrap_external>:1473)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875864.571, "ph": "X", "dur": 0.16869525262461046, "name": "str.rpartition", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875865.088, "ph": "X", "dur": 2.4765158735819104, "name": "posix.stat", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875865.01, "ph": "X", "dur": 2.6130372893657445, "name": "_path_stat (<frozen importlib._bootstrap_external>:140)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875868.151, "ph": "X", "dur": 0.06869548947084654, "name": "_make_relax_case.<locals>._relax_case (<frozen importlib._bootstrap_external>:71)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875870.735, "ph": "X", "dur": 0.10608670525877567, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875870.989, "ph": "X", "dur": 0.09043456841731695, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875871.207, "ph": "X", "dur": 0.13652141578383425, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875870.601, "ph": "X", "dur": 0.8278241262815936, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875871.644, "ph": "X", "dur": 0.11565189999522264, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875872.401, "ph": "X", "dur": 0.08347806315444642, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875872.635, "ph": "X", "dur": 0.08956500525945814, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875872.852, "ph": "X", "dur": 0.1339127263102578, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875872.273, "ph": "X", "dur": 0.8017372315458292, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875873.267, "ph": "X", "dur": 0.09913019999590512, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875873.969, "ph": "X", "dur": 0.08173893683872879, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875874.196, "ph": "X", "dur": 0.08695631578588169, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875874.408, "ph": "X", "dur": 0.12173884210023436, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875873.841, "ph": "X", "dur": 0.7747807736522059, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875874.806, "ph": "X", "dur": 0.09391282104875222, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875875.408, "ph": "X", "dur": 0.08434762631230523, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875875.641, "ph": "X", "dur": 0.09043456841731695, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875875.855, "ph": "X", "dur": 0.12695622104738727, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875875.28, "ph": "X", "dur": 0.7913024736515233, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875876.263, "ph": "X", "dur": 0.09913019999590512, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875876.894, "ph": "X", "dur": 0.08434762631230523, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875877.125, "ph": "X", "dur": 0.09130413157517578, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875877.341, "ph": "X", "dur": 0.12521709473166964, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875876.766, "ph": "X", "dur": 0.7886937841779469, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875877.751, "ph": "X", "dur": 0.09391282104875222, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875864.476, "ph": "X", "dur": 13.616489488911213, "name": "FileFinder.find_spec (<frozen importlib._bootstrap_external>:1597)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875878.229, "ph": "X", "dur": 0.07130417894442298, "name": "builtins.isinstance", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875878.516, "ph": "X", "dur": 0.18260826315035156, "name": "PathFinder._path_importer_cache (<frozen importlib._bootstrap_external>:1473)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875878.937, "ph": "X", "dur": 0.16695612630889284, "name": "str.rpartition", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875879.42, "ph": "X", "dur": 3.1226012998710115, "name": "posix.stat", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875879.344, "ph": "X", "dur": 3.279992231443457, "name": "_path_stat (<frozen importlib._bootstrap_external>:140)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875883.039, "ph": "X", "dur": 0.061738984207975994, "name": "_make_relax_case.<locals>._relax_case (<frozen importlib._bootstrap_external>:71)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875883.689, "ph": "X", "dur": 0.11304321052164619, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875883.95, "ph": "X", "dur": 0.08956500525945814, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875884.166, "ph": "X", "dur": 0.2069560315703984, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875883.559, "ph": "X", "dur": 0.9017369946995931, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875884.695, "ph": "X", "dur": 0.11565189999522264, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875885.433, "ph": "X", "dur": 0.09478238420661105, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875885.671, "ph": "X", "dur": 0.08956500525945814, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875885.884, "ph": "X", "dur": 0.2165212263068454, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875885.309, "ph": "X", "dur": 0.8791283525952639, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875886.402, "ph": "X", "dur": 0.09043456841731695, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875887.212, "ph": "X", "dur": 0.08608675262802286, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875887.441, "ph": "X", "dur": 0.09043456841731695, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875887.654, "ph": "X", "dur": 0.1860865157817868, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875887.084, "ph": "X", "dur": 0.8460849525966287, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875888.118, "ph": "X", "dur": 0.09043456841731695, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875890.422, "ph": "X", "dur": 0.08347806315444642, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875890.649, "ph": "X", "dur": 0.09043456841731695, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875890.864, "ph": "X", "dur": 0.13565185262597543, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875890.294, "ph": "X", "dur": 0.802606794703688, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875891.294, "ph": "X", "dur": 0.10608670525877567, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875891.921, "ph": "X", "dur": 0.08173893683872879, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875892.146, "ph": "X", "dur": 0.09043456841731695, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875892.36, "ph": "X", "dur": 0.11478233683736383, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875891.795, "ph": "X", "dur": 0.7669547052314765, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875892.755, "ph": "X", "dur": 0.08782587894374051, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875878.84, "ph": "X", "dur": 14.166922967835845, "name": "FileFinder.find_spec (<frozen importlib._bootstrap_external>:1597)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875893.22, "ph": "X", "dur": 0.06086942105011718, "name": "builtins.isinstance", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875893.489, "ph": "X", "dur": 0.5773899368182545, "name": "PathFinder._path_importer_cache (<frozen importlib._bootstrap_external>:1473)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875894.373, "ph": "X", "dur": 0.16782568946675167, "name": "str.rpartition", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875894.847, "ph": "X", "dur": 17.693871136111206, "name": "posix.stat", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875894.768, "ph": "X", "dur": 18.09213106241054, "name": "_path_stat (<frozen importlib._bootstrap_external>:140)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875917.92, "ph": "X", "dur": 4.506945847182248, "name": "posix.listdir", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875924.855, "ph": "X", "dur": 0.3669556526164207, "name": "str.startswith", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875927.002, "ph": "X", "dur": 0.38956429472074994, "name": "str.startswith", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875917.667, "ph": "X", "dur": 9.970411168009194, "name": "FileFinder._fill_cache (<frozen importlib._bootstrap_external>:1648)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875928.094, "ph": "X", "dur": 0.05826073157654073, "name": "_make_relax_case.<locals>._relax_case (<frozen importlib._bootstrap_external>:71)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875929.053, "ph": "X", "dur": 0.12782578420524607, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875929.413, "ph": "X", "dur": 0.10173888946948158, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875929.705, "ph": "X", "dur": 0.21999947893828067, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875928.858, "ph": "X", "dur": 1.2121710420551908, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875930.341, "ph": "X", "dur": 0.11304321052164619, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875931.088, "ph": "X", "dur": 0.09391282104875222, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875931.342, "ph": "X", "dur": 0.09999976315376394, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875931.587, "ph": "X", "dur": 0.15652136841458705, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875930.926, "ph": "X", "dur": 0.9513020946975456, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875932.148, "ph": "X", "dur": 0.09478238420661105, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875932.846, "ph": "X", "dur": 0.09043456841731695, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875933.081, "ph": "X", "dur": 0.08782587894374051, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875933.293, "ph": "X", "dur": 0.14695617367814007, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875932.7, "ph": "X", "dur": 0.8495632052280642, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875933.8, "ph": "X", "dur": 0.09565194736446986, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875934.445, "ph": "X", "dur": 0.08347806315444642, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875934.673, "ph": "X", "dur": 0.09130413157517578, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875934.89, "ph": "X", "dur": 0.12608665788952844, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875934.302, "ph": "X", "dur": 0.802606794703688, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875935.301, "ph": "X", "dur": 0.09652151052232867, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875935.95, "ph": "X", "dur": 0.08521718947016406, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875936.179, "ph": "X", "dur": 0.08782587894374051, "name": "str.rstrip", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875936.39, "ph": "X", "dur": 0.1286953473631049, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875935.825, "ph": "X", "dur": 0.7808677157572175, "name": "_path_join (<frozen importlib._bootstrap_external>:126)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875936.8, "ph": "X", "dur": 0.0930432578908934, "name": "_verbose_message (<frozen importlib._bootstrap>:491)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875894.28, "ph": "X", "dur": 44.901632782355726, "name": "FileFinder.find_spec (<frozen importlib._bootstrap_external>:1597)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875940.993, "ph": "X", "dur": 0.3347818157756445, "name": "ModuleSpec.__init__ (<frozen importlib._bootstrap>:599)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875799.821, "ph": "X", "dur": 141.95879420992543, "name": "PathFinder._get_spec (<frozen importlib._bootstrap_external>:1495)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875797.852, "ph": "X", "dur": 144.25965832561985, "name": "PathFinder.find_spec (<frozen importlib._bootstrap_external>:1524)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875942.925, "ph": "X", "dur": 0.11304321052164619, "name": "_imp.release_lock", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875942.808, "ph": "X", "dur": 0.32260793156562106, "name": "_ImportLockContext.__exit__ (<frozen importlib._bootstrap>:1226)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875943.95, "ph": "X", "dur": 0.113912773679505, "name": "_imp.acquire_lock", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875943.897, "ph": "X", "dur": 0.22608642104329238, "name": "_ImportLockContext.__enter__ (<frozen importlib._bootstrap>:1222)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875946.952, "ph": "X", "dur": 0.24782549998976283, "name": "str.rpartition", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875946.366, "ph": "X", "dur": 1.1008669578492623, "name": "_EditableFinder.find_spec (/root/miniconda3/envs/gs-lightning/lib/python3.12/site-packages/__editable___torch_2_9_0a0_git7cc5d03_finder.py:15)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875947.867, "ph": "X", "dur": 0.0773911210494347, "name": "_imp.release_lock", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875947.788, "ph": "X", "dur": 0.20347777893896316, "name": "_ImportLockContext.__exit__ (<frozen importlib._bootstrap>:1226)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875772.53, "ph": "X", "dur": 175.7639315348448, "name": "_find_spec (<frozen importlib._bootstrap>:1240)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875769.633, "ph": "X", "dur": 181.93609082932667, "name": "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875954.206, "ph": "X", "dur": 0.13826054209955188, "name": "_thread.get_ident", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875955.018, "ph": "X", "dur": 0.20260821578110433, "name": "builtins.len", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875955.451, "ph": "X", "dur": 0.19304302104465734, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875955.803, "ph": "X", "dur": 0.0730433052601406, "name": "builtins.len", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875956.081, "ph": "X", "dur": 0.11478233683736383, "name": "builtins.len", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875956.376, "ph": "X", "dur": 0.2156516631489866, "name": "_thread.RLock.__exit__", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875954.112, "ph": "X", "dur": 2.64608068936438, "name": "_ModuleLock.release (<frozen importlib._bootstrap>:372)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875952.553, "ph": "X", "dur": 4.356511420872673, "name": "_ModuleLockManager.__exit__ (<frozen importlib._bootstrap>:420)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875958.484, "ph": "X", "dur": 0.11478233683736383, "name": "_imp.acquire_lock", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875958.8, "ph": "X", "dur": 0.15652136841458705, "name": "dict.get", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875959.344, "ph": "X", "dur": 0.09652151052232867, "name": "_imp.release_lock", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875958.42, "ph": "X", "dur": 1.0956495789021092, "name": "_get_module_lock.<locals>.cb (<frozen importlib._bootstrap>:445)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875731.544, "ph": "X", "dur": 228.99858805896156, "name": "_find_and_load (<frozen importlib._bootstrap>:1349)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875967.244, "ph": "X", "dur": 1.1704320104779675, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875966.159, "ph": "X", "dur": 2.711297926203791, "name": "tabulate.<locals>.<genexpr> (/data/wangyingqi/code/pytorch/torch/_dynamo/utils.py:230)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875969.133, "ph": "X", "dur": 0.49825968945310206, "name": "tabulate.<locals>.<genexpr> (/data/wangyingqi/code/pytorch/torch/_dynamo/utils.py:230)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875965.006, "ph": "X", "dur": 5.192161615574995, "name": "str.join", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875727.521, "ph": "X", "dur": 243.0620330110122, "name": "tabulate (/data/wangyingqi/code/pytorch/torch/_dynamo/utils.py:221)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875723.662, "ph": "X", "dur": 247.59071793714094, "name": "compile_times (/data/wangyingqi/code/pytorch/torch/_dynamo/utils.py:793)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875973.603, "ph": "X", "dur": 0.22260816841185713, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875973.379, "ph": "X", "dur": 0.5582595473453605, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875974.606, "ph": "X", "dur": 0.08608675262802286, "name": "Manager.disable (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1369)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875974.999, "ph": "X", "dur": 0.45043371577086716, "name": "Logger.getEffectiveLevel (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1776)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875976.038, "ph": "X", "dur": 0.17739088420319865, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875975.9, "ph": "X", "dur": 0.4139120631407968, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875972.508, "ph": "X", "dur": 4.18172922614305, "name": "Logger.isEnabledFor (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1790)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875972.153, "ph": "X", "dur": 4.793901689275657, "name": "Logger.info (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1529)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875720.001, "ph": "X", "dur": 257.15243442095647, "name": "dump_compile_times (/data/wangyingqi/code/pytorch/torch/_dynamo/utils.py:830)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875982.021, "ph": "X", "dur": 0.34173832103851504, "name": "gc.isenabled", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875982.704, "ph": "X", "dur": 0.24956462630548046, "name": "gc.disable", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875985.639, "ph": "X", "dur": 0.1452170473624224, "name": "dict.items", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876023.304, "ph": "X", "dur": 0.7026070315499241, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876024.216, "ph": "X", "dur": 0.21912991578042187, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876026.894, "ph": "X", "dur": 0.0930432578908934, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876027.109, "ph": "X", "dur": 0.12086927894237555, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876027.333, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876027.492, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876027.65, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876027.828, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876027.999, "ph": "X", "dur": 0.053043352629387835, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876028.223, "ph": "X", "dur": 0.04434772105079966, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876028.356, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876028.505, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876028.64, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876028.778, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876028.914, "ph": "X", "dur": 0.046086847366517296, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876029.047, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876029.182, "ph": "X", "dur": 0.04695641052437611, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876029.399, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876029.533, "ph": "X", "dur": 0.04695641052437611, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876029.665, "ph": "X", "dur": 0.046086847366517296, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876029.798, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876029.935, "ph": "X", "dur": 0.04695641052437611, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876030.068, "ph": "X", "dur": 0.053043352629387835, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876030.22, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876030.355, "ph": "X", "dur": 0.04695641052437611, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876030.57, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876030.706, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876030.841, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876030.977, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876031.115, "ph": "X", "dur": 0.04695641052437611, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876031.265, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876031.402, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876031.538, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876031.766, "ph": "X", "dur": 0.04695641052437611, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876031.9, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876032.036, "ph": "X", "dur": 0.046086847366517296, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876032.17, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876032.305, "ph": "X", "dur": 0.046086847366517296, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876032.438, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876032.576, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876032.712, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876032.939, "ph": "X", "dur": 0.046086847366517296, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876033.072, "ph": "X", "dur": 0.0513042263136702, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876033.226, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876033.36, "ph": "X", "dur": 0.0513042263136702, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876033.499, "ph": "X", "dur": 0.046086847366517296, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876033.633, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876033.772, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876035.553, "ph": "X", "dur": 0.05478247894510547, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876035.789, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876035.928, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876036.066, "ph": "X", "dur": 0.0513042263136702, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876036.205, "ph": "X", "dur": 0.04695641052437611, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876036.339, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876036.476, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876036.613, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876036.75, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876036.888, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876037.025, "ph": "X", "dur": 0.04695641052437611, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876037.159, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876037.296, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876037.432, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876037.568, "ph": "X", "dur": 0.12782578420524607, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876037.783, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876037.918, "ph": "X", "dur": 0.045217284208658476, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876038.134, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876038.272, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876038.408, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876038.543, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876038.699, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876038.834, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876038.971, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876039.106, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876039.242, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876039.378, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876039.516, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876039.652, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876039.788, "ph": "X", "dur": 0.04434772105079966, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876039.92, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876040.056, "ph": "X", "dur": 0.046086847366517296, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876040.208, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876040.426, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876040.56, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876040.696, "ph": "X", "dur": 0.046086847366517296, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876040.828, "ph": "X", "dur": 0.0513042263136702, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876040.966, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876041.102, "ph": "X", "dur": 0.045217284208658476, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876041.234, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876041.372, "ph": "X", "dur": 0.046086847366517296, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876041.588, "ph": "X", "dur": 0.0513042263136702, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876041.725, "ph": "X", "dur": 0.0513042263136702, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876041.863, "ph": "X", "dur": 0.06782592631298771, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876042.016, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876042.17, "ph": "X", "dur": 0.1904343315710809, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876042.465, "ph": "X", "dur": 0.046086847366517296, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876044.226, "ph": "X", "dur": 0.057391168418681915, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876044.375, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876044.59, "ph": "X", "dur": 0.052173789471529015, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876044.731, "ph": "X", "dur": 0.0513042263136702, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876044.87, "ph": "X", "dur": 0.1860865157817868, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876045.142, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876045.291, "ph": "X", "dur": 0.18782564209750444, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876045.576, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876045.711, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876045.845, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876046.063, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876046.2, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876046.336, "ph": "X", "dur": 0.1747821947296222, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876046.598, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876046.736, "ph": "X", "dur": 0.1747821947296222, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876046.998, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876047.136, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876047.27, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876047.484, "ph": "X", "dur": 0.052173789471529015, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876047.624, "ph": "X", "dur": 0.0669563631551289, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876047.779, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876047.932, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876048.071, "ph": "X", "dur": 0.052173789471529015, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876048.211, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876048.364, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876048.502, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876048.709, "ph": "X", "dur": 0.0513042263136702, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876048.848, "ph": "X", "dur": 0.0513042263136702, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876048.986, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876049.123, "ph": "X", "dur": 0.04695641052437611, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876049.258, "ph": "X", "dur": 0.0513042263136702, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876049.397, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876049.533, "ph": "X", "dur": 0.052173789471529015, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876049.672, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876049.891, "ph": "X", "dur": 0.052173789471529015, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876050.032, "ph": "X", "dur": 0.053043352629387835, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876050.171, "ph": "X", "dur": 0.04434772105079966, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876050.3, "ph": "X", "dur": 0.052173789471529015, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876050.439, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876050.576, "ph": "X", "dur": 0.052173789471529015, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876050.716, "ph": "X", "dur": 0.04956509999795256, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876050.865, "ph": "X", "dur": 0.14434748420456361, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876051.098, "ph": "X", "dur": 0.050434663155811375, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876051.234, "ph": "X", "dur": 0.04869553684009374, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876051.37, "ph": "X", "dur": 0.04782597368223493, "name": "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876021.856, "ph": "X", "dur": 30.83036176188435, "name": "list.sort", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875985.293, "ph": "X", "dur": 73.29721770223539, "name": "finalize._select_for_exit (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:634)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876061.84, "ph": "X", "dur": 0.1434779210467048, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876064.251, "ph": "X", "dur": 0.4286946368243967, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876077.81, "ph": "X", "dur": 0.3304339999863504, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876075.16, "ph": "X", "dur": 3.191296789341858, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876073.122, "ph": "X", "dur": 5.3956393945139585, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876080.032, "ph": "X", "dur": 78.79372642306097, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876067.72, "ph": "X", "dur": 91.35369667517372, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876063.705, "ph": "X", "dur": 95.59020838026188, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876161.269, "ph": "X", "dur": 0.17391263157176337, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876162.591, "ph": "X", "dur": 0.18782564209750444, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876168.105, "ph": "X", "dur": 0.17304306841390457, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876166.945, "ph": "X", "dur": 1.4739095525706947, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876166.242, "ph": "X", "dur": 2.2756467841165238, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876168.906, "ph": "X", "dur": 29.799929419821655, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876163.751, "ph": "X", "dur": 35.055569145920344, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876162.301, "ph": "X", "dur": 36.699043514273505, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876200.257, "ph": "X", "dur": 0.12173884210023436, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876201.042, "ph": "X", "dur": 0.27826021051482136, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876202.622, "ph": "X", "dur": 0.9399977736453811, "name": "str.split", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876205.772, "ph": "X", "dur": 0.20869515788611606, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876205.369, "ph": "X", "dur": 0.6921722736556183, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876204.964, "ph": "X", "dur": 1.1721711367936853, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876206.402, "ph": "X", "dur": 24.96776695160021, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876201.936, "ph": "X", "dur": 29.534712656674714, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876200.831, "ph": "X", "dur": 30.872100793461577, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876232.755, "ph": "X", "dur": 0.1182605894687991, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876233.428, "ph": "X", "dur": 0.18521695262392798, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876235.568, "ph": "X", "dur": 0.36956434208999717, "name": "str.split", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876238.067, "ph": "X", "dur": 0.19217345788679854, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876237.825, "ph": "X", "dur": 0.532172652609596, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876237.301, "ph": "X", "dur": 1.1286929789007443, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876238.623, "ph": "X", "dur": 21.74777457804901, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876234.274, "ph": "X", "dur": 26.15993804102465, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876233.301, "ph": "X", "dur": 27.27384844624179, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876261.704, "ph": "X", "dur": 0.15826049473030468, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876262.37, "ph": "X", "dur": 0.19739083683395142, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876266.654, "ph": "X", "dur": 0.3313035631442092, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876266.366, "ph": "X", "dur": 0.731302615759265, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876265.674, "ph": "X", "dur": 1.4973877578328827, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876267.448, "ph": "X", "dur": 25.883416956825542, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876263.571, "ph": "X", "dur": 29.840798888241018, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876262.184, "ph": "X", "dur": 31.40601257238689, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876295.218, "ph": "X", "dur": 0.12521709473166964, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876295.819, "ph": "X", "dur": 0.15739093157244585, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876301.651, "ph": "X", "dur": 43.642505329776164, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876296.629, "ph": "X", "dur": 48.78162359272177, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876295.734, "ph": "X", "dur": 52.39031069783586, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876349.652, "ph": "X", "dur": 0.13565185262597543, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876350.371, "ph": "X", "dur": 0.1434779210467048, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876353.304, "ph": "X", "dur": 15.999092541444371, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876351.279, "ph": "X", "dur": 18.09560931504198, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876350.24, "ph": "X", "dur": 19.22604142025844, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876370.765, "ph": "X", "dur": 0.1182605894687991, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876371.362, "ph": "X", "dur": 0.12086927894237555, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876374.53, "ph": "X", "dur": 0.1799995736767751, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876374.131, "ph": "X", "dur": 0.7095635368127946, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876373.385, "ph": "X", "dur": 1.5417354788836823, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876375.132, "ph": "X", "dur": 5.606073678715792, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876372.483, "ph": "X", "dur": 8.333023741761043, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876371.263, "ph": "X", "dur": 9.705194404862256, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876382.448, "ph": "X", "dur": 0.0826084999965876, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876382.904, "ph": "X", "dur": 0.18521695262392798, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876385.05, "ph": "X", "dur": 130.01447467357673, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876383.845, "ph": "X", "dur": 131.30055858404992, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876382.821, "ph": "X", "dur": 132.46751234189645, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876516.563, "ph": "X", "dur": 0.09739107368018748, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876517.044, "ph": "X", "dur": 0.1599996210460223, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876520.552, "ph": "X", "dur": 48.48684168220763, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876517.628, "ph": "X", "dur": 51.51466059787203, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876516.962, "ph": "X", "dur": 52.34248472415363, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876569.995, "ph": "X", "dur": 0.13826054209955188, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876570.585, "ph": "X", "dur": 0.1860865157817868, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876573.769, "ph": "X", "dur": 27.259935435716052, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876571.377, "ph": "X", "dur": 29.761668640875865, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876570.479, "ph": "X", "dur": 30.76514452504494, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876602.677, "ph": "X", "dur": 0.09826063683804631, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876603.25, "ph": "X", "dur": 0.18260826315035156, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876605.381, "ph": "X", "dur": 7.432156310219308, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876604.578, "ph": "X", "dur": 8.305197720709561, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876603.099, "ph": "X", "dur": 9.87823747327616, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876613.833, "ph": "X", "dur": 0.1182605894687991, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876614.458, "ph": "X", "dur": 0.14434748420456361, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876616.239, "ph": "X", "dur": 5.991290157647248, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876615.252, "ph": "X", "dur": 7.039983326024982, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876614.359, "ph": "X", "dur": 8.035633141773328, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876622.988, "ph": "X", "dur": 0.12434753157381082, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876623.501, "ph": "X", "dur": 0.1860865157817868, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876624.865, "ph": "X", "dur": 2.569559131472804, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876624.261, "ph": "X", "dur": 3.2304271314455044, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876623.417, "ph": "X", "dur": 4.167816215617309, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876628.382, "ph": "X", "dur": 0.12521709473166964, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876628.989, "ph": "X", "dur": 0.18434738946606918, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876630.446, "ph": "X", "dur": 14.084314467839256, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876629.831, "ph": "X", "dur": 16.817351472989518, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876628.903, "ph": "X", "dur": 17.899957604523745, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876647.735, "ph": "X", "dur": 0.12347796841595199, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876648.344, "ph": "X", "dur": 0.16434743683531639, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876650.476, "ph": "X", "dur": 33.26774729336262, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876649.156, "ph": "X", "dur": 34.6764396090939, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876648.237, "ph": "X", "dur": 35.70513282484088, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876684.922, "ph": "X", "dur": 0.12608665788952844, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876685.456, "ph": "X", "dur": 0.18869520525536326, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876686.73, "ph": "X", "dur": 6.119985505010353, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876686.126, "ph": "X", "dur": 6.807809962876678, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876685.349, "ph": "X", "dur": 7.6869383154719415, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876693.911, "ph": "X", "dur": 0.13130403683668135, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876694.407, "ph": "X", "dur": 0.15652136841458705, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876696.37, "ph": "X", "dur": 7.6443297207368595, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876695.664, "ph": "X", "dur": 8.409545299652619, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876694.326, "ph": "X", "dur": 9.874759220644725, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876704.876, "ph": "X", "dur": 0.1339127263102578, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876705.381, "ph": "X", "dur": 0.16869525262461046, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876799.044, "ph": "X", "dur": 5413.954133744781, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876706.142, "ph": "X", "dur": 5507.481738314602, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831876705.298, "ph": "X", "dur": 5508.872169804018, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882218.772, "ph": "X", "dur": 0.3130427368291741, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882221.303, "ph": "X", "dur": 0.3739121578792913, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882224.643, "ph": "X", "dur": 6.244333036584164, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882223.207, "ph": "X", "dur": 7.769546815468528, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882220.953, "ph": "X", "dur": 10.171280257474582, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882232.178, "ph": "X", "dur": 0.17826044736105745, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882232.889, "ph": "X", "dur": 0.7426069368114296, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882235.345, "ph": "X", "dur": 17.809523036106427, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882234.251, "ph": "X", "dur": 18.992128930794422, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882232.782, "ph": "X", "dur": 20.954732978081772, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882254.819, "ph": "X", "dur": 0.08347806315444642, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882255.325, "ph": "X", "dur": 0.19478214736037497, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882260.612, "ph": "X", "dur": 0.2860862789355508, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882260.098, "ph": "X", "dur": 0.9999976315376394, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882259.136, "ph": "X", "dur": 2.17564702096276, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882261.615, "ph": "X", "dur": 32.9216611565348, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882256.085, "ph": "X", "dur": 38.519039203672016, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882255.237, "ph": "X", "dur": 39.52686290363038, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882295.511, "ph": "X", "dur": 0.1339127263102578, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882296.149, "ph": "X", "dur": 0.237390742095457, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882299.787, "ph": "X", "dur": 0.1460866105202812, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882299.439, "ph": "X", "dur": 0.5895638210282779, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882298.883, "ph": "X", "dur": 1.2278231788966494, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882300.313, "ph": "X", "dur": 21.362558099117557, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882296.953, "ph": "X", "dur": 28.915583688279238, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882296.049, "ph": "X", "dur": 30.00601588823419, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882327.922, "ph": "X", "dur": 0.15130398946743415, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882328.517, "ph": "X", "dur": 0.2504341894633393, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882333.091, "ph": "X", "dur": 0.19999952630752788, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882332.263, "ph": "X", "dur": 1.1426059894264855, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882331.716, "ph": "X", "dur": 1.7730392788741276, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882333.704, "ph": "X", "dur": 21.7121224885768, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882329.737, "ph": "X", "dur": 25.76254767788317, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882328.431, "ph": "X", "dur": 27.20080514098165, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882356.754, "ph": "X", "dur": 0.13652141578383425, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882357.319, "ph": "X", "dur": 0.19912996314966908, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882360.854, "ph": "X", "dur": 0.12695622104738727, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882360.537, "ph": "X", "dur": 0.5217378947152902, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882360.033, "ph": "X", "dur": 1.0895626367970976, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882361.321, "ph": "X", "dur": 19.613866588663473, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882358.595, "ph": "X", "dur": 22.40864257802171, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882357.219, "ph": "X", "dur": 23.9016825200653, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882382.278, "ph": "X", "dur": 0.1434779210467048, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882382.897, "ph": "X", "dur": 0.23130379999044529, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882386.403, "ph": "X", "dur": 0.13217359999454015, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882386.104, "ph": "X", "dur": 0.5486943526089135, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882385.669, "ph": "X", "dur": 1.0617366157456154, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882386.91, "ph": "X", "dur": 20.14690880443093, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882384.425, "ph": "X", "dur": 22.69559842011512, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882382.812, "ph": "X", "dur": 24.44602905688492, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882407.976, "ph": "X", "dur": 0.11999971578451672, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882408.697, "ph": "X", "dur": 0.32173836840776227, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882412.955, "ph": "X", "dur": 0.22173860525399833, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882412.072, "ph": "X", "dur": 1.1913015262665791, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882411.696, "ph": "X", "dur": 1.6452134946688817, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882413.614, "ph": "X", "dur": 20.49734275704803, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882409.981, "ph": "X", "dur": 24.212986130578756, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882408.403, "ph": "X", "dur": 25.908634288403448, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882435.072, "ph": "X", "dur": 0.12086927894237555, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882435.607, "ph": "X", "dur": 0.23826030525331585, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882438.502, "ph": "X", "dur": 0.1434779210467048, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882438.255, "ph": "X", "dur": 0.4799988631380669, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882437.72, "ph": "X", "dur": 1.08782351048138, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882439.028, "ph": "X", "dur": 22.68951147801011, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882436.415, "ph": "X", "dur": 25.36254862526811, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882435.519, "ph": "X", "dur": 26.401676598909397, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882462.702, "ph": "X", "dur": 0.10608670525877567, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882463.21, "ph": "X", "dur": 0.17565175788748102, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882466.545, "ph": "X", "dur": 0.1652169999931752, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882466.237, "ph": "X", "dur": 0.5513030420824899, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882465.709, "ph": "X", "dur": 1.15391031047865, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882470.845, "ph": "X", "dur": 22.56342482012058, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882464.261, "ph": "X", "dur": 29.237322056687, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882463.113, "ph": "X", "dur": 30.527753782949482, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882494.494, "ph": "X", "dur": 0.09217369473303459, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882495.024, "ph": "X", "dur": 0.24434724735832752, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882499.129, "ph": "X", "dur": 0.15739093157244585, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882498.827, "ph": "X", "dur": 0.5382595947146076, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882497.63, "ph": "X", "dur": 1.8060826788727626, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882499.64, "ph": "X", "dur": 23.484292204293066, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882495.768, "ph": "X", "dur": 27.416456804130636, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882494.939, "ph": "X", "dur": 28.389497977774653, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882524.026, "ph": "X", "dur": 0.11217364736378738, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882524.465, "ph": "X", "dur": 0.12347796841595199, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882527.295, "ph": "X", "dur": 0.18782564209750444, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882527.02, "ph": "X", "dur": 0.5382595947146076, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882526.5, "ph": "X", "dur": 1.1278234157428855, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882527.848, "ph": "X", "dur": 19.553866730771215, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882525.182, "ph": "X", "dur": 22.283425483290042, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882524.381, "ph": "X", "dur": 23.18690160430535, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882548.493, "ph": "X", "dur": 0.12521709473166964, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882549.198, "ph": "X", "dur": 0.20782559472825723, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882552.531, "ph": "X", "dur": 0.13130403683668135, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882552.273, "ph": "X", "dur": 0.4634771631387494, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882551.804, "ph": "X", "dur": 1.0147802052212394, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882553.016, "ph": "X", "dur": 20.96690686229179, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882550.024, "ph": "X", "dur": 24.02776917795483, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882549.086, "ph": "X", "dur": 25.13733176738268, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882575.279, "ph": "X", "dur": 0.11217364736378738, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882575.774, "ph": "X", "dur": 0.16173874736173993, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882579.366, "ph": "X", "dur": 0.1295649105209637, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882578.379, "ph": "X", "dur": 1.2069536631080378, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882577.845, "ph": "X", "dur": 1.8156478736092097, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882579.863, "ph": "X", "dur": 19.795605288655967, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882576.805, "ph": "X", "dur": 22.911250083264108, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882575.688, "ph": "X", "dur": 24.164290593738663, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882600.825, "ph": "X", "dur": 0.11739102631094028, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882601.384, "ph": "X", "dur": 0.2060864684125396, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882604.621, "ph": "X", "dur": 0.21304297367541014, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882603.824, "ph": "X", "dur": 1.086953947323521, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882603.308, "ph": "X", "dur": 1.6782568946675167, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882605.176, "ph": "X", "dur": 22.45820767801966, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882602.09, "ph": "X", "dur": 25.602548056837147, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882601.28, "ph": "X", "dur": 26.55645884100827, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882628.521, "ph": "X", "dur": 0.1034780157851992, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882629.063, "ph": "X", "dur": 0.23478205262188057, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882631.79, "ph": "X", "dur": 0.18782564209750444, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882631.374, "ph": "X", "dur": 2.7608630262017435, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882630.912, "ph": "X", "dur": 3.3017313103899277, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882634.442, "ph": "X", "dur": 21.042558857025508, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882629.812, "ph": "X", "dur": 25.735591219989544, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882628.949, "ph": "X", "dur": 26.750371425210783, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882656.403, "ph": "X", "dur": 0.09739107368018748, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882657.489, "ph": "X", "dur": 0.2060864684125396, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882661.373, "ph": "X", "dur": 0.18695607893964564, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882660.932, "ph": "X", "dur": 0.7043461578656417, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882659.886, "ph": "X", "dur": 1.823473942029939, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882661.909, "ph": "X", "dur": 20.442560278102928, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882658.284, "ph": "X", "dur": 24.126029814792872, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882657.4, "ph": "X", "dur": 25.155592593697715, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882683.323, "ph": "X", "dur": 0.15304311578315177, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882683.812, "ph": "X", "dur": 0.13130403683668135, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882686.846, "ph": "X", "dur": 0.16434743683531639, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882686.534, "ph": "X", "dur": 0.5521726052403487, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882685.989, "ph": "X", "dur": 1.173040699951544, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882687.354, "ph": "X", "dur": 20.27908240442547, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882684.556, "ph": "X", "dur": 23.14603213588599, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882683.724, "ph": "X", "dur": 24.10081248321497, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882708.513, "ph": "X", "dur": 0.09739107368018748, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882709.52, "ph": "X", "dur": 0.30608623156630355, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882712.297, "ph": "X", "dur": 0.17217350525604572, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882711.98, "ph": "X", "dur": 0.5704334315553838, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882711.525, "ph": "X", "dur": 1.0982582683756856, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882712.82, "ph": "X", "dur": 23.206901556936103, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882710.397, "ph": "X", "dur": 25.700808693675192, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882709.431, "ph": "X", "dur": 26.821675604155207, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882737.24, "ph": "X", "dur": 0.12782578420524607, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882737.846, "ph": "X", "dur": 0.21391253683326894, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882740.751, "ph": "X", "dur": 0.17565175788748102, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882740.33, "ph": "X", "dur": 0.6852157683927477, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882739.825, "ph": "X", "dur": 1.267823084158155, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882741.308, "ph": "X", "dur": 21.782557104363363, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882738.587, "ph": "X", "dur": 24.579941783195174, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882737.755, "ph": "X", "dur": 25.53124387789272, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882764.569, "ph": "X", "dur": 0.11304321052164619, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882765.146, "ph": "X", "dur": 0.1460866105202812, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882767.908, "ph": "X", "dur": 0.13826054209955188, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882767.658, "ph": "X", "dur": 0.46956410524376113, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882767.102, "ph": "X", "dur": 1.0904321999549564, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882768.374, "ph": "X", "dur": 22.35646878855018, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882765.765, "ph": "X", "dur": 25.05472326738609, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882765.042, "ph": "X", "dur": 25.90776472524559, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882792.334, "ph": "X", "dur": 0.113912773679505, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882792.876, "ph": "X", "dur": 0.27130370525195086, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882797.814, "ph": "X", "dur": 0.16173874736173993, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882797.444, "ph": "X", "dur": 0.607824647343313, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882796.9, "ph": "X", "dur": 1.2139101683709084, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882798.354, "ph": "X", "dur": 22.55820744117343, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882795.658, "ph": "X", "dur": 25.346026925268795, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882792.769, "ph": "X", "dur": 28.36167195672317, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882821.927, "ph": "X", "dur": 0.1434779210467048, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882822.554, "ph": "X", "dur": 0.1539126789410106, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882825.577, "ph": "X", "dur": 0.2052169052546808, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882825.335, "ph": "X", "dur": 0.5478247894510546, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882824.828, "ph": "X", "dur": 1.1260842894271679, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882826.19, "ph": "X", "dur": 22.583424772751332, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882823.307, "ph": "X", "dur": 25.53993950947131, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882822.435, "ph": "X", "dur": 26.539937141008952, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882850.038, "ph": "X", "dur": 0.12347796841595199, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882850.643, "ph": "X", "dur": 0.30695579472416235, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882853.55, "ph": "X", "dur": 0.113912773679505, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882853.309, "ph": "X", "dur": 0.432172889455832, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882852.754, "ph": "X", "dur": 1.063475742061333, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882854.031, "ph": "X", "dur": 19.49125818340538, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882851.5, "ph": "X", "dur": 22.095599841192534, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882850.534, "ph": "X", "dur": 23.20255374114681, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882874.501, "ph": "X", "dur": 0.13999966841526953, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882875.088, "ph": "X", "dur": 0.5852160052389838, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882879.275, "ph": "X", "dur": 0.16608656315103404, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882878.161, "ph": "X", "dur": 1.358257652575472, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882877.536, "ph": "X", "dur": 2.0530386157046667, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882879.801, "ph": "X", "dur": 20.19734346758674, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882876.32, "ph": "X", "dur": 23.736465520072123, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882874.983, "ph": "X", "dur": 25.226027209484275, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882901.366, "ph": "X", "dur": 0.19304302104465734, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882901.976, "ph": "X", "dur": 0.19652127367609262, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882905.304, "ph": "X", "dur": 0.18347782630821036, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882904.97, "ph": "X", "dur": 0.5947811999754307, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882904.427, "ph": "X", "dur": 1.206084099950179, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882905.799, "ph": "X", "dur": 20.628646793884712, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882902.756, "ph": "X", "dur": 23.727769888493537, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882901.887, "ph": "X", "dur": 24.730376209504755, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882927.258, "ph": "X", "dur": 0.12173884210023436, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882927.775, "ph": "X", "dur": 0.27739064735696256, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882931.296, "ph": "X", "dur": 0.17043437894032812, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882930.966, "ph": "X", "dur": 0.5765203736603957, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882930.423, "ph": "X", "dur": 1.1904319631087203, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882931.83, "ph": "X", "dur": 18.773868578171857, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882928.618, "ph": "X", "dur": 22.069512946456772, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882927.686, "ph": "X", "dur": 23.132119125360248, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882954.175, "ph": "X", "dur": 0.14782573683599887, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882954.888, "ph": "X", "dur": 0.2052169052546808, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882958.29, "ph": "X", "dur": 0.11217364736378738, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882957.946, "ph": "X", "dur": 0.5339117789253136, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882957.377, "ph": "X", "dur": 1.1773885157408381, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882958.751, "ph": "X", "dur": 19.246041372889195, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882955.792, "ph": "X", "dur": 22.267773346448582, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882954.8, "ph": "X", "dur": 23.399075014822902, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882979.331, "ph": "X", "dur": 0.1078258315744933, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882979.839, "ph": "X", "dur": 0.21912991578042187, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882982.774, "ph": "X", "dur": 0.18260826315035156, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882982.512, "ph": "X", "dur": 0.5234770210310078, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882981.969, "ph": "X", "dur": 1.1399972999529089, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882983.28, "ph": "X", "dur": 26.563415346271135, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882980.721, "ph": "X", "dur": 29.18340914089975, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831882979.751, "ph": "X", "dur": 30.289493477696166, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883010.743, "ph": "X", "dur": 0.09739107368018748, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883011.293, "ph": "X", "dur": 0.26956457893623326, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883014.692, "ph": "X", "dur": 0.21826035262256302, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883014.527, "ph": "X", "dur": 0.46173803682303177, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883014.069, "ph": "X", "dur": 0.9843454946961807, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883015.225, "ph": "X", "dur": 22.177338778031267, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883012.085, "ph": "X", "dur": 25.380809451583147, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883011.198, "ph": "X", "dur": 26.40950266733013, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883038.417, "ph": "X", "dur": 0.12260840525809319, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883038.958, "ph": "X", "dur": 0.22956467367472766, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883041.529, "ph": "X", "dur": 0.21391253683326894, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883041.295, "ph": "X", "dur": 0.5399987210303253, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883040.768, "ph": "X", "dur": 1.1478233683736383, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883042.081, "ph": "X", "dur": 21.084297888602734, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883039.774, "ph": "X", "dur": 23.463422688504455, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883038.87, "ph": "X", "dur": 24.49124634109358, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883064.05, "ph": "X", "dur": 0.12347796841595199, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883064.583, "ph": "X", "dur": 0.21739078946470422, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883067.95, "ph": "X", "dur": 0.12782578420524607, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883067.536, "ph": "X", "dur": 0.6191289683954776, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883067.011, "ph": "X", "dur": 1.2234753631073554, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883068.43, "ph": "X", "dur": 18.679955757123107, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883065.312, "ph": "X", "dur": 21.869513420149243, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883064.485, "ph": "X", "dur": 22.847771972740414, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883088.057, "ph": "X", "dur": 0.10956495789021094, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883088.593, "ph": "X", "dur": 0.20956472104397486, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883092.201, "ph": "X", "dur": 0.18173869999249273, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883091.803, "ph": "X", "dur": 0.6956505262870535, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883090.73, "ph": "X", "dur": 1.8365173893978213, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883092.746, "ph": "X", "dur": 19.62951872550493, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883089.609, "ph": "X", "dur": 22.839076341161825, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883088.493, "ph": "X", "dur": 26.024286188398673, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883115.623, "ph": "X", "dur": 0.18782564209750444, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883116.238, "ph": "X", "dur": 0.20347777893896316, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883119.535, "ph": "X", "dur": 0.1339127263102578, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883118.961, "ph": "X", "dur": 0.7860850947043705, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883118.363, "ph": "X", "dur": 1.4504313473085064, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883120.004, "ph": "X", "dur": 19.80777917286599, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883116.964, "ph": "X", "dur": 22.928641346421283, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883116.154, "ph": "X", "dur": 23.881682567434545, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883140.795, "ph": "X", "dur": 0.11217364736378738, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883141.318, "ph": "X", "dur": 0.133043163152399, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883144.509, "ph": "X", "dur": 0.13478228946811663, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883144.169, "ph": "X", "dur": 0.5521726052403487, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883143.364, "ph": "X", "dur": 1.4217357630991656, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883144.981, "ph": "X", "dur": 21.049515362288382, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883142.019, "ph": "X", "dur": 24.07733427795278, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883141.231, "ph": "X", "dur": 25.000810351598844, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883167.05, "ph": "X", "dur": 0.10695626841663447, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883167.562, "ph": "X", "dur": 0.20347777893896316, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883170.248, "ph": "X", "dur": 0.11130408420592856, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883170.001, "ph": "X", "dur": 0.43825983156084375, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883169.535, "ph": "X", "dur": 0.9739107368018749, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883170.724, "ph": "X", "dur": 18.980824609742257, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883168.281, "ph": "X", "dur": 21.51734034121642, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883167.475, "ph": "X", "dur": 22.46777287275611, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883190.717, "ph": "X", "dur": 0.12695622104738727, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883191.254, "ph": "X", "dur": 0.14782573683599887, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883194.108, "ph": "X", "dur": 0.12782578420524607, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883193.895, "ph": "X", "dur": 0.4199990052458086, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883193.337, "ph": "X", "dur": 1.0521714210091684, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883194.549, "ph": "X", "dur": 20.1243001623266, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883191.933, "ph": "X", "dur": 22.809511193794624, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883191.168, "ph": "X", "dur": 23.7095090621785, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883215.678, "ph": "X", "dur": 0.11913015262665792, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883216.197, "ph": "X", "dur": 0.14782573683599887, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883219.418, "ph": "X", "dur": 0.18260826315035156, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883219.169, "ph": "X", "dur": 0.5191292052417137, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883218.048, "ph": "X", "dur": 1.7086916051925753, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883219.989, "ph": "X", "dur": 17.503436804540126, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883216.905, "ph": "X", "dur": 20.659951067567633, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883216.115, "ph": "X", "dur": 21.59299233595014, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883238.374, "ph": "X", "dur": 0.1078258315744933, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883238.973, "ph": "X", "dur": 0.15043442630957532, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883242.274, "ph": "X", "dur": 0.12695622104738727, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883241.695, "ph": "X", "dur": 0.7826068420729352, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883241.194, "ph": "X", "dur": 1.3660837209962013, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883244.775, "ph": "X", "dur": 18.775607704487573, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883240.051, "ph": "X", "dur": 23.559944199026784, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883238.809, "ph": "X", "dur": 24.931245298970136, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883264.484, "ph": "X", "dur": 0.11130408420592856, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883265.017, "ph": "X", "dur": 0.22347773156971593, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883268.305, "ph": "X", "dur": 0.11478233683736383, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883267.967, "ph": "X", "dur": 0.5286943999781606, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883267.157, "ph": "X", "dur": 1.4017358104684128, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883268.76, "ph": "X", "dur": 19.011259320267314, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883265.787, "ph": "X", "dur": 22.054730372773175, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883264.929, "ph": "X", "dur": 23.0486410622058, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883288.671, "ph": "X", "dur": 0.12608665788952844, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883289.317, "ph": "X", "dur": 0.19826039999181025, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883292.813, "ph": "X", "dur": 0.113912773679505, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883292.447, "ph": "X", "dur": 0.5599986736610781, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883291.502, "ph": "X", "dur": 1.580865820987329, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883293.285, "ph": "X", "dur": 20.98951550439612, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883290.129, "ph": "X", "dur": 24.206899188473745, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883289.228, "ph": "X", "dur": 25.23994022001002, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883315.253, "ph": "X", "dur": 0.2017386526232455, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883315.868, "ph": "X", "dur": 0.1652169999931752, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883319.062, "ph": "X", "dur": 0.2069560315703984, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883318.742, "ph": "X", "dur": 0.6069550841854542, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883318.257, "ph": "X", "dur": 1.1573885631100853, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883319.555, "ph": "X", "dur": 21.29299304648885, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883316.619, "ph": "X", "dur": 24.316464146363955, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883315.782, "ph": "X", "dur": 25.30602702000729, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883341.816, "ph": "X", "dur": 0.10869539473235211, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883342.344, "ph": "X", "dur": 0.1652169999931752, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883345.854, "ph": "X", "dur": 0.10608670525877567, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883345.63, "ph": "X", "dur": 0.4078251210357851, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883344.354, "ph": "X", "dur": 1.7486915104540808, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883346.277, "ph": "X", "dur": 18.477347541342, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883343.157, "ph": "X", "dur": 21.68951384647247, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883342.261, "ph": "X", "dur": 22.68777235169439, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883365.645, "ph": "X", "dur": 0.1286953473631049, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883366.205, "ph": "X", "dur": 0.23391248946402174, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883369.74, "ph": "X", "dur": 0.16782568946675167, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883369.582, "ph": "X", "dur": 0.40521643156220866, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883368.212, "ph": "X", "dur": 1.846952147292127, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883370.253, "ph": "X", "dur": 19.583431878138413, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883367.189, "ph": "X", "dur": 22.709511430640863, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883366.099, "ph": "X", "dur": 23.946899804273958, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883390.945, "ph": "X", "dur": 0.1078258315744933, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883391.477, "ph": "X", "dur": 0.23391248946402174, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883394.892, "ph": "X", "dur": 0.12347796841595199, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883394.643, "ph": "X", "dur": 0.4660858526123258, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883394.198, "ph": "X", "dur": 2.784341231463932, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883397.24, "ph": "X", "dur": 20.49038625178516, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883392.711, "ph": "X", "dur": 25.080810162121857, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883391.389, "ph": "X", "dur": 26.545154519956103, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883418.538, "ph": "X", "dur": 0.12608665788952844, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883419.085, "ph": "X", "dur": 0.24695593683190398, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883421.957, "ph": "X", "dur": 0.1078258315744933, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883421.65, "ph": "X", "dur": 0.49652056313738446, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883420.986, "ph": "X", "dur": 1.2382579367909552, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883422.391, "ph": "X", "dur": 19.988648309700626, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883419.816, "ph": "X", "dur": 22.620815988539263, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883418.998, "ph": "X", "dur": 23.588639783236125, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883443.372, "ph": "X", "dur": 0.13217359999454015, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883443.978, "ph": "X", "dur": 0.24434724735832752, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883447.016, "ph": "X", "dur": 0.13913010525741068, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883446.638, "ph": "X", "dur": 0.6165202789219012, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883446.15, "ph": "X", "dur": 1.1747798262672615, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883447.502, "ph": "X", "dur": 18.752999062383246, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883445.094, "ph": "X", "dur": 21.242558383333037, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883443.89, "ph": "X", "dur": 22.603424725382087, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883467.313, "ph": "X", "dur": 374.87302516872523, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883843.767, "ph": "X", "dur": 0.3304339999863504, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883847.7, "ph": "X", "dur": 0.22347773156971593, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883847.303, "ph": "X", "dur": 0.7591286368107472, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883846.793, "ph": "X", "dur": 1.4078227525734246, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883848.618, "ph": "X", "dur": 23.714726441125656, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883844.999, "ph": "X", "dur": 27.454717583076423, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883843.49, "ph": "X", "dur": 29.140800546164673, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883874.233, "ph": "X", "dur": 0.13130403683668135, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883874.825, "ph": "X", "dur": 0.18869520525536326, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883877.723, "ph": "X", "dur": 0.16173874736173993, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883877.409, "ph": "X", "dur": 0.5730421210289603, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883876.954, "ph": "X", "dur": 1.0999973946914032, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883878.229, "ph": "X", "dur": 18.846042320274137, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883875.499, "ph": "X", "dur": 21.679079088578163, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883874.729, "ph": "X", "dur": 22.595598656961357, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883898.002, "ph": "X", "dur": 0.19304302104465734, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883898.657, "ph": "X", "dur": 0.2504341894633393, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883902.896, "ph": "X", "dur": 0.11304321052164619, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883902.039, "ph": "X", "dur": 1.0669539946927682, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883901.466, "ph": "X", "dur": 1.7173872367711633, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883903.383, "ph": "X", "dur": 20.49821232020589, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883899.734, "ph": "X", "dur": 24.232986083209507, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883898.569, "ph": "X", "dur": 25.53906994631345, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883924.875, "ph": "X", "dur": 0.11217364736378738, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883925.463, "ph": "X", "dur": 0.22434729472757475, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883931.646, "ph": "X", "dur": 0.13565185262597543, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883931.321, "ph": "X", "dur": 0.5704334315553838, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883930.954, "ph": "X", "dur": 0.9973889420640629, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883932.149, "ph": "X", "dur": 19.40343230446164, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883929.058, "ph": "X", "dur": 22.55733787801557, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883925.375, "ph": "X", "dur": 26.387763588383656, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883952.638, "ph": "X", "dur": 0.12695622104738727, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883953.214, "ph": "X", "dur": 0.1556518052567282, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883956.694, "ph": "X", "dur": 0.1295649105209637, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883956.408, "ph": "X", "dur": 0.5069553210316903, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883955.861, "ph": "X", "dur": 1.1295625420586033, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883957.181, "ph": "X", "dur": 19.306041230781453, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883954.116, "ph": "X", "dur": 22.43038165696818, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883953.125, "ph": "X", "dur": 23.552118130606054, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883977.32, "ph": "X", "dur": 0.11130408420592856, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883977.853, "ph": "X", "dur": 0.2365211789375982, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883981.509, "ph": "X", "dur": 0.13217359999454015, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883981.284, "ph": "X", "dur": 0.43478157892940844, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883980.099, "ph": "X", "dur": 1.6878220894039637, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883981.981, "ph": "X", "dur": 25.36602687789955, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883978.899, "ph": "X", "dur": 28.524280267242773, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831883977.764, "ph": "X", "dur": 29.78688597245377, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884008.421, "ph": "X", "dur": 0.14260835788884596, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884009.08, "ph": "X", "dur": 0.24086899472689227, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884012.044, "ph": "X", "dur": 0.14956486315171652, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884011.7, "ph": "X", "dur": 0.5956507631332896, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884011.156, "ph": "X", "dur": 1.2208666736337788, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884012.565, "ph": "X", "dur": 18.666912309755222, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884009.968, "ph": "X", "dur": 21.31907994122461, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884008.978, "ph": "X", "dur": 22.445164230651784, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884032.557, "ph": "X", "dur": 0.11217364736378738, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884033.113, "ph": "X", "dur": 0.2165212263068454, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884036.237, "ph": "X", "dur": 0.1339127263102578, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884035.99, "ph": "X", "dur": 0.48521624208521985, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884035.475, "ph": "X", "dur": 1.0799974420606506, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884036.742, "ph": "X", "dur": 19.448649588670296, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884033.904, "ph": "X", "dur": 22.34168621486658, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884033.026, "ph": "X", "dur": 23.37298812008714, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884057.235, "ph": "X", "dur": 0.13043447367882255, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884057.807, "ph": "X", "dur": 0.22260816841185713, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884061.573, "ph": "X", "dur": 0.1695648157824693, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884061.249, "ph": "X", "dur": 0.5878246947125603, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884060.383, "ph": "X", "dur": 1.529561594673659, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884062.139, "ph": "X", "dur": 20.260821578110434, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884059.027, "ph": "X", "dur": 23.430379288505822, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884057.713, "ph": "X", "dur": 24.873854130551454, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884083.423, "ph": "X", "dur": 0.1286953473631049, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884086.905, "ph": "X", "dur": 0.2669558894626568, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884089.845, "ph": "X", "dur": 0.18869520525536326, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884089.595, "ph": "X", "dur": 0.5165205157681372, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884089.024, "ph": "X", "dur": 1.1573885631100853, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884090.405, "ph": "X", "dur": 20.105169772853703, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884087.992, "ph": "X", "dur": 22.579076956962037, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884086.815, "ph": "X", "dur": 23.909508588486027, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884111.921, "ph": "X", "dur": 0.19391258420251617, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884112.516, "ph": "X", "dur": 0.20869515788611606, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884116.182, "ph": "X", "dur": 0.11130408420592856, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884115.888, "ph": "X", "dur": 0.48695536840093745, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884114.778, "ph": "X", "dur": 1.667822136773211, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884116.661, "ph": "X", "dur": 18.90430305185068, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884113.649, "ph": "X", "dur": 21.978208814881597, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884112.428, "ph": "X", "dur": 23.32777083587848, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884136.533, "ph": "X", "dur": 0.113912773679505, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884137.058, "ph": "X", "dur": 0.2417385578847511, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884140.2, "ph": "X", "dur": 0.17652132104533982, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884139.958, "ph": "X", "dur": 0.49652056313738446, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884139.401, "ph": "X", "dur": 1.1356494841636149, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884140.729, "ph": "X", "dur": 19.77995315181451, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884137.883, "ph": "X", "dur": 22.68777235169439, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884136.97, "ph": "X", "dur": 23.74690027796643, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884161.634, "ph": "X", "dur": 0.13130403683668135, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884162.186, "ph": "X", "dur": 0.49565099997952566, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884166.418, "ph": "X", "dur": 0.1295649105209637, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884166.098, "ph": "X", "dur": 0.5478247894510546, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884165.092, "ph": "X", "dur": 1.6252135420381286, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884166.886, "ph": "X", "dur": 19.965170104438435, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884163.413, "ph": "X", "dur": 23.492987835871656, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884162.1, "ph": "X", "dur": 24.95037568844303, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884188.225, "ph": "X", "dur": 0.14173879473098713, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884188.764, "ph": "X", "dur": 0.6130420262904659, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884192.013, "ph": "X", "dur": 0.11999971578451672, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884191.654, "ph": "X", "dur": 0.5565204210296427, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884191.196, "ph": "X", "dur": 1.0817365683763682, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884192.447, "ph": "X", "dur": 19.905170246546177, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884189.926, "ph": "X", "dur": 22.486033699071147, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884188.677, "ph": "X", "dur": 23.87472606217168, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884213.285, "ph": "X", "dur": 0.12782578420524607, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884213.804, "ph": "X", "dur": 0.17217350525604572, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884217.438, "ph": "X", "dur": 0.11043452104806974, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884216.815, "ph": "X", "dur": 0.8130415525979938, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884216.33, "ph": "X", "dur": 1.3747793525747896, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884217.878, "ph": "X", "dur": 21.666905204368142, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884215.133, "ph": "X", "dur": 24.471246388462824, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884213.716, "ph": "X", "dur": 26.020807935767237, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884243.338, "ph": "X", "dur": 0.14434748420456361, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884243.886, "ph": "X", "dur": 0.27478195788338616, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884246.784, "ph": "X", "dur": 0.10869539473235211, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884246.432, "ph": "X", "dur": 0.5408682841881841, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884245.829, "ph": "X", "dur": 1.2217362367916378, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884247.233, "ph": "X", "dur": 20.49647319389017, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884244.7, "ph": "X", "dur": 23.087771404309446, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884243.798, "ph": "X", "dur": 24.147768893739343, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884268.856, "ph": "X", "dur": 0.14260835788884596, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884269.394, "ph": "X", "dur": 0.24869506314762163, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884272.246, "ph": "X", "dur": 0.12608665788952844, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884272.003, "ph": "X", "dur": 0.4486945894551495, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884271.437, "ph": "X", "dur": 1.0834756946920858, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884272.7, "ph": "X", "dur": 18.48604317292059, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884270.176, "ph": "X", "dur": 21.07038487807699, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884269.306, "ph": "X", "dur": 22.04342605172101, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884292.138, "ph": "X", "dur": 0.113912773679505, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884292.966, "ph": "X", "dur": 0.22434729472757475, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884296.384, "ph": "X", "dur": 0.17652132104533982, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884296.045, "ph": "X", "dur": 0.5913029473439955, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884295.429, "ph": "X", "dur": 1.2791274052103196, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884296.883, "ph": "X", "dur": 18.552999536075717, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884293.617, "ph": "X", "dur": 21.875600362254257, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884292.886, "ph": "X", "dur": 22.76081565695453, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884316.857, "ph": "X", "dur": 0.1747821947296222, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884317.585, "ph": "X", "dur": 0.18695607893964564, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884321.001, "ph": "X", "dur": 0.2008690894653867, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884320.624, "ph": "X", "dur": 0.6852157683927477, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884319.698, "ph": "X", "dur": 1.6843438367725283, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884321.533, "ph": "X", "dur": 16.921699051932578, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884318.387, "ph": "X", "dur": 20.125169725484458, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884317.463, "ph": "X", "dur": 21.20342804122939, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884340.172, "ph": "X", "dur": 0.13913010525741068, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884340.702, "ph": "X", "dur": 0.1860865157817868, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884343.456, "ph": "X", "dur": 0.16869525262461046, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884343.074, "ph": "X", "dur": 0.6260854736583482, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884342.601, "ph": "X", "dur": 1.1608668157415205, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884343.935, "ph": "X", "dur": 19.343432446569384, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884341.448, "ph": "X", "dur": 21.892122062253573, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884340.614, "ph": "X", "dur": 22.866032799055446, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884364.333, "ph": "X", "dur": 0.113912773679505, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884364.842, "ph": "X", "dur": 0.13043447367882255, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884367.79, "ph": "X", "dur": 0.12434753157381082, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884367.418, "ph": "X", "dur": 0.5739116841868191, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884366.907, "ph": "X", "dur": 1.1573885631100853, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884368.208, "ph": "X", "dur": 17.777349199265654, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884365.78, "ph": "X", "dur": 21.992121825407338, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884364.757, "ph": "X", "dur": 23.17559728325319, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884389.52, "ph": "X", "dur": 0.11217364736378738, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884390.054, "ph": "X", "dur": 0.19739083683395142, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884393.502, "ph": "X", "dur": 0.17913001051891628, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884393.16, "ph": "X", "dur": 0.6026072683961601, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884392.687, "ph": "X", "dur": 1.1530407473207913, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884394.016, "ph": "X", "dur": 16.87648176772392, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884391.095, "ph": "X", "dur": 19.85299645707465, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884389.962, "ph": "X", "dur": 21.137341241232118, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884411.937, "ph": "X", "dur": 0.12521709473166964, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884412.533, "ph": "X", "dur": 0.16695612630889284, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884415.658, "ph": "X", "dur": 0.19652127367609262, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884414.825, "ph": "X", "dur": 1.1382581736371913, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884414.355, "ph": "X", "dur": 1.6773873315096577, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884416.209, "ph": "X", "dur": 19.13995466763042, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884413.281, "ph": "X", "dur": 22.120817172770444, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884412.444, "ph": "X", "dur": 23.090380093783025, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884436.258, "ph": "X", "dur": 0.13130403683668135, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884436.808, "ph": "X", "dur": 0.16869525262461046, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884439.466, "ph": "X", "dur": 0.12260840525809319, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884439.266, "ph": "X", "dur": 0.44086852103442015, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884438.726, "ph": "X", "dur": 1.0530409841670274, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884439.946, "ph": "X", "dur": 18.394739041345414, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884437.712, "ph": "X", "dur": 20.691255341250546, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884436.703, "ph": "X", "dur": 21.818209193835575, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884459.681, "ph": "X", "dur": 0.11478233683736383, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884460.194, "ph": "X", "dur": 0.23391248946402174, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884464.128, "ph": "X", "dur": 0.15130398946743415, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884463.27, "ph": "X", "dur": 1.086953947323521, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884462.739, "ph": "X", "dur": 1.681735147298952, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884464.562, "ph": "X", "dur": 17.310393783495467, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884461.494, "ph": "X", "dur": 20.461690667575823, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884460.105, "ph": "X", "dur": 21.948643667514396, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884482.639, "ph": "X", "dur": 0.12608665788952844, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884483.17, "ph": "X", "dur": 0.22173860525399833, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884485.697, "ph": "X", "dur": 0.1799995736767751, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884485.398, "ph": "X", "dur": 0.5991290157647249, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884484.852, "ph": "X", "dur": 1.2199971104759202, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884486.257, "ph": "X", "dur": 19.967778793912014, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884483.89, "ph": "X", "dur": 22.392120878022393, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884483.085, "ph": "X", "dur": 23.346901225351374, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884507.338, "ph": "X", "dur": 0.19217345788679854, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884507.919, "ph": "X", "dur": 0.2165212263068454, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884511.758, "ph": "X", "dur": 0.18260826315035156, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884511.44, "ph": "X", "dur": 0.579129063133972, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884510.31, "ph": "X", "dur": 1.7765175315055628, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884514.182, "ph": "X", "dur": 19.181693699207642, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884509.2, "ph": "X", "dur": 24.219942635841626, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884507.831, "ph": "X", "dur": 25.721678209463803, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884534.297, "ph": "X", "dur": 0.113912773679505, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884534.831, "ph": "X", "dur": 0.19999952630752788, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884538.257, "ph": "X", "dur": 0.16608656315103404, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884537.954, "ph": "X", "dur": 0.5452160999774782, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884537.404, "ph": "X", "dur": 1.1652146315308145, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884538.744, "ph": "X", "dur": 17.237350478235328, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884535.616, "ph": "X", "dur": 20.436473335997913, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884534.745, "ph": "X", "dur": 21.41821014122052, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884556.716, "ph": "X", "dur": 0.12782578420524607, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884557.224, "ph": "X", "dur": 0.13217359999454015, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884560.479, "ph": "X", "dur": 0.11217364736378738, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884559.472, "ph": "X", "dur": 1.1956493420558731, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884559.005, "ph": "X", "dur": 1.7269524315076104, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884560.904, "ph": "X", "dur": 20.69038577809269, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884557.954, "ph": "X", "dur": 23.699074304284196, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884557.14, "ph": "X", "dur": 24.668637225296777, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884582.8, "ph": "X", "dur": 0.12434753157381082, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884583.336, "ph": "X", "dur": 0.5956507631332896, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884586.582, "ph": "X", "dur": 0.1860865157817868, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884586.242, "ph": "X", "dur": 0.6026072683961601, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884585.729, "ph": "X", "dur": 1.1773885157408381, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884587.086, "ph": "X", "dur": 20.513864457047347, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884584.591, "ph": "X", "dur": 23.066032325362976, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884583.25, "ph": "X", "dur": 24.53907231477581, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884608.529, "ph": "X", "dur": 16.855612251935305, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884626.388, "ph": "X", "dur": 0.2052169052546808, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884629.22, "ph": "X", "dur": 0.19391258420251617, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884628.87, "ph": "X", "dur": 0.6191289683954776, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884628.319, "ph": "X", "dur": 1.2408666262645318, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884629.737, "ph": "X", "dur": 19.122563404473244, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884627.152, "ph": "X", "dur": 21.78342666752122, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884626.297, "ph": "X", "dur": 22.78603298853244, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884650.176, "ph": "X", "dur": 0.1295649105209637, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884650.718, "ph": "X", "dur": 0.12086927894237555, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884653.309, "ph": "X", "dur": 0.16608656315103404, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884653.131, "ph": "X", "dur": 0.4191294420879497, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884652.519, "ph": "X", "dur": 1.0999973946914032, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884653.812, "ph": "X", "dur": 20.73995087809064, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884651.363, "ph": "X", "dur": 23.266031851670505, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884650.631, "ph": "X", "dur": 24.131247193740027, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884675.46, "ph": "X", "dur": 0.12782578420524607, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884676.039, "ph": "X", "dur": 0.2582602578840686, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884679.684, "ph": "X", "dur": 0.113912773679505, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884679.294, "ph": "X", "dur": 2.3582552841131115, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884678.643, "ph": "X", "dur": 3.0852100840830823, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884681.928, "ph": "X", "dur": 19.884300730757566, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884676.748, "ph": "X", "dur": 25.123418756856935, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884675.951, "ph": "X", "dur": 26.074720851554485, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884702.856, "ph": "X", "dur": 0.1295649105209637, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884703.441, "ph": "X", "dur": 0.12521709473166964, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884706.879, "ph": "X", "dur": 0.17913001051891628, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884706.546, "ph": "X", "dur": 0.5939116368175719, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884705.54, "ph": "X", "dur": 1.661735194668199, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884707.34, "ph": "X", "dur": 20.169517446535256, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884704.508, "ph": "X", "dur": 23.05385844115295, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884703.355, "ph": "X", "dur": 24.338203225310426, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884728.464, "ph": "X", "dur": 0.11913015262665792, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884728.999, "ph": "X", "dur": 0.1556518052567282, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884731.686, "ph": "X", "dur": 0.11130408420592856, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884731.438, "ph": "X", "dur": 0.43652070524512604, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884730.873, "ph": "X", "dur": 1.0713018104820624, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884732.095, "ph": "X", "dur": 19.04343315710809, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884729.872, "ph": "X", "dur": 21.32342775701391, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884728.912, "ph": "X", "dur": 22.43385990959962, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884752.107, "ph": "X", "dur": 0.11304321052164619, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884752.649, "ph": "X", "dur": 0.1339127263102578, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884756.021, "ph": "X", "dur": 0.15913005788816348, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884755.753, "ph": "X", "dur": 0.5121726999788432, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884754.604, "ph": "X", "dur": 1.738256752559775, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884756.541, "ph": "X", "dur": 20.1390827360102, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884753.341, "ph": "X", "dur": 23.392988072717895, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884752.561, "ph": "X", "dur": 24.301681572680355, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884777.745, "ph": "X", "dur": 0.12434753157381082, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884778.338, "ph": "X", "dur": 0.14782573683599887, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884782.266, "ph": "X", "dur": 0.11304321052164619, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884781.767, "ph": "X", "dur": 0.7130417894442298, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884781.328, "ph": "X", "dur": 1.2243449262652142, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884782.754, "ph": "X", "dur": 18.369521709767504, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884779.385, "ph": "X", "dur": 21.799078804362683, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884778.252, "ph": "X", "dur": 23.08603227799373, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884802.67, "ph": "X", "dur": 0.13565185262597543, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884803.24, "ph": "X", "dur": 0.2113038473596925, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884806.728, "ph": "X", "dur": 0.17043437894032812, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884806.483, "ph": "X", "dur": 0.5069553210316903, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884805.951, "ph": "X", "dur": 1.1086930262699917, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884807.23, "ph": "X", "dur": 18.591260315021504, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884804.692, "ph": "X", "dur": 21.181688962282923, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884803.152, "ph": "X", "dur": 22.848641535898274, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884826.662, "ph": "X", "dur": 0.13565185262597543, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884827.209, "ph": "X", "dur": 0.25391244209477454, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884832.505, "ph": "X", "dur": 0.11217364736378738, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884832.199, "ph": "X", "dur": 0.49565099997952566, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884831.728, "ph": "X", "dur": 1.0434757894305804, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884832.972, "ph": "X", "dur": 19.13908510447256, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884830.039, "ph": "X", "dur": 22.151251883295505, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884827.12, "ph": "X", "dur": 25.221679393694984, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884853.311, "ph": "X", "dur": 0.13130403683668135, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884853.943, "ph": "X", "dur": 0.1956517105182338, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884857.221, "ph": "X", "dur": 0.19217345788679854, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884856.483, "ph": "X", "dur": 1.0078236999583687, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884855.871, "ph": "X", "dur": 1.6895612157196813, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884857.762, "ph": "X", "dur": 19.44691046235458, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884854.74, "ph": "X", "dur": 22.522555351701214, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884853.856, "ph": "X", "dur": 23.54168337271175, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884878.191, "ph": "X", "dur": 0.11304321052164619, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884878.705, "ph": "X", "dur": 0.20869515788611606, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884882.976, "ph": "X", "dur": 0.12347796841595199, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884882.651, "ph": "X", "dur": 0.5234770210310078, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884881.142, "ph": "X", "dur": 2.0973863367554664, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884883.44, "ph": "X", "dur": 17.52256719401302, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884879.648, "ph": "X", "dur": 21.403427567536916, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884878.619, "ph": "X", "dur": 22.539946614858394, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884902.073, "ph": "X", "dur": 0.113912773679505, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884902.605, "ph": "X", "dur": 0.21391253683326894, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884907.153, "ph": "X", "dur": 0.11999971578451672, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884906.192, "ph": "X", "dur": 1.159127689425803, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884905.471, "ph": "X", "dur": 1.9556475420244792, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884907.607, "ph": "X", "dur": 20.27647371495189, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884903.982, "ph": "X", "dur": 23.954725872694684, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884902.524, "ph": "X", "dur": 25.548635141049896, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884928.798, "ph": "X", "dur": 0.2417385578847511, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884929.456, "ph": "X", "dur": 0.2113038473596925, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884932.789, "ph": "X", "dur": 0.18260826315035156, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884932.52, "ph": "X", "dur": 0.5278248368203018, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884931.35, "ph": "X", "dur": 1.762604520979822, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884933.297, "ph": "X", "dur": 19.543431972876906, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884930.39, "ph": "X", "dur": 22.50429452538618, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884929.369, "ph": "X", "dur": 23.68168304112702, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884954.407, "ph": "X", "dur": 0.1339127263102578, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884955.0, "ph": "X", "dur": 0.2113038473596925, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884957.811, "ph": "X", "dur": 0.650433242078395, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884957.405, "ph": "X", "dur": 1.1356494841636149, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884956.805, "ph": "X", "dur": 1.8052131157149038, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884958.822, "ph": "X", "dur": 17.60082787822031, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884955.625, "ph": "X", "dur": 20.853863651770144, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884954.911, "ph": "X", "dur": 21.702557293840353, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884979.949, "ph": "X", "dur": 0.11217364736378738, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884980.516, "ph": "X", "dur": 0.27043414209409206, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884983.995, "ph": "X", "dur": 0.19391258420251617, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884983.609, "ph": "X", "dur": 0.6721723210248655, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884983.007, "ph": "X", "dur": 1.3417359525761545, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884984.545, "ph": "X", "dur": 24.782549998976283, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884981.765, "ph": "X", "dur": 27.62863021464819, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831884980.429, "ph": "X", "dur": 29.115583214586763, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885010.437, "ph": "X", "dur": 0.11304321052164619, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885010.979, "ph": "X", "dur": 0.22521685788543358, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885014.64, "ph": "X", "dur": 0.19739083683395142, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885014.26, "ph": "X", "dur": 0.6669549420777126, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885013.624, "ph": "X", "dur": 1.3808662946798014, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885015.209, "ph": "X", "dur": 18.21734815714221, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885011.735, "ph": "X", "dur": 21.756470209627597, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885010.884, "ph": "X", "dur": 22.73472876221877, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885034.415, "ph": "X", "dur": 0.10695626841663447, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885034.965, "ph": "X", "dur": 0.19912996314966908, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885038.689, "ph": "X", "dur": 0.18260826315035156, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885038.331, "ph": "X", "dur": 0.6165202789219012, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885037.562, "ph": "X", "dur": 1.4565182894135182, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885039.231, "ph": "X", "dur": 17.422567430859253, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885036.279, "ph": "X", "dur": 20.43908202547149, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885034.878, "ph": "X", "dur": 21.98255663067089, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885057.548, "ph": "X", "dur": 0.11217364736378738, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885058.074, "ph": "X", "dur": 0.13043447367882255, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885061.13, "ph": "X", "dur": 0.1860865157817868, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885060.785, "ph": "X", "dur": 0.607824647343313, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885060.066, "ph": "X", "dur": 1.3886923631005306, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885061.655, "ph": "X", "dur": 18.384304283451108, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885058.787, "ph": "X", "dur": 21.34342770964466, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885057.987, "ph": "X", "dur": 22.29299067802649, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885080.93, "ph": "X", "dur": 7.926068183883116, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885089.338, "ph": "X", "dur": 0.23999943156903344, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885093.354, "ph": "X", "dur": 0.17826044736105745, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885092.943, "ph": "X", "dur": 0.6643462526041362, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885092.238, "ph": "X", "dur": 1.433909647309189, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885093.877, "ph": "X", "dur": 24.407768277939134, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885090.9, "ph": "X", "dur": 27.459065398865718, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885089.245, "ph": "X", "dur": 29.25906113563347, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885119.84, "ph": "X", "dur": 0.11304321052164619, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885120.357, "ph": "X", "dur": 0.24869506314762163, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885124.062, "ph": "X", "dur": 0.18173869999249273, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885123.285, "ph": "X", "dur": 1.0339105946941334, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885122.739, "ph": "X", "dur": 1.6565178157210463, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885124.607, "ph": "X", "dur": 17.526045446644453, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885121.116, "ph": "X", "dur": 21.082558762287015, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885120.267, "ph": "X", "dur": 25.396461588424604, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885146.872, "ph": "X", "dur": 0.15130398946743415, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885147.53, "ph": "X", "dur": 0.13217359999454015, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885150.611, "ph": "X", "dur": 0.17826044736105745, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885150.188, "ph": "X", "dur": 0.6956505262870535, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885149.68, "ph": "X", "dur": 1.2852143473153315, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885151.162, "ph": "X", "dur": 17.024307504559918, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885148.153, "ph": "X", "dur": 20.114734967590152, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885147.427, "ph": "X", "dur": 20.950385162292473, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885169.583, "ph": "X", "dur": 0.12608665788952844, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885170.109, "ph": "X", "dur": 0.13043447367882255, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885173.489, "ph": "X", "dur": 0.18260826315035156, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885173.059, "ph": "X", "dur": 0.6930418368134771, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885172.472, "ph": "X", "dur": 1.3599967788911898, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885174.012, "ph": "X", "dur": 16.61822150983985, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885170.825, "ph": "X", "dur": 19.871257283389685, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885170.027, "ph": "X", "dur": 20.756472578089962, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885191.45, "ph": "X", "dur": 0.11304321052164619, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885192.46, "ph": "X", "dur": 0.19999952630752788, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885196.407, "ph": "X", "dur": 0.1808691368346339, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885195.602, "ph": "X", "dur": 1.064345305219192, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885194.471, "ph": "X", "dur": 2.2652120262222177, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885196.914, "ph": "X", "dur": 18.175609125564993, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885193.24, "ph": "X", "dur": 21.93038284119936, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885192.377, "ph": "X", "dur": 22.892989256949072, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885216.18, "ph": "X", "dur": 0.1286953473631049, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885216.698, "ph": "X", "dur": 0.2017386526232455, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885219.635, "ph": "X", "dur": 0.1808691368346339, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885219.268, "ph": "X", "dur": 0.6243463473426306, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885218.378, "ph": "X", "dur": 1.5852136367766232, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885220.141, "ph": "X", "dur": 22.514729283280484, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885217.434, "ph": "X", "dur": 25.27559230948223, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885216.614, "ph": "X", "dur": 26.212111830496173, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885243.698, "ph": "X", "dur": 0.11478233683736383, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885244.346, "ph": "X", "dur": 0.22347773156971593, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885247.49, "ph": "X", "dur": 0.13043447367882255, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885247.177, "ph": "X", "dur": 0.5208683315574314, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885246.171, "ph": "X", "dur": 1.599996210460223, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885248.017, "ph": "X", "dur": 20.215604293901777, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885245.178, "ph": "X", "dur": 23.135597377991683, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885244.136, "ph": "X", "dur": 24.29994244636464, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885269.108, "ph": "X", "dur": 0.1286953473631049, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885269.656, "ph": "X", "dur": 0.13217359999454015, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885272.785, "ph": "X", "dur": 0.10869539473235211, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885272.443, "ph": "X", "dur": 0.5313030894517371, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885271.868, "ph": "X", "dur": 1.1678233210043911, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885275.132, "ph": "X", "dur": 21.164297699125747, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885270.561, "ph": "X", "dur": 25.819938846301852, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885269.568, "ph": "X", "dur": 26.932110125203273, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885297.45, "ph": "X", "dur": 0.09826063683804631, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885298.097, "ph": "X", "dur": 0.21999947893828067, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885301.996, "ph": "X", "dur": 0.12521709473166964, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885301.623, "ph": "X", "dur": 0.5739116841868191, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885300.514, "ph": "X", "dur": 1.7547784525590926, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885302.464, "ph": "X", "dur": 19.66777950445072, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885299.27, "ph": "X", "dur": 22.919945714842694, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885298.006, "ph": "X", "dur": 24.3260293411004, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885323.116, "ph": "X", "dur": 0.1286953473631049, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885323.656, "ph": "X", "dur": 0.13217359999454015, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885326.39, "ph": "X", "dur": 0.11130408420592856, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885326.057, "ph": "X", "dur": 0.5199987683995725, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885325.415, "ph": "X", "dur": 1.2243449262652142, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885326.826, "ph": "X", "dur": 19.546040662350485, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885324.253, "ph": "X", "dur": 22.184295283294137, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885323.569, "ph": "X", "dur": 22.979076009577096, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885347.269, "ph": "X", "dur": 0.2356516157797394, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885347.996, "ph": "X", "dur": 0.16173874736173993, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885350.896, "ph": "X", "dur": 0.1808691368346339, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885350.546, "ph": "X", "dur": 0.607824647343313, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885350.085, "ph": "X", "dur": 1.154779873636509, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885351.435, "ph": "X", "dur": 18.69821658343814, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885348.669, "ph": "X", "dur": 21.527775099110727, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885347.909, "ph": "X", "dur": 22.381686120128087, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885370.922, "ph": "X", "dur": 0.12521709473166964, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885371.456, "ph": "X", "dur": 0.23043423683258646, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885375.818, "ph": "X", "dur": 0.2113038473596925, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885375.413, "ph": "X", "dur": 0.6930418368134771, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885373.819, "ph": "X", "dur": 2.364342226218123, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885376.355, "ph": "X", "dur": 19.892996362336156, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885372.189, "ph": "X", "dur": 24.1234211253193, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885371.37, "ph": "X", "dur": 25.057331956859667, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885397.737, "ph": "X", "dur": 0.12434753157381082, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885398.269, "ph": "X", "dur": 0.23043423683258646, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885401.709, "ph": "X", "dur": 0.19130389472893972, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885401.235, "ph": "X", "dur": 0.7782590262836411, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885400.664, "ph": "X", "dur": 1.4191270736255892, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885402.28, "ph": "X", "dur": 19.45734522024889, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885399.024, "ph": "X", "dur": 22.77733735695385, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885398.18, "ph": "X", "dur": 23.751248093755724, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885422.629, "ph": "X", "dur": 0.11217364736378738, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885423.14, "ph": "X", "dur": 0.21999947893828067, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885426.442, "ph": "X", "dur": 0.17652132104533982, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885425.778, "ph": "X", "dur": 0.9173891315410518, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885425.274, "ph": "X", "dur": 3.4469483577523503, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885428.969, "ph": "X", "dur": 18.795607657118325, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885423.919, "ph": "X", "dur": 23.92429116216963, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885423.052, "ph": "X", "dur": 24.9173322884444, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885448.743, "ph": "X", "dur": 0.1286953473631049, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885449.278, "ph": "X", "dur": 0.38956429472074994, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885689.101, "ph": "X", "dur": 0.29565147367199773, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885688.087, "ph": "X", "dur": 1.4391270262563418, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885687.483, "ph": "X", "dur": 2.152168815700572, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885690.09, "ph": "X", "dur": 20.515603583363067, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885450.311, "ph": "X", "dur": 260.40633975766417, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885449.189, "ph": "X", "dur": 261.7976408102383, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885712.041, "ph": "X", "dur": 0.13913010525741068, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885712.774, "ph": "X", "dur": 0.2330429263061629, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885716.23, "ph": "X", "dur": 0.17217350525604572, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885715.743, "ph": "X", "dur": 0.7382591210221355, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885715.052, "ph": "X", "dur": 1.4982573209907415, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885716.782, "ph": "X", "dur": 18.58865162554793, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885713.672, "ph": "X", "dur": 21.768644093837622, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885712.656, "ph": "X", "dur": 22.927771783263427, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885736.343, "ph": "X", "dur": 0.2008690894653867, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885736.976, "ph": "X", "dur": 0.19999952630752788, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885740.4, "ph": "X", "dur": 0.2060864684125396, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885739.599, "ph": "X", "dur": 1.0834756946920858, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885739.015, "ph": "X", "dur": 1.7365176262440574, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885740.911, "ph": "X", "dur": 17.207785330868127, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885737.697, "ph": "X", "dur": 20.50516882546876, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885736.889, "ph": "X", "dur": 21.403427567536916, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885759.411, "ph": "X", "dur": 0.11478233683736383, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885759.902, "ph": "X", "dur": 0.2017386526232455, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885763.775, "ph": "X", "dur": 0.17652132104533982, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885762.917, "ph": "X", "dur": 1.1113017157435678, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885762.237, "ph": "X", "dur": 1.8626042841335857, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885764.293, "ph": "X", "dur": 21.42168839385195, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885760.549, "ph": "X", "dur": 25.24689672527289, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885759.816, "ph": "X", "dur": 26.098199056816668, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885786.7, "ph": "X", "dur": 0.34434701051209143, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885787.461, "ph": "X", "dur": 0.2504341894633393, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885791.187, "ph": "X", "dur": 0.18347782630821036, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885790.879, "ph": "X", "dur": 0.5773899368182545, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885790.378, "ph": "X", "dur": 1.1530407473207913, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885791.71, "ph": "X", "dur": 19.039954904476655, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885788.399, "ph": "X", "dur": 22.43125122012604, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885787.373, "ph": "X", "dur": 23.55733550955321, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885811.637, "ph": "X", "dur": 0.12347796841595199, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885812.149, "ph": "X", "dur": 0.2739123947255273, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885818.794, "ph": "X", "dur": 0.19391258420251617, "name": "list.remove", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885818.291, "ph": "X", "dur": 0.7739112104943471, "name": "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885817.367, "ph": "X", "dur": 1.7695610262426924, "name": "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885819.335, "ph": "X", "dur": 19.87908335181041, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885815.603, "ph": "X", "dur": 23.689509109547746, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885812.067, "ph": "X", "dur": 27.350370004133367, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885840.237, "ph": "X", "dur": 0.1860865157817868, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885840.851, "ph": "X", "dur": 0.7799981525993587, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885844.317, "ph": "X", "dur": 34.34774473542326, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885842.217, "ph": "X", "dur": 36.54165258270106, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885840.76, "ph": "X", "dur": 38.139040103687705, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885879.704, "ph": "X", "dur": 0.09826063683804631, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885880.206, "ph": "X", "dur": 0.41478162629865567, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885883.14, "ph": "X", "dur": 45.601631124432075, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885881.176, "ph": "X", "dur": 47.632061098032416, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885880.124, "ph": "X", "dur": 48.79640616640537, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885929.57, "ph": "X", "dur": 0.16086918420388113, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885930.123, "ph": "X", "dur": 0.5904333841861367, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885932.631, "ph": "X", "dur": 23.133858251675964, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885931.209, "ph": "X", "dur": 24.61994168845668, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885930.04, "ph": "X", "dur": 25.890373462088412, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885956.962, "ph": "X", "dur": 0.11913015262665792, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885957.449, "ph": "X", "dur": 0.22173860525399833, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885959.745, "ph": "X", "dur": 147.76921523073804, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885958.022, "ph": "X", "dur": 149.5857326675051, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831885957.368, "ph": "X", "dur": 150.36051344115734, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886108.848, "ph": "X", "dur": 0.16782568946675167, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886109.533, "ph": "X", "dur": 0.5713029947132428, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886115.235, "ph": "X", "dur": 67.96244772877155, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886110.693, "ph": "X", "dur": 72.61548018647407, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886109.423, "ph": "X", "dur": 74.05634633904614, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886184.507, "ph": "X", "dur": 0.2626080736733627, "name": "list.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886185.25, "ph": "X", "dur": 0.2504341894633393, "name": "dict.pop", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886187.321, "ph": "X", "dur": 13.853010667848812, "name": "torch._C.reset", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886186.222, "ph": "X", "dur": 15.014747046748191, "name": "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886185.14, "ph": "X", "dur": 16.24257022564484, "name": "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886205.228, "ph": "X", "dur": 0.717389605233524, "name": "gc.enable", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875979.447, "ph": "X", "dur": 10226.657517619662, "name": "finalize._exitfunc (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:641)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886216.052, "ph": "X", "dur": 0.8043459210194056, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886215.155, "ph": "X", "dur": 1.842604331502833, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886217.92, "ph": "X", "dur": 0.06869548947084654, "name": "Manager.disable (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1369)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886218.474, "ph": "X", "dur": 0.6139115894483247, "name": "Logger.getEffectiveLevel (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1776)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886219.952, "ph": "X", "dur": 0.29304278419842134, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886219.822, "ph": "X", "dur": 0.5113031368209843, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886212.652, "ph": "X", "dur": 8.315632478603867, "name": "Logger.isEnabledFor (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1790)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886212.108, "ph": "X", "dur": 9.162586994358353, "name": "Logger.info (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1529)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886224.565, "ph": "X", "dur": 0.22347773156971593, "name": "Logger.isEnabledFor (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1790)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886224.405, "ph": "X", "dur": 3.019123284085812, "name": "Logger.info (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1529)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886228.441, "ph": "X", "dur": 0.08608675262802286, "name": "Logger.isEnabledFor (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1790)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886228.293, "ph": "X", "dur": 0.3052166684084447, "name": "Logger.info (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1529)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886209.847, "ph": "X", "dur": 20.166039193903824, "name": "dump_cache_stats (/data/wangyingqi/code/pytorch/torch/_subclasses/fake_tensor.py:3178)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886231.596, "ph": "X", "dur": 0.5956507631332896, "name": "_set_python_exit_flag (/data/wangyingqi/code/pytorch/torch/utils/data/_utils/__init__.py:45)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886245.542, "ph": "X", "dur": 0.28173846314625667, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886243.82, "ph": "X", "dur": 2.232168626223583, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886246.395, "ph": "X", "dur": 0.9243456368039223, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886249.745, "ph": "X", "dur": 0.13043447367882255, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886249.15, "ph": "X", "dur": 0.8495632052280642, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886253.106, "ph": "X", "dur": 0.13913010525741068, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886251.866, "ph": "X", "dur": 1.5513006736201294, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886248.905, "ph": "X", "dur": 4.677380226122576, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886255.876, "ph": "X", "dur": 0.1078258315744933, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886255.719, "ph": "X", "dur": 0.34608613682780914, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886259.636, "ph": "X", "dur": 0.13130403683668135, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886259.533, "ph": "X", "dur": 0.30869492103988, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886261.09, "ph": "X", "dur": 0.1339127263102578, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886260.992, "ph": "X", "dur": 0.30869492103988, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886259.078, "ph": "X", "dur": 2.324342320956618, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886261.864, "ph": "X", "dur": 0.09739107368018748, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886261.693, "ph": "X", "dur": 0.34347744735423263, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886255.429, "ph": "X", "dur": 6.792157826035219, "name": "LazyTraceHandler.close (/data/wangyingqi/code/pytorch/torch/_logging/_internal.py:1077)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886262.659, "ph": "X", "dur": 0.09652151052232867, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886262.559, "ph": "X", "dur": 0.27826021051482136, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886264.804, "ph": "X", "dur": 0.15652136841458705, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886264.261, "ph": "X", "dur": 0.7747807736522059, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886265.274, "ph": "X", "dur": 0.27652108419910376, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886266.29, "ph": "X", "dur": 0.12086927894237555, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886266.15, "ph": "X", "dur": 0.33652094209136213, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886267.422, "ph": "X", "dur": 0.9834759315383219, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886268.647, "ph": "X", "dur": 1.7939087946627392, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886271.036, "ph": "X", "dur": 0.08695631578588169, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886270.903, "ph": "X", "dur": 0.3043471052505859, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886265.974, "ph": "X", "dur": 5.319987399780241, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886271.892, "ph": "X", "dur": 0.13565185262597543, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886271.792, "ph": "X", "dur": 0.3104340473555976, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886273.075, "ph": "X", "dur": 0.0930432578908934, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886272.99, "ph": "X", "dur": 0.25391244209477454, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886271.522, "ph": "X", "dur": 1.8199956893985039, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886273.657, "ph": "X", "dur": 0.08869544210159933, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886273.516, "ph": "X", "dur": 0.3034775420927271, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886274.462, "ph": "X", "dur": 0.2365211789375982, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886274.179, "ph": "X", "dur": 0.5930420736597131, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886274.879, "ph": "X", "dur": 0.15043442630957532, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886275.437, "ph": "X", "dur": 0.12173884210023436, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886275.312, "ph": "X", "dur": 0.32347749472347986, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886277.816, "ph": "X", "dur": 0.17913001051891628, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886278.429, "ph": "X", "dur": 0.23912986841117465, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886278.968, "ph": "X", "dur": 0.08521718947016406, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886278.842, "ph": "X", "dur": 0.2860862789355508, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886275.193, "ph": "X", "dur": 4.001729652466275, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886279.592, "ph": "X", "dur": 0.13043447367882255, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886279.495, "ph": "X", "dur": 0.3026079789348683, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886280.245, "ph": "X", "dur": 0.08086937368086997, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886280.141, "ph": "X", "dur": 0.2582602578840686, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886279.358, "ph": "X", "dur": 1.1182582210064387, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886280.723, "ph": "X", "dur": 0.08869544210159933, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886280.599, "ph": "X", "dur": 0.2843471526198331, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886281.565, "ph": "X", "dur": 0.24695593683190398, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886281.27, "ph": "X", "dur": 0.6165202789219012, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886282.002, "ph": "X", "dur": 0.10956495789021094, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886282.485, "ph": "X", "dur": 0.11999971578451672, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886282.361, "ph": "X", "dur": 0.31739055261846816, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886282.821, "ph": "X", "dur": 0.13478228946811663, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886283.097, "ph": "X", "dur": 0.15217355262529295, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886283.546, "ph": "X", "dur": 0.08347806315444642, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886283.409, "ph": "X", "dur": 0.29652103682985653, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886282.244, "ph": "X", "dur": 1.5321702841472353, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886284.133, "ph": "X", "dur": 0.13565185262597543, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886284.038, "ph": "X", "dur": 0.3078253578820212, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886284.616, "ph": "X", "dur": 0.08086937368086997, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886284.52, "ph": "X", "dur": 0.25304287893691574, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886283.923, "ph": "X", "dur": 0.9252151999617811, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886285.082, "ph": "X", "dur": 0.09043456841731695, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886284.97, "ph": "X", "dur": 0.27826021051482136, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886286.337, "ph": "X", "dur": 0.22521685788543358, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886285.616, "ph": "X", "dur": 1.020867147326251, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886286.72, "ph": "X", "dur": 0.08173893683872879, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886287.163, "ph": "X", "dur": 0.12260840525809319, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886287.038, "ph": "X", "dur": 0.32347749472347986, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886287.47, "ph": "X", "dur": 0.11478233683736383, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886287.69, "ph": "X", "dur": 0.14956486315171652, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886288.12, "ph": "X", "dur": 0.08347806315444642, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886287.996, "ph": "X", "dur": 0.28260802630411547, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886286.921, "ph": "X", "dur": 1.424344452572742, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886288.708, "ph": "X", "dur": 0.133043163152399, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886288.612, "ph": "X", "dur": 0.3043471052505859, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886289.651, "ph": "X", "dur": 0.07999981052301115, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886289.555, "ph": "X", "dur": 0.24869506314762163, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886288.497, "ph": "X", "dur": 1.3799967315219424, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886290.113, "ph": "X", "dur": 0.08956500525945814, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886290.002, "ph": "X", "dur": 0.27652108419910376, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886292.84, "ph": "X", "dur": 0.14695617367814007, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886292.538, "ph": "X", "dur": 0.5252161473467254, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886293.165, "ph": "X", "dur": 0.08434762631230523, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886293.622, "ph": "X", "dur": 0.11913015262665792, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886293.496, "ph": "X", "dur": 0.32173836840776227, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886293.948, "ph": "X", "dur": 0.1182605894687991, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886294.236, "ph": "X", "dur": 0.15739093157244585, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886294.672, "ph": "X", "dur": 0.08347806315444642, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886294.55, "ph": "X", "dur": 0.28173846314625667, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886293.379, "ph": "X", "dur": 1.519996399937212, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886295.254, "ph": "X", "dur": 0.133043163152399, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886295.15, "ph": "X", "dur": 0.3130427368291741, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886295.79, "ph": "X", "dur": 0.08173893683872879, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886295.696, "ph": "X", "dur": 0.2504341894633393, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886295.037, "ph": "X", "dur": 0.9808672420647455, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886296.255, "ph": "X", "dur": 0.08608675262802286, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886296.141, "ph": "X", "dur": 0.27478195788338616, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886297.419, "ph": "X", "dur": 0.23912986841117465, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886296.685, "ph": "X", "dur": 1.0504322946934508, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886297.804, "ph": "X", "dur": 0.08782587894374051, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886298.259, "ph": "X", "dur": 0.11913015262665792, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886298.133, "ph": "X", "dur": 0.3199992420920446, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886298.552, "ph": "X", "dur": 0.11565189999522264, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886298.775, "ph": "X", "dur": 0.15304311578315177, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886299.212, "ph": "X", "dur": 0.08434762631230523, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886299.088, "ph": "X", "dur": 0.2843471526198331, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886298.012, "ph": "X", "dur": 1.4313009578356124, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886299.792, "ph": "X", "dur": 0.13217359999454015, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886299.695, "ph": "X", "dur": 0.3026079789348683, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886300.323, "ph": "X", "dur": 0.07913024736515234, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886300.228, "ph": "X", "dur": 0.24956462630548046, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886299.579, "ph": "X", "dur": 0.9695629210125808, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886300.785, "ph": "X", "dur": 0.08608675262802286, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886300.673, "ph": "X", "dur": 0.27217326840980965, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886301.855, "ph": "X", "dur": 0.22434729472757475, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886301.289, "ph": "X", "dur": 0.8669544683852404, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886302.242, "ph": "X", "dur": 0.08086937368086997, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886302.679, "ph": "X", "dur": 0.1182605894687991, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886302.565, "ph": "X", "dur": 0.3078253578820212, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886302.975, "ph": "X", "dur": 0.11304321052164619, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886303.182, "ph": "X", "dur": 0.15304311578315177, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886303.621, "ph": "X", "dur": 0.08695631578588169, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886303.495, "ph": "X", "dur": 0.28869496840912723, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886302.447, "ph": "X", "dur": 1.406083626257707, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886304.204, "ph": "X", "dur": 0.1339127263102578, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886304.107, "ph": "X", "dur": 0.30608623156630355, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886304.665, "ph": "X", "dur": 0.09130413157517578, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886304.572, "ph": "X", "dur": 1.9339084630780088, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886303.993, "ph": "X", "dur": 2.5904286472614153, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886306.836, "ph": "X", "dur": 0.09565194736446986, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886306.721, "ph": "X", "dur": 0.2860862789355508, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886307.601, "ph": "X", "dur": 0.23478205262188057, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886307.35, "ph": "X", "dur": 0.5591291105032193, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886307.985, "ph": "X", "dur": 0.0826084999965876, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886308.419, "ph": "X", "dur": 0.10956495789021094, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886308.307, "ph": "X", "dur": 0.29739059998771533, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886308.705, "ph": "X", "dur": 0.1182605894687991, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886308.918, "ph": "X", "dur": 0.15913005788816348, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886309.364, "ph": "X", "dur": 0.09739107368018748, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886309.237, "ph": "X", "dur": 0.29912972630343304, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886308.189, "ph": "X", "dur": 1.414779257836295, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886309.958, "ph": "X", "dur": 0.1339127263102578, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886309.864, "ph": "X", "dur": 0.3043471052505859, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886310.452, "ph": "X", "dur": 0.09130413157517578, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886310.365, "ph": "X", "dur": 0.25391244209477454, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886309.748, "ph": "X", "dur": 0.9434760262768164, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886310.927, "ph": "X", "dur": 0.08695631578588169, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886310.816, "ph": "X", "dur": 0.27217326840980965, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886311.639, "ph": "X", "dur": 0.2269559842011512, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886311.444, "ph": "X", "dur": 0.49825968945310206, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886312.025, "ph": "X", "dur": 0.0826084999965876, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886312.468, "ph": "X", "dur": 0.11043452104806974, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886312.345, "ph": "X", "dur": 0.30869492103988, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886312.779, "ph": "X", "dur": 0.113912773679505, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886312.987, "ph": "X", "dur": 0.15130398946743415, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886313.408, "ph": "X", "dur": 0.09739107368018748, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886313.297, "ph": "X", "dur": 0.2852167157776919, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886312.228, "ph": "X", "dur": 1.4234748894148832, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886314.007, "ph": "X", "dur": 0.133043163152399, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886313.911, "ph": "X", "dur": 0.30608623156630355, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886314.464, "ph": "X", "dur": 0.0930432578908934, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886314.376, "ph": "X", "dur": 0.25565156841049214, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886313.797, "ph": "X", "dur": 0.906954373646746, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886314.938, "ph": "X", "dur": 0.09043456841731695, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886314.826, "ph": "X", "dur": 0.27652108419910376, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886315.575, "ph": "X", "dur": 0.14434748420456361, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886315.385, "ph": "X", "dur": 0.4078251210357851, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886315.868, "ph": "X", "dur": 0.0826084999965876, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886316.311, "ph": "X", "dur": 0.11130408420592856, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886316.185, "ph": "X", "dur": 0.3104340473555976, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886316.602, "ph": "X", "dur": 0.11304321052164619, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886316.808, "ph": "X", "dur": 0.1539126789410106, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886317.232, "ph": "X", "dur": 0.09739107368018748, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886317.121, "ph": "X", "dur": 1.9321693367622912, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886316.069, "ph": "X", "dur": 3.0556449367158827, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886319.488, "ph": "X", "dur": 0.1339127263102578, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886319.392, "ph": "X", "dur": 0.30608623156630355, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886319.947, "ph": "X", "dur": 0.09391282104875222, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886319.863, "ph": "X", "dur": 0.24869506314762163, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886319.275, "ph": "X", "dur": 0.9199978210146282, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886320.428, "ph": "X", "dur": 0.08782587894374051, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886320.317, "ph": "X", "dur": 0.2730428315676685, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886321.08, "ph": "X", "dur": 0.2321733631483041, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886320.957, "ph": "X", "dur": 0.4304337631401144, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886321.464, "ph": "X", "dur": 0.0826084999965876, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886321.914, "ph": "X", "dur": 0.10956495789021094, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886321.788, "ph": "X", "dur": 0.3104340473555976, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886322.197, "ph": "X", "dur": 0.11913015262665792, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886322.41, "ph": "X", "dur": 0.15130398946743415, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886322.83, "ph": "X", "dur": 0.09652151052232867, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886322.719, "ph": "X", "dur": 0.28173846314625667, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886321.673, "ph": "X", "dur": 1.3947793052055422, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886323.418, "ph": "X", "dur": 0.133043163152399, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886323.324, "ph": "X", "dur": 0.30173841577700944, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886323.881, "ph": "X", "dur": 0.0930432578908934, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886323.796, "ph": "X", "dur": 0.2513037526211981, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886323.211, "ph": "X", "dur": 0.9217369473303458, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886324.369, "ph": "X", "dur": 0.08956500525945814, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886324.256, "ph": "X", "dur": 0.27565152104124496, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886324.995, "ph": "X", "dur": 0.2356516157797394, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886324.871, "ph": "X", "dur": 0.43391201577154964, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886325.385, "ph": "X", "dur": 0.07999981052301115, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886325.829, "ph": "X", "dur": 0.12086927894237555, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886325.706, "ph": "X", "dur": 0.3208688052499034, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886326.141, "ph": "X", "dur": 0.11478233683736383, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886326.398, "ph": "X", "dur": 0.15043442630957532, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886326.827, "ph": "X", "dur": 0.08434762631230523, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886326.711, "ph": "X", "dur": 0.27565152104124496, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886325.591, "ph": "X", "dur": 1.4634747946763889, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886327.411, "ph": "X", "dur": 0.13565185262597543, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886327.315, "ph": "X", "dur": 0.3078253578820212, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886327.876, "ph": "X", "dur": 0.08086937368086997, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886327.787, "ph": "X", "dur": 0.24347768420046872, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886327.202, "ph": "X", "dur": 0.9034761210153107, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886328.343, "ph": "X", "dur": 0.08869544210159933, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886328.228, "ph": "X", "dur": 0.27739064735696256, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886328.99, "ph": "X", "dur": 0.24086899472689227, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886328.783, "ph": "X", "dur": 0.522607457873149, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886329.378, "ph": "X", "dur": 0.08086937368086997, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886329.822, "ph": "X", "dur": 0.11913015262665792, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886329.698, "ph": "X", "dur": 2.4356464051625464, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886332.252, "ph": "X", "dur": 0.12173884210023436, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886332.481, "ph": "X", "dur": 0.1539126789410106, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886332.92, "ph": "X", "dur": 0.08521718947016406, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886332.795, "ph": "X", "dur": 0.2869558420934096, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886329.579, "ph": "X", "dur": 3.5721654524840196, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886333.515, "ph": "X", "dur": 0.13217359999454015, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886333.418, "ph": "X", "dur": 0.3043471052505859, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886333.967, "ph": "X", "dur": 0.07652155789157589, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886333.883, "ph": "X", "dur": 0.2356516157797394, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886333.301, "ph": "X", "dur": 0.8921717999631461, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886334.428, "ph": "X", "dur": 0.09043456841731695, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886334.317, "ph": "X", "dur": 0.27652108419910376, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886335.087, "ph": "X", "dur": 0.14260835788884596, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886334.888, "ph": "X", "dur": 0.41478162629865567, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886335.371, "ph": "X", "dur": 0.08173893683872879, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886335.797, "ph": "X", "dur": 0.11999971578451672, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886335.683, "ph": "X", "dur": 0.3095644841977388, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886336.088, "ph": "X", "dur": 0.113912773679505, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886336.298, "ph": "X", "dur": 0.15130398946743415, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886336.733, "ph": "X", "dur": 0.08347806315444642, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886336.61, "ph": "X", "dur": 0.28086889998839787, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886335.567, "ph": "X", "dur": 1.4121705683627186, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886337.328, "ph": "X", "dur": 0.13217359999454015, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886337.231, "ph": "X", "dur": 0.3034775420927271, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886337.785, "ph": "X", "dur": 0.08173893683872879, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886337.698, "ph": "X", "dur": 0.24260812104260993, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886337.116, "ph": "X", "dur": 0.8999978683838755, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886338.254, "ph": "X", "dur": 0.09130413157517578, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886338.138, "ph": "X", "dur": 0.28173846314625667, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886339.205, "ph": "X", "dur": 0.22608642104329238, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886339.003, "ph": "X", "dur": 0.5043466315581138, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886339.586, "ph": "X", "dur": 0.0826084999965876, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886340.032, "ph": "X", "dur": 0.11913015262665792, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886339.907, "ph": "X", "dur": 0.31826011577632696, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886340.321, "ph": "X", "dur": 0.11043452104806974, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886340.525, "ph": "X", "dur": 0.14260835788884596, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886340.948, "ph": "X", "dur": 0.0826084999965876, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886340.824, "ph": "X", "dur": 0.27999933683053907, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886339.79, "ph": "X", "dur": 1.3843445473112366, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886341.53, "ph": "X", "dur": 0.13130403683668135, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886341.435, "ph": "X", "dur": 0.30086885261915064, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886341.978, "ph": "X", "dur": 0.08173893683872879, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886341.891, "ph": "X", "dur": 0.24347768420046872, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886341.321, "ph": "X", "dur": 0.8860848578581344, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886342.443, "ph": "X", "dur": 0.08869544210159933, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886342.328, "ph": "X", "dur": 2.1860817788570657, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886345.098, "ph": "X", "dur": 0.25391244209477454, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886344.804, "ph": "X", "dur": 0.6208680947111952, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886345.498, "ph": "X", "dur": 0.08608675262802286, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886345.947, "ph": "X", "dur": 0.11913015262665792, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886345.824, "ph": "X", "dur": 0.31739055261846816, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886346.27, "ph": "X", "dur": 0.11565189999522264, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886346.479, "ph": "X", "dur": 0.14434748420456361, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886346.904, "ph": "X", "dur": 0.07999981052301115, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886346.782, "ph": "X", "dur": 0.27912977367268027, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886345.706, "ph": "X", "dur": 1.4217357630991656, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886347.489, "ph": "X", "dur": 0.1286953473631049, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886347.391, "ph": "X", "dur": 0.3034775420927271, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886347.949, "ph": "X", "dur": 0.09043456841731695, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886347.873, "ph": "X", "dur": 0.24260812104260993, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886347.275, "ph": "X", "dur": 0.9130413157517577, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886348.435, "ph": "X", "dur": 0.09043456841731695, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886348.311, "ph": "X", "dur": 0.28869496840912723, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886349.181, "ph": "X", "dur": 0.14260835788884596, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886348.871, "ph": "X", "dur": 0.5278248368203018, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886349.481, "ph": "X", "dur": 0.08086937368086997, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886349.924, "ph": "X", "dur": 0.11739102631094028, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886349.801, "ph": "X", "dur": 0.3147818631448917, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886350.239, "ph": "X", "dur": 0.11478233683736383, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886350.448, "ph": "X", "dur": 0.1556518052567282, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886350.884, "ph": "X", "dur": 0.09565194736446986, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886350.762, "ph": "X", "dur": 0.2913036578827036, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886349.682, "ph": "X", "dur": 1.4391270262563418, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886351.47, "ph": "X", "dur": 0.13217359999454015, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886351.375, "ph": "X", "dur": 0.30086885261915064, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886352.041, "ph": "X", "dur": 0.07913024736515234, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886351.955, "ph": "X", "dur": 0.2417385578847511, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886351.264, "ph": "X", "dur": 1.00695413680051, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886352.503, "ph": "X", "dur": 0.08782587894374051, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886352.391, "ph": "X", "dur": 0.27217326840980965, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886353.309, "ph": "X", "dur": 0.1460866105202812, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886353.002, "ph": "X", "dur": 0.5295639631360194, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886353.601, "ph": "X", "dur": 0.08434762631230523, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886354.031, "ph": "X", "dur": 0.1182605894687991, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886353.92, "ph": "X", "dur": 0.3043471052505859, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886354.333, "ph": "X", "dur": 0.11304321052164619, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886354.542, "ph": "X", "dur": 0.14956486315171652, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886354.97, "ph": "X", "dur": 0.08521718947016406, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886354.849, "ph": "X", "dur": 0.28086889998839787, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886353.803, "ph": "X", "dur": 1.4026053736262716, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886355.557, "ph": "X", "dur": 0.13130403683668135, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886355.46, "ph": "X", "dur": 0.30173841577700944, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886358.173, "ph": "X", "dur": 0.07999981052301115, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886358.071, "ph": "X", "dur": 0.2582602578840686, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886355.344, "ph": "X", "dur": 3.0460797419794354, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886358.629, "ph": "X", "dur": 0.08869544210159933, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886358.515, "ph": "X", "dur": 0.27826021051482136, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886359.427, "ph": "X", "dur": 0.30695579472416235, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886359.151, "ph": "X", "dur": 0.6565201841834067, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886359.897, "ph": "X", "dur": 0.08434762631230523, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886360.334, "ph": "X", "dur": 0.1182605894687991, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886360.221, "ph": "X", "dur": 0.30608623156630355, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886360.634, "ph": "X", "dur": 0.11565189999522264, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886360.844, "ph": "X", "dur": 0.1434779210467048, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886361.268, "ph": "X", "dur": 0.08521718947016406, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886361.143, "ph": "X", "dur": 0.2852167157776919, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886360.103, "ph": "X", "dur": 1.3939097420476836, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886361.845, "ph": "X", "dur": 0.13130403683668135, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886361.75, "ph": "X", "dur": 0.30173841577700944, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886362.398, "ph": "X", "dur": 0.07913024736515234, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886362.313, "ph": "X", "dur": 0.23826030525331585, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886361.635, "ph": "X", "dur": 0.9869541841697571, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886362.884, "ph": "X", "dur": 0.08695631578588169, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886362.76, "ph": "X", "dur": 0.2869558420934096, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886363.568, "ph": "X", "dur": 0.2365211789375982, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886363.339, "ph": "X", "dur": 0.5399987210303253, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886363.965, "ph": "X", "dur": 0.08347806315444642, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886364.397, "ph": "X", "dur": 0.11999971578451672, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886364.287, "ph": "X", "dur": 0.30608623156630355, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886364.69, "ph": "X", "dur": 0.11217364736378738, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886364.896, "ph": "X", "dur": 0.14086923157312833, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886365.315, "ph": "X", "dur": 0.08347806315444642, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886365.191, "ph": "X", "dur": 0.28260802630411547, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886364.17, "ph": "X", "dur": 1.372170663101213, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886365.896, "ph": "X", "dur": 0.13130403683668135, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886365.8, "ph": "X", "dur": 0.29999928946129184, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886366.347, "ph": "X", "dur": 0.08086937368086997, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886366.261, "ph": "X", "dur": 0.2417385578847511, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886365.685, "ph": "X", "dur": 0.8791283525952639, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886366.803, "ph": "X", "dur": 0.08869544210159933, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886366.688, "ph": "X", "dur": 0.27826021051482136, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886367.519, "ph": "X", "dur": 0.23043423683258646, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886367.307, "ph": "X", "dur": 0.5182596420838548, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886367.897, "ph": "X", "dur": 0.08347806315444642, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886368.341, "ph": "X", "dur": 0.11652146315308146, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886368.219, "ph": "X", "dur": 0.3139122999870329, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886368.631, "ph": "X", "dur": 0.11304321052164619, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886368.838, "ph": "X", "dur": 0.13999966841526953, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886370.959, "ph": "X", "dur": 0.08434762631230523, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886370.835, "ph": "X", "dur": 0.2843471526198331, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886368.102, "ph": "X", "dur": 3.0852100840830823, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886371.544, "ph": "X", "dur": 0.13565185262597543, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886371.447, "ph": "X", "dur": 0.30695579472416235, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886372.027, "ph": "X", "dur": 0.07652155789157589, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886371.931, "ph": "X", "dur": 0.24608637367404518, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886371.329, "ph": "X", "dur": 0.9191282578567694, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886372.495, "ph": "X", "dur": 0.08869544210159933, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886372.384, "ph": "X", "dur": 0.2730428315676685, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886373.691, "ph": "X", "dur": 0.22956467367472766, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886373.464, "ph": "X", "dur": 0.5356509052410312, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886374.088, "ph": "X", "dur": 0.08869544210159933, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886374.536, "ph": "X", "dur": 0.11999971578451672, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886374.411, "ph": "X", "dur": 0.3191296789341858, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886374.835, "ph": "X", "dur": 0.11913015262665792, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886375.048, "ph": "X", "dur": 0.1452170473624224, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886375.471, "ph": "X", "dur": 0.07565199473371707, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886375.351, "ph": "X", "dur": 0.2678254526205156, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886374.295, "ph": "X", "dur": 1.4095618788891422, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886376.049, "ph": "X", "dur": 0.133043163152399, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886375.953, "ph": "X", "dur": 0.3043471052505859, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886376.574, "ph": "X", "dur": 0.0773911210494347, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886376.478, "ph": "X", "dur": 0.24869506314762163, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886375.839, "ph": "X", "dur": 0.9626064157497103, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886377.038, "ph": "X", "dur": 0.08695631578588169, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886376.927, "ph": "X", "dur": 0.2739123947255273, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886377.682, "ph": "X", "dur": 0.1434779210467048, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886377.471, "ph": "X", "dur": 0.4286946368243967, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886377.97, "ph": "X", "dur": 0.08521718947016406, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886378.417, "ph": "X", "dur": 0.11652146315308146, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886378.293, "ph": "X", "dur": 0.3147818631448917, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886378.709, "ph": "X", "dur": 0.11304321052164619, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886378.917, "ph": "X", "dur": 0.14260835788884596, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886379.337, "ph": "X", "dur": 0.08347806315444642, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886379.215, "ph": "X", "dur": 0.27999933683053907, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886378.176, "ph": "X", "dur": 1.3860836736269542, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886379.918, "ph": "X", "dur": 0.13565185262597543, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886379.822, "ph": "X", "dur": 0.3052166684084447, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886380.383, "ph": "X", "dur": 0.07913024736515234, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886380.288, "ph": "X", "dur": 0.24782549998976283, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886379.709, "ph": "X", "dur": 0.8999978683838755, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886380.846, "ph": "X", "dur": 0.08695631578588169, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886380.733, "ph": "X", "dur": 0.27478195788338616, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886381.464, "ph": "X", "dur": 0.2330429263061629, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886381.271, "ph": "X", "dur": 0.5008683789266786, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886381.839, "ph": "X", "dur": 0.0826084999965876, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886384.067, "ph": "X", "dur": 0.11999971578451672, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886383.939, "ph": "X", "dur": 0.32434705788133866, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886384.368, "ph": "X", "dur": 0.11565189999522264, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886384.581, "ph": "X", "dur": 0.14434748420456361, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886385.007, "ph": "X", "dur": 0.08347806315444642, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886384.885, "ph": "X", "dur": 0.27999933683053907, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886383.817, "ph": "X", "dur": 1.415648820994154, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886385.597, "ph": "X", "dur": 0.13478228946811663, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886385.501, "ph": "X", "dur": 0.30608623156630355, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886386.148, "ph": "X", "dur": 0.0826084999965876, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886386.05, "ph": "X", "dur": 0.25565156841049214, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886385.383, "ph": "X", "dur": 0.9991280683797806, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886386.617, "ph": "X", "dur": 0.08956500525945814, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886386.504, "ph": "X", "dur": 0.27826021051482136, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886387.751, "ph": "X", "dur": 0.22434729472757475, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886387.065, "ph": "X", "dur": 0.9843454946961807, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886388.138, "ph": "X", "dur": 0.08521718947016406, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886388.587, "ph": "X", "dur": 0.11913015262665792, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886388.462, "ph": "X", "dur": 0.3191296789341858, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886388.88, "ph": "X", "dur": 0.11043452104806974, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886389.088, "ph": "X", "dur": 0.13999966841526953, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886389.51, "ph": "X", "dur": 0.08347806315444642, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886389.384, "ph": "X", "dur": 0.28260802630411547, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886388.344, "ph": "X", "dur": 1.3904314894162482, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886390.081, "ph": "X", "dur": 0.13217359999454015, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886389.986, "ph": "X", "dur": 0.30086885261915064, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886390.544, "ph": "X", "dur": 0.07826068420729353, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886390.45, "ph": "X", "dur": 0.24695593683190398, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886389.872, "ph": "X", "dur": 0.8999978683838755, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886391.007, "ph": "X", "dur": 0.09217369473303459, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886390.895, "ph": "X", "dur": 0.27912977367268027, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886391.658, "ph": "X", "dur": 0.22521685788543358, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886391.459, "ph": "X", "dur": 0.5008683789266786, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886392.033, "ph": "X", "dur": 0.0826084999965876, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886392.484, "ph": "X", "dur": 0.11652146315308146, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886392.363, "ph": "X", "dur": 0.3147818631448917, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886392.775, "ph": "X", "dur": 0.11304321052164619, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886392.981, "ph": "X", "dur": 0.1539126789410106, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886393.417, "ph": "X", "dur": 0.08695631578588169, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886393.293, "ph": "X", "dur": 0.2860862789355508, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886392.244, "ph": "X", "dur": 1.4026053736262716, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886393.997, "ph": "X", "dur": 0.133043163152399, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886393.9, "ph": "X", "dur": 0.30695579472416235, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886394.542, "ph": "X", "dur": 0.07913024736515234, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886394.448, "ph": "X", "dur": 0.24869506314762163, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886393.783, "ph": "X", "dur": 0.9886933104854747, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886397.14, "ph": "X", "dur": 0.09739107368018748, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886397.013, "ph": "X", "dur": 0.30173841577700944, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886397.852, "ph": "X", "dur": 0.2165212263068454, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886397.595, "ph": "X", "dur": 0.5504334789246311, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886398.223, "ph": "X", "dur": 0.08608675262802286, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886398.682, "ph": "X", "dur": 0.11739102631094028, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886398.547, "ph": "X", "dur": 0.32869487367063277, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886398.989, "ph": "X", "dur": 0.11913015262665792, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886399.231, "ph": "X", "dur": 0.14695617367814007, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886399.693, "ph": "X", "dur": 0.08608675262802286, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886399.539, "ph": "X", "dur": 0.31652098946060936, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886398.43, "ph": "X", "dur": 1.495648631517165, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886400.279, "ph": "X", "dur": 0.13217359999454015, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886400.182, "ph": "X", "dur": 0.3034775420927271, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886400.742, "ph": "X", "dur": 0.07826068420729353, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886400.647, "ph": "X", "dur": 0.24608637367404518, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886400.065, "ph": "X", "dur": 0.9043456841731695, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886401.221, "ph": "X", "dur": 0.08956500525945814, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886401.11, "ph": "X", "dur": 0.27739064735696256, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886401.804, "ph": "X", "dur": 0.13130403683668135, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886401.641, "ph": "X", "dur": 0.36956434208999717, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886402.083, "ph": "X", "dur": 0.08173893683872879, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886402.495, "ph": "X", "dur": 0.1182605894687991, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886402.403, "ph": "X", "dur": 0.2852167157776919, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886402.79, "ph": "X", "dur": 0.11478233683736383, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886403.001, "ph": "X", "dur": 0.14434748420456361, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886403.393, "ph": "X", "dur": 0.08173893683872879, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886403.304, "ph": "X", "dur": 0.24608637367404518, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886402.285, "ph": "X", "dur": 1.3330403209975663, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886403.974, "ph": "X", "dur": 0.1339127263102578, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886403.877, "ph": "X", "dur": 0.3078253578820212, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886404.442, "ph": "X", "dur": 0.0773911210494347, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886404.346, "ph": "X", "dur": 0.24869506314762163, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886403.763, "ph": "X", "dur": 0.906954373646746, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886404.873, "ph": "X", "dur": 0.08869544210159933, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886404.795, "ph": "X", "dur": 0.24347768420046872, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886405.544, "ph": "X", "dur": 0.17217350525604572, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886405.296, "ph": "X", "dur": 0.49739012629524326, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886405.876, "ph": "X", "dur": 0.0826084999965876, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886406.293, "ph": "X", "dur": 0.11652146315308146, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886406.203, "ph": "X", "dur": 0.28260802630411547, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886406.587, "ph": "X", "dur": 0.11304321052164619, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886406.795, "ph": "X", "dur": 0.14260835788884596, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886407.181, "ph": "X", "dur": 0.08347806315444642, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886407.095, "ph": "X", "dur": 0.24260812104260993, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886406.077, "ph": "X", "dur": 1.3330403209975663, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886407.756, "ph": "X", "dur": 0.13217359999454015, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886407.66, "ph": "X", "dur": 5.001727284003915, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886413.021, "ph": "X", "dur": 0.07913024736515234, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886412.926, "ph": "X", "dur": 0.24782549998976283, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886407.546, "ph": "X", "dur": 5.701725626080262, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886413.45, "ph": "X", "dur": 0.08695631578588169, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886413.37, "ph": "X", "dur": 0.24260812104260993, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886414.539, "ph": "X", "dur": 0.16173874736173993, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886413.875, "ph": "X", "dur": 0.9026065578574519, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886414.858, "ph": "X", "dur": 0.08521718947016406, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886415.267, "ph": "X", "dur": 0.11565189999522264, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886415.179, "ph": "X", "dur": 0.27652108419910376, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886415.555, "ph": "X", "dur": 0.11999971578451672, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886415.768, "ph": "X", "dur": 0.1460866105202812, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886416.157, "ph": "X", "dur": 0.08521718947016406, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886416.07, "ph": "X", "dur": 0.24695593683190398, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886415.061, "ph": "X", "dur": 1.324344689418978, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886416.729, "ph": "X", "dur": 0.13565185262597543, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886416.634, "ph": "X", "dur": 0.30608623156630355, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886417.259, "ph": "X", "dur": 0.07913024736515234, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886417.163, "ph": "X", "dur": 0.24956462630548046, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886416.523, "ph": "X", "dur": 0.9652151052232868, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886417.688, "ph": "X", "dur": 0.08695631578588169, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886417.61, "ph": "X", "dur": 0.237390742095457, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886418.48, "ph": "X", "dur": 0.25478200525263334, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886418.219, "ph": "X", "dur": 0.5913029473439955, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886418.891, "ph": "X", "dur": 0.0826084999965876, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886419.301, "ph": "X", "dur": 0.11739102631094028, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886419.213, "ph": "X", "dur": 0.28173846314625667, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886419.592, "ph": "X", "dur": 0.11043452104806974, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886419.796, "ph": "X", "dur": 0.1599996210460223, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886420.199, "ph": "X", "dur": 0.0826084999965876, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886420.111, "ph": "X", "dur": 0.24347768420046872, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886419.096, "ph": "X", "dur": 1.3269533788925547, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886420.762, "ph": "X", "dur": 0.13217359999454015, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886420.667, "ph": "X", "dur": 0.3034775420927271, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886421.222, "ph": "X", "dur": 0.07913024736515234, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886421.127, "ph": "X", "dur": 0.24521681051618638, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886420.556, "ph": "X", "dur": 0.8904326736474285, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886421.652, "ph": "X", "dur": 0.08695631578588169, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886421.571, "ph": "X", "dur": 0.24347768420046872, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886422.537, "ph": "X", "dur": 0.3191296789341858, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886422.212, "ph": "X", "dur": 0.7199982947071004, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886423.01, "ph": "X", "dur": 0.08347806315444642, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886423.418, "ph": "X", "dur": 0.11739102631094028, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886423.326, "ph": "X", "dur": 0.28347758946197427, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886423.781, "ph": "X", "dur": 0.113912773679505, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886423.99, "ph": "X", "dur": 0.14434748420456361, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886425.948, "ph": "X", "dur": 0.0826084999965876, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886425.856, "ph": "X", "dur": 0.2513037526211981, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886423.21, "ph": "X", "dur": 2.966949494614283, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886426.541, "ph": "X", "dur": 0.13565185262597543, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886426.444, "ph": "X", "dur": 0.30695579472416235, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886427.013, "ph": "X", "dur": 0.07999981052301115, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886426.916, "ph": "X", "dur": 0.2521733157790569, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886426.326, "ph": "X", "dur": 0.9182586946989106, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886427.463, "ph": "X", "dur": 0.08521718947016406, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886427.385, "ph": "X", "dur": 0.2356516157797394, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886428.419, "ph": "X", "dur": 0.23826030525331585, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886428.062, "ph": "X", "dur": 0.669563631551289, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886428.84, "ph": "X", "dur": 0.08956500525945814, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886429.259, "ph": "X", "dur": 0.1182605894687991, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886429.17, "ph": "X", "dur": 0.28347758946197427, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886429.556, "ph": "X", "dur": 0.11565189999522264, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886429.77, "ph": "X", "dur": 0.14782573683599887, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886430.162, "ph": "X", "dur": 0.0826084999965876, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886430.075, "ph": "X", "dur": 0.24347768420046872, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886429.051, "ph": "X", "dur": 1.3356490104711427, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886430.755, "ph": "X", "dur": 0.133043163152399, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886430.659, "ph": "X", "dur": 0.3026079789348683, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886431.295, "ph": "X", "dur": 0.07999981052301115, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886431.198, "ph": "X", "dur": 0.24956462630548046, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886430.546, "ph": "X", "dur": 0.9921715631169101, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886431.742, "ph": "X", "dur": 0.08956500525945814, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886431.662, "ph": "X", "dur": 0.24608637367404518, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886432.685, "ph": "X", "dur": 0.3156514263027505, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886432.405, "ph": "X", "dur": 0.6704331947091479, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886433.156, "ph": "X", "dur": 0.08086937368086997, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886433.573, "ph": "X", "dur": 0.11652146315308146, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886433.481, "ph": "X", "dur": 0.2860862789355508, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886433.953, "ph": "X", "dur": 0.11217364736378738, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886434.16, "ph": "X", "dur": 0.13913010525741068, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886434.548, "ph": "X", "dur": 0.08173893683872879, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886434.457, "ph": "X", "dur": 0.24608637367404518, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886433.361, "ph": "X", "dur": 1.410431442047001, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886435.112, "ph": "X", "dur": 0.13130403683668135, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886435.017, "ph": "X", "dur": 0.3026079789348683, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886435.574, "ph": "X", "dur": 0.07913024736515234, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886435.479, "ph": "X", "dur": 0.24782549998976283, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886434.905, "ph": "X", "dur": 0.8956500525945814, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886436.0, "ph": "X", "dur": 0.08521718947016406, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886435.922, "ph": "X", "dur": 0.23912986841117465, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886437.265, "ph": "X", "dur": 0.8156502420715702, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886438.177, "ph": "X", "dur": 0.46956410524376113, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886441.199, "ph": "X", "dur": 0.12434753157381082, "name": "Handler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1039)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886441.967, "ph": "X", "dur": 0.13826054209955188, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886441.871, "ph": "X", "dur": 0.3078253578820212, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886443.328, "ph": "X", "dur": 0.07913024736515234, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886443.241, "ph": "X", "dur": 0.24086899472689227, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886441.753, "ph": "X", "dur": 1.808691368346339, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886443.981, "ph": "X", "dur": 0.09043456841731695, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886444.758, "ph": "X", "dur": 0.3121731736713153, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886444.473, "ph": "X", "dur": 0.6730418841827243, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886445.231, "ph": "X", "dur": 0.09217369473303459, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886445.664, "ph": "X", "dur": 0.10869539473235211, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886445.581, "ph": "X", "dur": 0.2686950157783744, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886446.063, "ph": "X", "dur": 0.1556518052567282, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886446.341, "ph": "X", "dur": 0.17391263157176337, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886446.77, "ph": "X", "dur": 0.08521718947016406, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886446.682, "ph": "X", "dur": 0.24869506314762163, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886445.455, "ph": "X", "dur": 1.5599963051987173, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886447.391, "ph": "X", "dur": 0.13217359999454015, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886447.296, "ph": "X", "dur": 0.30086885261915064, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886447.869, "ph": "X", "dur": 0.07826068420729353, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886447.775, "ph": "X", "dur": 0.24608637367404518, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886447.181, "ph": "X", "dur": 0.9147804420674754, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886448.307, "ph": "X", "dur": 0.0773911210494347, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886448.218, "ph": "X", "dur": 0.23912986841117465, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886449.117, "ph": "X", "dur": 0.25565156841049214, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886448.851, "ph": "X", "dur": 0.5965203262911484, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886449.527, "ph": "X", "dur": 0.07217374210228181, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886449.92, "ph": "X", "dur": 0.10956495789021094, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886449.839, "ph": "X", "dur": 0.266086326304798, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886450.203, "ph": "X", "dur": 0.12521709473166964, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886450.423, "ph": "X", "dur": 0.14173879473098713, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886450.814, "ph": "X", "dur": 0.08347806315444642, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886450.723, "ph": "X", "dur": 0.24869506314762163, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886449.722, "ph": "X", "dur": 1.3173881841561077, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886451.385, "ph": "X", "dur": 0.1339127263102578, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886451.288, "ph": "X", "dur": 0.30695579472416235, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886451.847, "ph": "X", "dur": 0.07913024736515234, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886451.75, "ph": "X", "dur": 0.24956462630548046, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886451.173, "ph": "X", "dur": 0.9008674315417343, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886452.279, "ph": "X", "dur": 0.07826068420729353, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886452.198, "ph": "X", "dur": 0.23391248946402174, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886453.927, "ph": "X", "dur": 0.3052166684084447, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886453.421, "ph": "X", "dur": 0.8869544210159933, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886454.395, "ph": "X", "dur": 0.3095644841977388, "name": "builtins.getattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886455.406, "ph": "X", "dur": 0.11217364736378738, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886455.225, "ph": "X", "dur": 0.36956434208999717, "name": "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886456.751, "ph": "X", "dur": 1.3565185262597543, "name": "_StderrHandler.stream (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1299)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886460.529, "ph": "X", "dur": 0.4530424052444436, "name": "_StderrHandler.stream (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1299)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886461.113, "ph": "X", "dur": 0.21478209999112777, "name": "builtins.hasattr", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886461.502, "ph": "X", "dur": 0.08608675262802286, "name": "_StderrHandler.stream (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1299)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886461.776, "ph": "X", "dur": 0.14956486315171652, "name": "_io.TextIOWrapper.flush", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886462.558, "ph": "X", "dur": 0.08521718947016406, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886462.384, "ph": "X", "dur": 0.3330426894599269, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886455.056, "ph": "X", "dur": 7.727807783891306, "name": "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886463.445, "ph": "X", "dur": 0.13652141578383425, "name": "_thread.RLock.acquire", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886463.336, "ph": "X", "dur": 0.32434705788133866, "name": "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886464.456, "ph": "X", "dur": 0.08173893683872879, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886464.368, "ph": "X", "dur": 0.24608637367404518, "name": "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886463.211, "ph": "X", "dur": 1.4817356209914239, "name": "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886465.002, "ph": "X", "dur": 0.07913024736515234, "name": "_thread.RLock.release", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886464.866, "ph": "X", "dur": 0.2913036578827036, "name": "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886236.844, "ph": "X", "dur": 228.67945838002737, "name": "shutdown (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:2245)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886470.471, "ph": "X", "dur": 1.1008669578492623, "name": "info (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:52)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886472.582, "ph": "X", "dur": 0.14782573683599887, "name": "debug (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:48)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886479.847, "ph": "X", "dur": 0.6617375631305596, "name": "_run_finalizers.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:287)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886481.097, "ph": "X", "dur": 1.28608391047319, "name": "list.sort", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886475.716, "ph": "X", "dur": 6.974766089185571, "name": "_run_finalizers (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:271)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886483.942, "ph": "X", "dur": 0.9321717052246516, "name": "current_process (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/process.py:37)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886487.884, "ph": "X", "dur": 1.0947800157442504, "name": "_cleanup (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/process.py:61)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886485.715, "ph": "X", "dur": 3.643469631428443, "name": "active_children (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/process.py:43)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886489.828, "ph": "X", "dur": 0.49217274734809036, "name": "_cleanup (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/process.py:61)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886489.606, "ph": "X", "dur": 1.166953757846532, "name": "active_children (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/process.py:43)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886491.275, "ph": "X", "dur": 0.13826054209955188, "name": "debug (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:48)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886493.917, "ph": "X", "dur": 0.11130408420592856, "name": "_run_finalizers.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:285)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886494.569, "ph": "X", "dur": 0.15217355262529295, "name": "list.sort", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886495.13, "ph": "X", "dur": 0.39217298419432645, "name": "dict.get", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886496.636, "ph": "X", "dur": 0.31739055261846816, "name": "sub_debug (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:44)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886501.245, "ph": "X", "dur": 1.8843433630800561, "name": "posix.getpid", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886505.042, "ph": "X", "dur": 0.10695626841663447, "name": "sub_debug (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:44)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886499.768, "ph": "X", "dur": 47.68597401381966, "name": "Finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:208)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886492.435, "ph": "X", "dur": 55.07552172930389, "name": "_run_finalizers (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:271)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831886468.451, "ph": "X", "dur": 79.09633440199585, "name": "_exit_function (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:323)", "cat": "FEE"}, {"pid": 1867968, "tid": 1867968, "ts": 2254831875363.884, "ph": "X", "dur": 11183.67264226434, "name": "atexit._run_exitfuncs", "cat": "FEE"}], "viztracer_metadata": {"version": "1.0.4", "baseTimeNanoseconds": 1750988053957141002}, "file_info": {"files": {"/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py": ["\"\"\"Weak reference support for Python.\n\nThis module is an implementation of PEP 205:\n\nhttps://peps.python.org/pep-0205/\n\"\"\"\n\n# Naming convention: Variables named \"wr\" are weak reference objects;\n# they are called this instead of \"ref\" to avoid name collisions with\n# the module-global ref() function imported from _weakref.\n\nfrom _weakref import (\n     getweakrefcount,\n     getweakrefs,\n     ref,\n     proxy,\n     CallableProxyType,\n     ProxyType,\n     ReferenceType,\n     _remove_dead_weakref)\n\nfrom _weakrefset import WeakSet, _IterationGuard\n\nimport _collections_abc  # Import after _weakref to avoid circular import.\nimport sys\nimport itertools\n\nProxyTypes = (ProxyType, CallableProxyType)\n\n__all__ = [\"ref\", \"proxy\", \"getweakrefcount\", \"getweakrefs\",\n           \"WeakKeyDictionary\", \"ReferenceType\", \"ProxyType\",\n           \"CallableProxyType\", \"ProxyTypes\", \"WeakValueDictionary\",\n           \"WeakSet\", \"WeakMethod\", \"finalize\"]\n\n\n_collections_abc.MutableSet.register(WeakSet)\n\nclass WeakMethod(ref):\n    \"\"\"\n    A custom `weakref.ref` subclass which simulates a weak reference to\n    a bound method, working around the lifetime problem of bound methods.\n    \"\"\"\n\n    __slots__ = \"_func_ref\", \"_meth_type\", \"_alive\", \"__weakref__\"\n\n    def __new__(cls, meth, callback=None):\n        try:\n            obj = meth.__self__\n            func = meth.__func__\n        except AttributeError:\n            raise TypeError(\"argument should be a bound method, not {}\"\n                            .format(type(meth))) from None\n        def _cb(arg):\n            # The self-weakref trick is needed to avoid creating a reference\n            # cycle.\n            self = self_wr()\n            if self._alive:\n                self._alive = False\n                if callback is not None:\n                    callback(self)\n        self = ref.__new__(cls, obj, _cb)\n        self._func_ref = ref(func, _cb)\n        self._meth_type = type(meth)\n        self._alive = True\n        self_wr = ref(self)\n        return self\n\n    def __call__(self):\n        obj = super().__call__()\n        func = self._func_ref()\n        if obj is None or func is None:\n            return None\n        return self._meth_type(func, obj)\n\n    def __eq__(self, other):\n        if isinstance(other, WeakMethod):\n            if not self._alive or not other._alive:\n                return self is other\n            return ref.__eq__(self, other) and self._func_ref == other._func_ref\n        return NotImplemented\n\n    def __ne__(self, other):\n        if isinstance(other, WeakMethod):\n            if not self._alive or not other._alive:\n                return self is not other\n            return ref.__ne__(self, other) or self._func_ref != other._func_ref\n        return NotImplemented\n\n    __hash__ = ref.__hash__\n\n\nclass WeakValueDictionary(_collections_abc.MutableMapping):\n    \"\"\"Mapping class that references values weakly.\n\n    Entries in the dictionary will be discarded when no strong\n    reference to the value exists anymore\n    \"\"\"\n    # We inherit the constructor without worrying about the input\n    # dictionary; since it uses our .update() method, we get the right\n    # checks (if the other dictionary is a WeakValueDictionary,\n    # objects are unwrapped on the way out, and we always wrap on the\n    # way in).\n\n    def __init__(self, other=(), /, **kw):\n        def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(wr.key)\n                else:\n                    # Atomic removal is necessary since this function\n                    # can be called asynchronously by the GC\n                    _atomic_removal(self.data, wr.key)\n        self._remove = remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        self.data = {}\n        self.update(other, **kw)\n\n    def _commit_removals(self, _atomic_removal=_remove_dead_weakref):\n        pop = self._pending_removals.pop\n        d = self.data\n        # We shouldn't encounter any KeyError, because this method should\n        # always be called *before* mutating the dict.\n        while True:\n            try:\n                key = pop()\n            except IndexError:\n                return\n            _atomic_removal(d, key)\n\n    def __getitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        o = self.data[key]()\n        if o is None:\n            raise KeyError(key)\n        else:\n            return o\n\n    def __delitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        del self.data[key]\n\n    def __len__(self):\n        if self._pending_removals:\n            self._commit_removals()\n        return len(self.data)\n\n    def __contains__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data[key]()\n        except KeyError:\n            return False\n        return o is not None\n\n    def __repr__(self):\n        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n\n    def __setitem__(self, key, value):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data[key] = KeyedRef(value, self._remove, key)\n\n    def copy(self):\n        if self._pending_removals:\n            self._commit_removals()\n        new = WeakValueDictionary()\n        with _IterationGuard(self):\n            for key, wr in self.data.items():\n                o = wr()\n                if o is not None:\n                    new[key] = o\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        if self._pending_removals:\n            self._commit_removals()\n        new = self.__class__()\n        with _IterationGuard(self):\n            for key, wr in self.data.items():\n                o = wr()\n                if o is not None:\n                    new[deepcopy(key, memo)] = o\n        return new\n\n    def get(self, key, default=None):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            wr = self.data[key]\n        except KeyError:\n            return default\n        else:\n            o = wr()\n            if o is None:\n                # This should only happen\n                return default\n            else:\n                return o\n\n    def items(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for k, wr in self.data.items():\n                v = wr()\n                if v is not None:\n                    yield k, v\n\n    def keys(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for k, wr in self.data.items():\n                if wr() is not None:\n                    yield k\n\n    __iter__ = keys\n\n    def itervaluerefs(self):\n        \"\"\"Return an iterator that yields the weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            yield from self.data.values()\n\n    def values(self):\n        if self._pending_removals:\n            self._commit_removals()\n        with _IterationGuard(self):\n            for wr in self.data.values():\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    def popitem(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            key, wr = self.data.popitem()\n            o = wr()\n            if o is not None:\n                return key, o\n\n    def pop(self, key, *args):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data.pop(key)()\n        except KeyError:\n            o = None\n        if o is None:\n            if args:\n                return args[0]\n            else:\n                raise KeyError(key)\n        else:\n            return o\n\n    def setdefault(self, key, default=None):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            o = None\n        if o is None:\n            if self._pending_removals:\n                self._commit_removals()\n            self.data[key] = KeyedRef(default, self._remove, key)\n            return default\n        else:\n            return o\n\n    def update(self, other=None, /, **kwargs):\n        if self._pending_removals:\n            self._commit_removals()\n        d = self.data\n        if other is not None:\n            if not hasattr(other, \"items\"):\n                other = dict(other)\n            for key, o in other.items():\n                d[key] = KeyedRef(o, self._remove, key)\n        for key, o in kwargs.items():\n            d[key] = KeyedRef(o, self._remove, key)\n\n    def valuerefs(self):\n        \"\"\"Return a list of weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        if self._pending_removals:\n            self._commit_removals()\n        return list(self.data.values())\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def __or__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.copy()\n            c.update(other)\n            return c\n        return NotImplemented\n\n    def __ror__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.__class__()\n            c.update(other)\n            c.update(self)\n            return c\n        return NotImplemented\n\n\nclass KeyedRef(ref):\n    \"\"\"Specialized reference that includes a key corresponding to the value.\n\n    This is used in the WeakValueDictionary to avoid having to create\n    a function object for each key stored in the mapping.  A shared\n    callback object can use the 'key' attribute of a KeyedRef instead\n    of getting a reference to the key from an enclosing scope.\n\n    \"\"\"\n\n    __slots__ = \"key\",\n\n    def __new__(type, ob, callback, key):\n        self = ref.__new__(type, ob, callback)\n        self.key = key\n        return self\n\n    def __init__(self, ob, callback, key):\n        super().__init__(ob, callback)\n\n\nclass WeakKeyDictionary(_collections_abc.MutableMapping):\n    \"\"\" Mapping class that references keys weakly.\n\n    Entries in the dictionary will be discarded when there is no\n    longer a strong reference to the key. This can be used to\n    associate additional data with an object owned by other parts of\n    an application without adding attributes to those objects. This\n    can be especially useful with objects that override attribute\n    accesses.\n    \"\"\"\n\n    def __init__(self, dict=None):\n        self.data = {}\n        def remove(k, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(k)\n                else:\n                    try:\n                        del self.data[k]\n                    except KeyError:\n                        pass\n        self._remove = remove\n        # A list of dead weakrefs (keys to be removed)\n        self._pending_removals = []\n        self._iterating = set()\n        self._dirty_len = False\n        if dict is not None:\n            self.update(dict)\n\n    def _commit_removals(self):\n        # NOTE: We don't need to call this method before mutating the dict,\n        # because a dead weakref never compares equal to a live weakref,\n        # even if they happened to refer to equal objects.\n        # However, it means keys may already have been removed.\n        pop = self._pending_removals.pop\n        d = self.data\n        while True:\n            try:\n                key = pop()\n            except IndexError:\n                return\n\n            try:\n                del d[key]\n            except KeyError:\n                pass\n\n    def _scrub_removals(self):\n        d = self.data\n        self._pending_removals = [k for k in self._pending_removals if k in d]\n        self._dirty_len = False\n\n    def __delitem__(self, key):\n        self._dirty_len = True\n        del self.data[ref(key)]\n\n    def __getitem__(self, key):\n        return self.data[ref(key)]\n\n    def __len__(self):\n        if self._dirty_len and self._pending_removals:\n            # self._pending_removals may still contain keys which were\n            # explicitly removed, we have to scrub them (see issue #21173).\n            self._scrub_removals()\n        return len(self.data) - len(self._pending_removals)\n\n    def __repr__(self):\n        return \"<%s at %#x>\" % (self.__class__.__name__, id(self))\n\n    def __setitem__(self, key, value):\n        self.data[ref(key, self._remove)] = value\n\n    def copy(self):\n        new = WeakKeyDictionary()\n        with _IterationGuard(self):\n            for key, value in self.data.items():\n                o = key()\n                if o is not None:\n                    new[o] = value\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        new = self.__class__()\n        with _IterationGuard(self):\n            for key, value in self.data.items():\n                o = key()\n                if o is not None:\n                    new[o] = deepcopy(value, memo)\n        return new\n\n    def get(self, key, default=None):\n        return self.data.get(ref(key),default)\n\n    def __contains__(self, key):\n        try:\n            wr = ref(key)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def items(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.items():\n                key = wr()\n                if key is not None:\n                    yield key, value\n\n    def keys(self):\n        with _IterationGuard(self):\n            for wr in self.data:\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    __iter__ = keys\n\n    def values(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.items():\n                if wr() is not None:\n                    yield value\n\n    def keyrefs(self):\n        \"\"\"Return a list of weak references to the keys.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the keys around longer than needed.\n\n        \"\"\"\n        return list(self.data)\n\n    def popitem(self):\n        self._dirty_len = True\n        while True:\n            key, value = self.data.popitem()\n            o = key()\n            if o is not None:\n                return o, value\n\n    def pop(self, key, *args):\n        self._dirty_len = True\n        return self.data.pop(ref(key), *args)\n\n    def setdefault(self, key, default=None):\n        return self.data.setdefault(ref(key, self._remove),default)\n\n    def update(self, dict=None, /, **kwargs):\n        d = self.data\n        if dict is not None:\n            if not hasattr(dict, \"items\"):\n                dict = type({})(dict)\n            for key, value in dict.items():\n                d[ref(key, self._remove)] = value\n        if len(kwargs):\n            self.update(kwargs)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def __or__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.copy()\n            c.update(other)\n            return c\n        return NotImplemented\n\n    def __ror__(self, other):\n        if isinstance(other, _collections_abc.Mapping):\n            c = self.__class__()\n            c.update(other)\n            c.update(self)\n            return c\n        return NotImplemented\n\n\nclass finalize:\n    \"\"\"Class for finalization of weakrefable objects\n\n    finalize(obj, func, *args, **kwargs) returns a callable finalizer\n    object which will be called when obj is garbage collected. The\n    first time the finalizer is called it evaluates func(*arg, **kwargs)\n    and returns the result. After this the finalizer is dead, and\n    calling it just returns None.\n\n    When the program exits any remaining finalizers for which the\n    atexit attribute is true will be run in reverse order of creation.\n    By default atexit is true.\n    \"\"\"\n\n    # Finalizer objects don't have any state of their own.  They are\n    # just used as keys to lookup _Info objects in the registry.  This\n    # ensures that they cannot be part of a ref-cycle.\n\n    __slots__ = ()\n    _registry = {}\n    _shutdown = False\n    _index_iter = itertools.count()\n    _dirty = False\n    _registered_with_atexit = False\n\n    class _Info:\n        __slots__ = (\"weakref\", \"func\", \"args\", \"kwargs\", \"atexit\", \"index\")\n\n    def __init__(self, obj, func, /, *args, **kwargs):\n        if not self._registered_with_atexit:\n            # We may register the exit function more than once because\n            # of a thread race, but that is harmless\n            import atexit\n            atexit.register(self._exitfunc)\n            finalize._registered_with_atexit = True\n        info = self._Info()\n        info.weakref = ref(obj, self)\n        info.func = func\n        info.args = args\n        info.kwargs = kwargs or None\n        info.atexit = True\n        info.index = next(self._index_iter)\n        self._registry[self] = info\n        finalize._dirty = True\n\n    def __call__(self, _=None):\n        \"\"\"If alive then mark as dead and return func(*args, **kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.pop(self, None)\n        if info and not self._shutdown:\n            return info.func(*info.args, **(info.kwargs or {}))\n\n    def detach(self):\n        \"\"\"If alive then mark as dead and return (obj, func, args, kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None and self._registry.pop(self, None):\n            return (obj, info.func, info.args, info.kwargs or {})\n\n    def peek(self):\n        \"\"\"If alive then return (obj, func, args, kwargs);\n        otherwise return None\"\"\"\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None:\n            return (obj, info.func, info.args, info.kwargs or {})\n\n    @property\n    def alive(self):\n        \"\"\"Whether finalizer is alive\"\"\"\n        return self in self._registry\n\n    @property\n    def atexit(self):\n        \"\"\"Whether finalizer should be called at exit\"\"\"\n        info = self._registry.get(self)\n        return bool(info) and info.atexit\n\n    @atexit.setter\n    def atexit(self, value):\n        info = self._registry.get(self)\n        if info:\n            info.atexit = bool(value)\n\n    def __repr__(self):\n        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is None:\n            return '<%s object at %#x; dead>' % (type(self).__name__, id(self))\n        else:\n            return '<%s object at %#x; for %r at %#x>' % \\\n                (type(self).__name__, id(self), type(obj).__name__, id(obj))\n\n    @classmethod\n    def _select_for_exit(cls):\n        # Return live finalizers marked for exit, oldest first\n        L = [(f,i) for (f,i) in cls._registry.items() if i.atexit]\n        L.sort(key=lambda item:item[1].index)\n        return [f for (f,i) in L]\n\n    @classmethod\n    def _exitfunc(cls):\n        # At shutdown invoke finalizers for which atexit is true.\n        # This is called once all other non-daemonic threads have been\n        # joined.\n        reenable_gc = False\n        try:\n            if cls._registry:\n                import gc\n                if gc.isenabled():\n                    reenable_gc = True\n                    gc.disable()\n                pending = None\n                while True:\n                    if pending is None or finalize._dirty:\n                        pending = cls._select_for_exit()\n                        finalize._dirty = False\n                    if not pending:\n                        break\n                    f = pending.pop()\n                    try:\n                        # gc is disabled, so (assuming no daemonic\n                        # threads) the following is the only line in\n                        # this function which might trigger creation\n                        # of a new finalizer\n                        f()\n                    except Exception:\n                        sys.excepthook(*sys.exc_info())\n                    assert f not in cls._registry\n        finally:\n            # prevent any more finalizers from executing during shutdown\n            finalize._shutdown = True\n            if reenable_gc:\n                gc.enable()\n", 674], "<frozen genericpath>": ["\"\"\"\nPath operations common to more than one OS\nDo not use directly.  The OS specific modules import the appropriate\nfunctions from this module themselves.\n\"\"\"\nimport os\nimport stat\n\n__all__ = ['commonprefix', 'exists', 'getatime', 'getctime', 'getmtime',\n           'getsize', 'isdir', 'isfile', 'islink', 'samefile', 'sameopenfile',\n           'samestat', 'ALLOW_MISSING']\n\n\n# Does a path exist?\n# This is false for dangling symbolic links on systems that support them.\ndef exists(path):\n    \"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\n    try:\n        os.stat(path)\n    except (OSError, ValueError):\n        return False\n    return True\n\n\n# This follows symbolic links, so both islink() and isdir() can be true\n# for the same path on systems that support symlinks\ndef isfile(path):\n    \"\"\"Test whether a path is a regular file\"\"\"\n    try:\n        st = os.stat(path)\n    except (OSError, ValueError):\n        return False\n    return stat.S_ISREG(st.st_mode)\n\n\n# Is a path a directory?\n# This follows symbolic links, so both islink() and isdir()\n# can be true for the same path on systems that support symlinks\ndef isdir(s):\n    \"\"\"Return true if the pathname refers to an existing directory.\"\"\"\n    try:\n        st = os.stat(s)\n    except (OSError, ValueError):\n        return False\n    return stat.S_ISDIR(st.st_mode)\n\n\n# Is a path a symbolic link?\n# This will always return false on systems where os.lstat doesn't exist.\n\ndef islink(path):\n    \"\"\"Test whether a path is a symbolic link\"\"\"\n    try:\n        st = os.lstat(path)\n    except (OSError, ValueError, AttributeError):\n        return False\n    return stat.S_ISLNK(st.st_mode)\n\n\ndef getsize(filename):\n    \"\"\"Return the size of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_size\n\n\ndef getmtime(filename):\n    \"\"\"Return the last modification time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_mtime\n\n\ndef getatime(filename):\n    \"\"\"Return the last access time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_atime\n\n\ndef getctime(filename):\n    \"\"\"Return the metadata change time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_ctime\n\n\n# Return the longest prefix of all list elements.\ndef commonprefix(m):\n    \"Given a list of pathnames, returns the longest common leading component\"\n    if not m: return ''\n    # Some people pass in a list of pathname parts to operate in an OS-agnostic\n    # fashion; don't try to translate in that case as that's an abuse of the\n    # API and they are already doing what they need to be OS-agnostic and so\n    # they most likely won't be using an os.PathLike object in the sublists.\n    if not isinstance(m[0], (list, tuple)):\n        m = tuple(map(os.fspath, m))\n    s1 = min(m)\n    s2 = max(m)\n    for i, c in enumerate(s1):\n        if c != s2[i]:\n            return s1[:i]\n    return s1\n\n# Are two stat buffers (obtained from stat, fstat or lstat)\n# describing the same file?\ndef samestat(s1, s2):\n    \"\"\"Test whether two stat buffers reference the same file\"\"\"\n    return (s1.st_ino == s2.st_ino and\n            s1.st_dev == s2.st_dev)\n\n\n# Are two filenames really pointing to the same file?\ndef samefile(f1, f2):\n    \"\"\"Test whether two pathnames reference the same actual file or directory\n\n    This is determined by the device number and i-node number and\n    raises an exception if an os.stat() call on either pathname fails.\n    \"\"\"\n    s1 = os.stat(f1)\n    s2 = os.stat(f2)\n    return samestat(s1, s2)\n\n\n# Are two open files really referencing the same file?\n# (Not necessarily the same file descriptor!)\ndef sameopenfile(fp1, fp2):\n    \"\"\"Test whether two open file objects reference the same file\"\"\"\n    s1 = os.fstat(fp1)\n    s2 = os.fstat(fp2)\n    return samestat(s1, s2)\n\n\n# Split a path in root and extension.\n# The extension is everything starting at the last dot in the last\n# pathname component; the root is everything before that.\n# It is always true that root + ext == p.\n\n# Generic implementation of splitext, to be parametrized with\n# the separators\ndef _splitext(p, sep, altsep, extsep):\n    \"\"\"Split the extension from a pathname.\n\n    Extension is everything from the last dot to the end, ignoring\n    leading dots.  Returns \"(root, ext)\"; ext may be empty.\"\"\"\n    # NOTE: This code must work for text and bytes strings.\n\n    sepIndex = p.rfind(sep)\n    if altsep:\n        altsepIndex = p.rfind(altsep)\n        sepIndex = max(sepIndex, altsepIndex)\n\n    dotIndex = p.rfind(extsep)\n    if dotIndex > sepIndex:\n        # skip all leading dots\n        filenameIndex = sepIndex + 1\n        while filenameIndex < dotIndex:\n            if p[filenameIndex:filenameIndex+1] != extsep:\n                return p[:dotIndex], p[dotIndex:]\n            filenameIndex += 1\n\n    return p, p[:0]\n\ndef _check_arg_types(funcname, *args):\n    hasstr = hasbytes = False\n    for s in args:\n        if isinstance(s, str):\n            hasstr = True\n        elif isinstance(s, bytes):\n            hasbytes = True\n        else:\n            raise TypeError(f'{funcname}() argument must be str, bytes, or '\n                            f'os.PathLike object, not {s.__class__.__name__!r}') from None\n    if hasstr and hasbytes:\n        raise TypeError(\"Can't mix strings and bytes in path components\") from None\n\n# A singleton with a true boolean value.\n@object.__new__\nclass ALLOW_MISSING:\n    \"\"\"Special value for use in realpath().\"\"\"\n    def __repr__(self):\n        return 'os.path.ALLOW_MISSING'\n    def __reduce__(self):\n        return self.__class__.__name__\n", 176], "<frozen posixpath>": ["\"\"\"Common operations on Posix pathnames.\n\nInstead of importing this module directly, import os and refer to\nthis module as os.path.  The \"os.path\" name is an alias for this\nmodule on Posix systems; on other systems (e.g. Windows),\nos.path provides the same operations in a manner specific to that\nplatform, and is an alias to another module (e.g. ntpath).\n\nSome of this can actually be useful on non-Posix systems too, e.g.\nfor manipulation of the pathname component of URLs.\n\"\"\"\n\n# Strings representing various path-related bits and pieces.\n# These are primarily for export; internally, they are hardcoded.\n# Should be set before imports for resolving cyclic dependency.\ncurdir = '.'\npardir = '..'\nextsep = '.'\nsep = '/'\npathsep = ':'\ndefpath = '/bin:/usr/bin'\naltsep = None\ndevnull = '/dev/null'\n\nimport os\nimport sys\nimport stat\nimport genericpath\nfrom genericpath import *\n\n__all__ = [\"normcase\",\"isabs\",\"join\",\"splitdrive\",\"splitroot\",\"split\",\"splitext\",\n           \"basename\",\"dirname\",\"commonprefix\",\"getsize\",\"getmtime\",\n           \"getatime\",\"getctime\",\"islink\",\"exists\",\"lexists\",\"isdir\",\"isfile\",\n           \"ismount\", \"expanduser\",\"expandvars\",\"normpath\",\"abspath\",\n           \"samefile\",\"sameopenfile\",\"samestat\",\n           \"curdir\",\"pardir\",\"sep\",\"pathsep\",\"defpath\",\"altsep\",\"extsep\",\n           \"devnull\",\"realpath\",\"supports_unicode_filenames\",\"relpath\",\n           \"commonpath\", \"isjunction\", \"ALLOW_MISSING\"]\n\n\ndef _get_sep(path):\n    if isinstance(path, bytes):\n        return b'/'\n    else:\n        return '/'\n\n# Normalize the case of a pathname.  Trivial in Posix, string.lower on Mac.\n# On MS-DOS this may also turn slashes into backslashes; however, other\n# normalizations (such as optimizing '../' away) are not allowed\n# (another function should be defined to do that).\n\ndef normcase(s):\n    \"\"\"Normalize case of pathname.  Has no effect under Posix\"\"\"\n    return os.fspath(s)\n\n\n# Return whether a path is absolute.\n# Trivial in Posix, harder on the Mac or MS-DOS.\n\ndef isabs(s):\n    \"\"\"Test whether a path is absolute\"\"\"\n    s = os.fspath(s)\n    sep = _get_sep(s)\n    return s.startswith(sep)\n\n\n# Join pathnames.\n# Ignore the previous parts if a part is absolute.\n# Insert a '/' unless the first part is empty or already ends in '/'.\n\ndef join(a, *p):\n    \"\"\"Join two or more pathname components, inserting '/' as needed.\n    If any component is an absolute path, all previous path components\n    will be discarded.  An empty last part will result in a path that\n    ends with a separator.\"\"\"\n    a = os.fspath(a)\n    sep = _get_sep(a)\n    path = a\n    try:\n        if not p:\n            path[:0] + sep  #23780: Ensure compatible data type even if p is null.\n        for b in map(os.fspath, p):\n            if b.startswith(sep):\n                path = b\n            elif not path or path.endswith(sep):\n                path += b\n            else:\n                path += sep + b\n    except (TypeError, AttributeError, BytesWarning):\n        genericpath._check_arg_types('join', a, *p)\n        raise\n    return path\n\n\n# Split a path in head (everything up to the last '/') and tail (the\n# rest).  If the path ends in '/', tail will be empty.  If there is no\n# '/' in the path, head  will be empty.\n# Trailing '/'es are stripped from head unless it is the root.\n\ndef split(p):\n    \"\"\"Split a pathname.  Returns tuple \"(head, tail)\" where \"tail\" is\n    everything after the final slash.  Either part may be empty.\"\"\"\n    p = os.fspath(p)\n    sep = _get_sep(p)\n    i = p.rfind(sep) + 1\n    head, tail = p[:i], p[i:]\n    if head and head != sep*len(head):\n        head = head.rstrip(sep)\n    return head, tail\n\n\n# Split a path in root and extension.\n# The extension is everything starting at the last dot in the last\n# pathname component; the root is everything before that.\n# It is always true that root + ext == p.\n\ndef splitext(p):\n    p = os.fspath(p)\n    if isinstance(p, bytes):\n        sep = b'/'\n        extsep = b'.'\n    else:\n        sep = '/'\n        extsep = '.'\n    return genericpath._splitext(p, sep, None, extsep)\nsplitext.__doc__ = genericpath._splitext.__doc__\n\n# Split a pathname into a drive specification and the rest of the\n# path.  Useful on DOS/Windows/NT; on Unix, the drive is always empty.\n\ndef splitdrive(p):\n    \"\"\"Split a pathname into drive and path. On Posix, drive is always\n    empty.\"\"\"\n    p = os.fspath(p)\n    return p[:0], p\n\n\ndef splitroot(p):\n    \"\"\"Split a pathname into drive, root and tail. On Posix, drive is always\n    empty; the root may be empty, a single slash, or two slashes. The tail\n    contains anything after the root. For example:\n\n        splitroot('foo/bar') == ('', '', 'foo/bar')\n        splitroot('/foo/bar') == ('', '/', 'foo/bar')\n        splitroot('//foo/bar') == ('', '//', 'foo/bar')\n        splitroot('///foo/bar') == ('', '/', '//foo/bar')\n    \"\"\"\n    p = os.fspath(p)\n    if isinstance(p, bytes):\n        sep = b'/'\n        empty = b''\n    else:\n        sep = '/'\n        empty = ''\n    if p[:1] != sep:\n        # Relative path, e.g.: 'foo'\n        return empty, empty, p\n    elif p[1:2] != sep or p[2:3] == sep:\n        # Absolute path, e.g.: '/foo', '///foo', '////foo', etc.\n        return empty, sep, p[1:]\n    else:\n        # Precisely two leading slashes, e.g.: '//foo'. Implementation defined per POSIX, see\n        # https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap04.html#tag_04_13\n        return empty, p[:2], p[2:]\n\n\n# Return the tail (basename) part of a path, same as split(path)[1].\n\ndef basename(p):\n    \"\"\"Returns the final component of a pathname\"\"\"\n    p = os.fspath(p)\n    sep = _get_sep(p)\n    i = p.rfind(sep) + 1\n    return p[i:]\n\n\n# Return the head (dirname) part of a path, same as split(path)[0].\n\ndef dirname(p):\n    \"\"\"Returns the directory component of a pathname\"\"\"\n    p = os.fspath(p)\n    sep = _get_sep(p)\n    i = p.rfind(sep) + 1\n    head = p[:i]\n    if head and head != sep*len(head):\n        head = head.rstrip(sep)\n    return head\n\n\n# Is a path a junction?\n\ndef isjunction(path):\n    \"\"\"Test whether a path is a junction\n    Junctions are not a part of posix semantics\"\"\"\n    os.fspath(path)\n    return False\n\n\n# Being true for dangling symbolic links is also useful.\n\ndef lexists(path):\n    \"\"\"Test whether a path exists.  Returns True for broken symbolic links\"\"\"\n    try:\n        os.lstat(path)\n    except (OSError, ValueError):\n        return False\n    return True\n\n\n# Is a path a mount point?\n# (Does this work for all UNIXes?  Is it even guaranteed to work by Posix?)\n\ndef ismount(path):\n    \"\"\"Test whether a path is a mount point\"\"\"\n    try:\n        s1 = os.lstat(path)\n    except (OSError, ValueError):\n        # It doesn't exist -- so not a mount point. :-)\n        return False\n    else:\n        # A symlink can never be a mount point\n        if stat.S_ISLNK(s1.st_mode):\n            return False\n\n    path = os.fspath(path)\n    if isinstance(path, bytes):\n        parent = join(path, b'..')\n    else:\n        parent = join(path, '..')\n    parent = realpath(parent)\n    try:\n        s2 = os.lstat(parent)\n    except (OSError, ValueError):\n        return False\n\n    dev1 = s1.st_dev\n    dev2 = s2.st_dev\n    if dev1 != dev2:\n        return True     # path/.. on a different device as path\n    ino1 = s1.st_ino\n    ino2 = s2.st_ino\n    if ino1 == ino2:\n        return True     # path/.. is the same i-node as path\n    return False\n\n\n# Expand paths beginning with '~' or '~user'.\n# '~' means $HOME; '~user' means that user's home directory.\n# If the path doesn't begin with '~', or if the user or $HOME is unknown,\n# the path is returned unchanged (leaving error reporting to whatever\n# function is called with the expanded path as argument).\n# See also module 'glob' for expansion of *, ? and [...] in pathnames.\n# (A function should also be defined to do full *sh-style environment\n# variable expansion.)\n\ndef expanduser(path):\n    \"\"\"Expand ~ and ~user constructions.  If user or $HOME is unknown,\n    do nothing.\"\"\"\n    path = os.fspath(path)\n    if isinstance(path, bytes):\n        tilde = b'~'\n    else:\n        tilde = '~'\n    if not path.startswith(tilde):\n        return path\n    sep = _get_sep(path)\n    i = path.find(sep, 1)\n    if i < 0:\n        i = len(path)\n    if i == 1:\n        if 'HOME' not in os.environ:\n            try:\n                import pwd\n            except ImportError:\n                # pwd module unavailable, return path unchanged\n                return path\n            try:\n                userhome = pwd.getpwuid(os.getuid()).pw_dir\n            except KeyError:\n                # bpo-10496: if the current user identifier doesn't exist in the\n                # password database, return the path unchanged\n                return path\n        else:\n            userhome = os.environ['HOME']\n    else:\n        try:\n            import pwd\n        except ImportError:\n            # pwd module unavailable, return path unchanged\n            return path\n        name = path[1:i]\n        if isinstance(name, bytes):\n            name = os.fsdecode(name)\n        try:\n            pwent = pwd.getpwnam(name)\n        except KeyError:\n            # bpo-10496: if the user name from the path doesn't exist in the\n            # password database, return the path unchanged\n            return path\n        userhome = pwent.pw_dir\n    # if no user home, return the path unchanged on VxWorks\n    if userhome is None and sys.platform == \"vxworks\":\n        return path\n    if isinstance(path, bytes):\n        userhome = os.fsencode(userhome)\n        root = b'/'\n    else:\n        root = '/'\n    userhome = userhome.rstrip(root)\n    return (userhome + path[i:]) or root\n\n\n# Expand paths containing shell variable substitutions.\n# This expands the forms $variable and ${variable} only.\n# Non-existent variables are left unchanged.\n\n_varprog = None\n_varprogb = None\n\ndef expandvars(path):\n    \"\"\"Expand shell variables of form $var and ${var}.  Unknown variables\n    are left unchanged.\"\"\"\n    path = os.fspath(path)\n    global _varprog, _varprogb\n    if isinstance(path, bytes):\n        if b'$' not in path:\n            return path\n        if not _varprogb:\n            import re\n            _varprogb = re.compile(br'\\$(\\w+|\\{[^}]*\\})', re.ASCII)\n        search = _varprogb.search\n        start = b'{'\n        end = b'}'\n        environ = getattr(os, 'environb', None)\n    else:\n        if '$' not in path:\n            return path\n        if not _varprog:\n            import re\n            _varprog = re.compile(r'\\$(\\w+|\\{[^}]*\\})', re.ASCII)\n        search = _varprog.search\n        start = '{'\n        end = '}'\n        environ = os.environ\n    i = 0\n    while True:\n        m = search(path, i)\n        if not m:\n            break\n        i, j = m.span(0)\n        name = m.group(1)\n        if name.startswith(start) and name.endswith(end):\n            name = name[1:-1]\n        try:\n            if environ is None:\n                value = os.fsencode(os.environ[os.fsdecode(name)])\n            else:\n                value = environ[name]\n        except KeyError:\n            i = j\n        else:\n            tail = path[j:]\n            path = path[:i] + value\n            i = len(path)\n            path += tail\n    return path\n\n\n# Normalize a path, e.g. A//B, A/./B and A/foo/../B all become A/B.\n# It should be understood that this may change the meaning of the path\n# if it contains symbolic links!\n\ntry:\n    from posix import _path_normpath as normpath\n\nexcept ImportError:\n    def normpath(path):\n        \"\"\"Normalize path, eliminating double slashes, etc.\"\"\"\n        path = os.fspath(path)\n        if isinstance(path, bytes):\n            sep = b'/'\n            empty = b''\n            dot = b'.'\n            dotdot = b'..'\n        else:\n            sep = '/'\n            empty = ''\n            dot = '.'\n            dotdot = '..'\n        if path == empty:\n            return dot\n        _, initial_slashes, path = splitroot(path)\n        comps = path.split(sep)\n        new_comps = []\n        for comp in comps:\n            if comp in (empty, dot):\n                continue\n            if (comp != dotdot or (not initial_slashes and not new_comps) or\n                 (new_comps and new_comps[-1] == dotdot)):\n                new_comps.append(comp)\n            elif new_comps:\n                new_comps.pop()\n        comps = new_comps\n        path = initial_slashes + sep.join(comps)\n        return path or dot\n\n\ndef abspath(path):\n    \"\"\"Return an absolute path.\"\"\"\n    path = os.fspath(path)\n    if not isabs(path):\n        if isinstance(path, bytes):\n            cwd = os.getcwdb()\n        else:\n            cwd = os.getcwd()\n        path = join(cwd, path)\n    return normpath(path)\n\n\n# Return a canonical path (i.e. the absolute location of a file on the\n# filesystem).\n\ndef realpath(filename, *, strict=False):\n    \"\"\"Return the canonical path of the specified filename, eliminating any\nsymbolic links encountered in the path.\"\"\"\n    filename = os.fspath(filename)\n    path, ok = _joinrealpath(filename[:0], filename, strict, {})\n    return abspath(path)\n\n# Join two paths, normalizing and eliminating any symbolic links\n# encountered in the second path.\ndef _joinrealpath(path, rest, strict, seen):\n    if isinstance(path, bytes):\n        sep = b'/'\n        curdir = b'.'\n        pardir = b'..'\n    else:\n        sep = '/'\n        curdir = '.'\n        pardir = '..'\n        getcwd = os.getcwd\n    if strict is ALLOW_MISSING:\n        ignored_error = FileNotFoundError\n    elif strict:\n        ignored_error = ()\n    else:\n        ignored_error = OSError\n\n    maxlinks = None\n\n    if isabs(rest):\n        rest = rest[1:]\n        path = sep\n\n    while rest:\n        name, _, rest = rest.partition(sep)\n        if not name or name == curdir:\n            # current dir\n            continue\n        if name == pardir:\n            # parent dir\n            if path:\n                path, name = split(path)\n                if name == pardir:\n                    path = join(path, pardir, pardir)\n            else:\n                path = pardir\n            continue\n        newpath = join(path, name)\n        try:\n            st = os.lstat(newpath)\n        except ignored_error:\n            is_link = False\n        else:\n            is_link = stat.S_ISLNK(st.st_mode)\n        if not is_link:\n            path = newpath\n            continue\n        # Resolve the symbolic link\n        if newpath in seen:\n            # Already seen this path\n            path = seen[newpath]\n            if path is not None:\n                # use cached value\n                continue\n            # The symlink is not resolved, so we must have a symlink loop.\n            if strict:\n                # Raise OSError(errno.ELOOP)\n                os.stat(newpath)\n            else:\n                # Return already resolved part + rest of the path unchanged.\n                return join(newpath, rest), False\n        seen[newpath] = None # not resolved symlink\n        path, ok = _joinrealpath(path, os.readlink(newpath), strict, seen)\n        if not ok:\n            return join(path, rest), False\n        seen[newpath] = path # resolved symlink\n\n    return path, True\n\n\nsupports_unicode_filenames = (sys.platform == 'darwin')\n\ndef relpath(path, start=None):\n    \"\"\"Return a relative version of a path\"\"\"\n\n    if not path:\n        raise ValueError(\"no path specified\")\n\n    path = os.fspath(path)\n    if isinstance(path, bytes):\n        curdir = b'.'\n        sep = b'/'\n        pardir = b'..'\n    else:\n        curdir = '.'\n        sep = '/'\n        pardir = '..'\n\n    if start is None:\n        start = curdir\n    else:\n        start = os.fspath(start)\n\n    try:\n        start_list = [x for x in abspath(start).split(sep) if x]\n        path_list = [x for x in abspath(path).split(sep) if x]\n        # Work out how much of the filepath is shared by start and path.\n        i = len(commonprefix([start_list, path_list]))\n\n        rel_list = [pardir] * (len(start_list)-i) + path_list[i:]\n        if not rel_list:\n            return curdir\n        return join(*rel_list)\n    except (TypeError, AttributeError, BytesWarning, DeprecationWarning):\n        genericpath._check_arg_types('relpath', path, start)\n        raise\n\n\n# Return the longest common sub-path of the sequence of paths given as input.\n# The paths are not normalized before comparing them (this is the\n# responsibility of the caller). Any trailing separator is stripped from the\n# returned path.\n\ndef commonpath(paths):\n    \"\"\"Given a sequence of path names, returns the longest common sub-path.\"\"\"\n\n    if not paths:\n        raise ValueError('commonpath() arg is an empty sequence')\n\n    paths = tuple(map(os.fspath, paths))\n    if isinstance(paths[0], bytes):\n        sep = b'/'\n        curdir = b'.'\n    else:\n        sep = '/'\n        curdir = '.'\n\n    try:\n        split_paths = [path.split(sep) for path in paths]\n\n        try:\n            isabs, = set(p[:1] == sep for p in paths)\n        except ValueError:\n            raise ValueError(\"Can't mix absolute and relative paths\") from None\n\n        split_paths = [[c for c in s if c and c != curdir] for s in split_paths]\n        s1 = min(split_paths)\n        s2 = max(split_paths)\n        common = s1\n        for i, c in enumerate(s1):\n            if c != s2[i]:\n                common = s1[:i]\n                break\n\n        prefix = sep if isabs else sep[:0]\n        return prefix + sep.join(common)\n    except (TypeError, AttributeError):\n        genericpath._check_arg_types('commonpath', *paths)\n        raise\n", 580], "/root/miniconda3/envs/gs-lightning/lib/python3.12/shutil.py": ["\"\"\"Utility functions for copying and archiving files and directory trees.\n\nXXX The functions here don't copy the resource fork or other metadata on Mac.\n\n\"\"\"\n\nimport os\nimport sys\nimport stat\nimport fnmatch\nimport collections\nimport errno\nimport warnings\n\ntry:\n    import zlib\n    del zlib\n    _ZLIB_SUPPORTED = True\nexcept ImportError:\n    _ZLIB_SUPPORTED = False\n\ntry:\n    import bz2\n    del bz2\n    _BZ2_SUPPORTED = True\nexcept ImportError:\n    _BZ2_SUPPORTED = False\n\ntry:\n    import lzma\n    del lzma\n    _LZMA_SUPPORTED = True\nexcept ImportError:\n    _LZMA_SUPPORTED = False\n\n_WINDOWS = os.name == 'nt'\nposix = nt = None\nif os.name == 'posix':\n    import posix\nelif _WINDOWS:\n    import nt\n\nif sys.platform == 'win32':\n    import _winapi\nelse:\n    _winapi = None\n\nCOPY_BUFSIZE = 1024 * 1024 if _WINDOWS else 64 * 1024\n# This should never be removed, see rationale in:\n# https://bugs.python.org/issue43743#msg393429\n_USE_CP_SENDFILE = hasattr(os, \"sendfile\") and sys.platform.startswith(\"linux\")\n_HAS_FCOPYFILE = posix and hasattr(posix, \"_fcopyfile\")  # macOS\n\n# CMD defaults in Windows 10\n_WIN_DEFAULT_PATHEXT = \".COM;.EXE;.BAT;.CMD;.VBS;.JS;.WS;.MSC\"\n\n__all__ = [\"copyfileobj\", \"copyfile\", \"copymode\", \"copystat\", \"copy\", \"copy2\",\n           \"copytree\", \"move\", \"rmtree\", \"Error\", \"SpecialFileError\",\n           \"ExecError\", \"make_archive\", \"get_archive_formats\",\n           \"register_archive_format\", \"unregister_archive_format\",\n           \"get_unpack_formats\", \"register_unpack_format\",\n           \"unregister_unpack_format\", \"unpack_archive\",\n           \"ignore_patterns\", \"chown\", \"which\", \"get_terminal_size\",\n           \"SameFileError\"]\n           # disk_usage is added later, if available on the platform\n\nclass Error(OSError):\n    pass\n\nclass SameFileError(Error):\n    \"\"\"Raised when source and destination are the same file.\"\"\"\n\nclass SpecialFileError(OSError):\n    \"\"\"Raised when trying to do a kind of operation (e.g. copying) which is\n    not supported on a special file (e.g. a named pipe)\"\"\"\n\nclass ExecError(OSError):\n    \"\"\"Raised when a command could not be executed\"\"\"\n\nclass ReadError(OSError):\n    \"\"\"Raised when an archive cannot be read\"\"\"\n\nclass RegistryError(Exception):\n    \"\"\"Raised when a registry operation with the archiving\n    and unpacking registries fails\"\"\"\n\nclass _GiveupOnFastCopy(Exception):\n    \"\"\"Raised as a signal to fallback on using raw read()/write()\n    file copy when fast-copy functions fail to do so.\n    \"\"\"\n\ndef _fastcopy_fcopyfile(fsrc, fdst, flags):\n    \"\"\"Copy a regular file content or metadata by using high-performance\n    fcopyfile(3) syscall (macOS).\n    \"\"\"\n    try:\n        infd = fsrc.fileno()\n        outfd = fdst.fileno()\n    except Exception as err:\n        raise _GiveupOnFastCopy(err)  # not a regular file\n\n    try:\n        posix._fcopyfile(infd, outfd, flags)\n    except OSError as err:\n        err.filename = fsrc.name\n        err.filename2 = fdst.name\n        if err.errno in {errno.EINVAL, errno.ENOTSUP}:\n            raise _GiveupOnFastCopy(err)\n        else:\n            raise err from None\n\ndef _fastcopy_sendfile(fsrc, fdst):\n    \"\"\"Copy data from one regular mmap-like fd to another by using\n    high-performance sendfile(2) syscall.\n    This should work on Linux >= 2.6.33 only.\n    \"\"\"\n    # Note: copyfileobj() is left alone in order to not introduce any\n    # unexpected breakage. Possible risks by using zero-copy calls\n    # in copyfileobj() are:\n    # - fdst cannot be open in \"a\"(ppend) mode\n    # - fsrc and fdst may be open in \"t\"(ext) mode\n    # - fsrc may be a BufferedReader (which hides unread data in a buffer),\n    #   GzipFile (which decompresses data), HTTPResponse (which decodes\n    #   chunks).\n    # - possibly others (e.g. encrypted fs/partition?)\n    global _USE_CP_SENDFILE\n    try:\n        infd = fsrc.fileno()\n        outfd = fdst.fileno()\n    except Exception as err:\n        raise _GiveupOnFastCopy(err)  # not a regular file\n\n    # Hopefully the whole file will be copied in a single call.\n    # sendfile() is called in a loop 'till EOF is reached (0 return)\n    # so a bufsize smaller or bigger than the actual file size\n    # should not make any difference, also in case the file content\n    # changes while being copied.\n    try:\n        blocksize = max(os.fstat(infd).st_size, 2 ** 23)  # min 8MiB\n    except OSError:\n        blocksize = 2 ** 27  # 128MiB\n    # On 32-bit architectures truncate to 1GiB to avoid OverflowError,\n    # see bpo-38319.\n    if sys.maxsize < 2 ** 32:\n        blocksize = min(blocksize, 2 ** 30)\n\n    offset = 0\n    while True:\n        try:\n            sent = os.sendfile(outfd, infd, offset, blocksize)\n        except OSError as err:\n            # ...in oder to have a more informative exception.\n            err.filename = fsrc.name\n            err.filename2 = fdst.name\n\n            if err.errno == errno.ENOTSOCK:\n                # sendfile() on this platform (probably Linux < 2.6.33)\n                # does not support copies between regular files (only\n                # sockets).\n                _USE_CP_SENDFILE = False\n                raise _GiveupOnFastCopy(err)\n\n            if err.errno == errno.ENOSPC:  # filesystem is full\n                raise err from None\n\n            # Give up on first call and if no data was copied.\n            if offset == 0 and os.lseek(outfd, 0, os.SEEK_CUR) == 0:\n                raise _GiveupOnFastCopy(err)\n\n            raise err\n        else:\n            if sent == 0:\n                break  # EOF\n            offset += sent\n\ndef _copyfileobj_readinto(fsrc, fdst, length=COPY_BUFSIZE):\n    \"\"\"readinto()/memoryview() based variant of copyfileobj().\n    *fsrc* must support readinto() method and both files must be\n    open in binary mode.\n    \"\"\"\n    # Localize variable access to minimize overhead.\n    fsrc_readinto = fsrc.readinto\n    fdst_write = fdst.write\n    with memoryview(bytearray(length)) as mv:\n        while True:\n            n = fsrc_readinto(mv)\n            if not n:\n                break\n            elif n < length:\n                with mv[:n] as smv:\n                    fdst_write(smv)\n                break\n            else:\n                fdst_write(mv)\n\ndef copyfileobj(fsrc, fdst, length=0):\n    \"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\"\n    if not length:\n        length = COPY_BUFSIZE\n    # Localize variable access to minimize overhead.\n    fsrc_read = fsrc.read\n    fdst_write = fdst.write\n    while buf := fsrc_read(length):\n        fdst_write(buf)\n\ndef _samefile(src, dst):\n    # Macintosh, Unix.\n    if isinstance(src, os.DirEntry) and hasattr(os.path, 'samestat'):\n        try:\n            return os.path.samestat(src.stat(), os.stat(dst))\n        except OSError:\n            return False\n\n    if hasattr(os.path, 'samefile'):\n        try:\n            return os.path.samefile(src, dst)\n        except OSError:\n            return False\n\n    # All other platforms: check for same pathname.\n    return (os.path.normcase(os.path.abspath(src)) ==\n            os.path.normcase(os.path.abspath(dst)))\n\ndef _stat(fn):\n    return fn.stat() if isinstance(fn, os.DirEntry) else os.stat(fn)\n\ndef _islink(fn):\n    return fn.is_symlink() if isinstance(fn, os.DirEntry) else os.path.islink(fn)\n\ndef copyfile(src, dst, *, follow_symlinks=True):\n    \"\"\"Copy data from src to dst in the most efficient way possible.\n\n    If follow_symlinks is not set and src is a symbolic link, a new\n    symlink will be created instead of copying the file it points to.\n\n    \"\"\"\n    sys.audit(\"shutil.copyfile\", src, dst)\n\n    if _samefile(src, dst):\n        raise SameFileError(\"{!r} and {!r} are the same file\".format(src, dst))\n\n    file_size = 0\n    for i, fn in enumerate([src, dst]):\n        try:\n            st = _stat(fn)\n        except OSError:\n            # File most likely does not exist\n            pass\n        else:\n            # XXX What about other special files? (sockets, devices...)\n            if stat.S_ISFIFO(st.st_mode):\n                fn = fn.path if isinstance(fn, os.DirEntry) else fn\n                raise SpecialFileError(\"`%s` is a named pipe\" % fn)\n            if _WINDOWS and i == 0:\n                file_size = st.st_size\n\n    if not follow_symlinks and _islink(src):\n        os.symlink(os.readlink(src), dst)\n    else:\n        with open(src, 'rb') as fsrc:\n            try:\n                with open(dst, 'wb') as fdst:\n                    # macOS\n                    if _HAS_FCOPYFILE:\n                        try:\n                            _fastcopy_fcopyfile(fsrc, fdst, posix._COPYFILE_DATA)\n                            return dst\n                        except _GiveupOnFastCopy:\n                            pass\n                    # Linux\n                    elif _USE_CP_SENDFILE:\n                        try:\n                            _fastcopy_sendfile(fsrc, fdst)\n                            return dst\n                        except _GiveupOnFastCopy:\n                            pass\n                    # Windows, see:\n                    # https://github.com/python/cpython/pull/7160#discussion_r195405230\n                    elif _WINDOWS and file_size > 0:\n                        _copyfileobj_readinto(fsrc, fdst, min(file_size, COPY_BUFSIZE))\n                        return dst\n\n                    copyfileobj(fsrc, fdst)\n\n            # Issue 43219, raise a less confusing exception\n            except IsADirectoryError as e:\n                if not os.path.exists(dst):\n                    raise FileNotFoundError(f'Directory does not exist: {dst}') from e\n                else:\n                    raise\n\n    return dst\n\ndef copymode(src, dst, *, follow_symlinks=True):\n    \"\"\"Copy mode bits from src to dst.\n\n    If follow_symlinks is not set, symlinks aren't followed if and only\n    if both `src` and `dst` are symlinks.  If `lchmod` isn't available\n    (e.g. Linux) this method does nothing.\n\n    \"\"\"\n    sys.audit(\"shutil.copymode\", src, dst)\n\n    if not follow_symlinks and _islink(src) and os.path.islink(dst):\n        if os.name == 'nt':\n            stat_func, chmod_func = os.lstat, os.chmod\n        elif hasattr(os, 'lchmod'):\n            stat_func, chmod_func = os.lstat, os.lchmod\n        else:\n            return\n    else:\n        if os.name == 'nt' and os.path.islink(dst):\n            dst = os.path.realpath(dst, strict=True)\n        stat_func, chmod_func = _stat, os.chmod\n\n    st = stat_func(src)\n    chmod_func(dst, stat.S_IMODE(st.st_mode))\n\nif hasattr(os, 'listxattr'):\n    def _copyxattr(src, dst, *, follow_symlinks=True):\n        \"\"\"Copy extended filesystem attributes from `src` to `dst`.\n\n        Overwrite existing attributes.\n\n        If `follow_symlinks` is false, symlinks won't be followed.\n\n        \"\"\"\n\n        try:\n            names = os.listxattr(src, follow_symlinks=follow_symlinks)\n        except OSError as e:\n            if e.errno not in (errno.ENOTSUP, errno.ENODATA, errno.EINVAL):\n                raise\n            return\n        for name in names:\n            try:\n                value = os.getxattr(src, name, follow_symlinks=follow_symlinks)\n                os.setxattr(dst, name, value, follow_symlinks=follow_symlinks)\n            except OSError as e:\n                if e.errno not in (errno.EPERM, errno.ENOTSUP, errno.ENODATA,\n                                   errno.EINVAL, errno.EACCES):\n                    raise\nelse:\n    def _copyxattr(*args, **kwargs):\n        pass\n\ndef copystat(src, dst, *, follow_symlinks=True):\n    \"\"\"Copy file metadata\n\n    Copy the permission bits, last access time, last modification time, and\n    flags from `src` to `dst`. On Linux, copystat() also copies the \"extended\n    attributes\" where possible. The file contents, owner, and group are\n    unaffected. `src` and `dst` are path-like objects or path names given as\n    strings.\n\n    If the optional flag `follow_symlinks` is not set, symlinks aren't\n    followed if and only if both `src` and `dst` are symlinks.\n    \"\"\"\n    sys.audit(\"shutil.copystat\", src, dst)\n\n    def _nop(*args, ns=None, follow_symlinks=None):\n        pass\n\n    # follow symlinks (aka don't not follow symlinks)\n    follow = follow_symlinks or not (_islink(src) and os.path.islink(dst))\n    if follow:\n        # use the real function if it exists\n        def lookup(name):\n            return getattr(os, name, _nop)\n    else:\n        # use the real function only if it exists\n        # *and* it supports follow_symlinks\n        def lookup(name):\n            fn = getattr(os, name, _nop)\n            if fn in os.supports_follow_symlinks:\n                return fn\n            return _nop\n\n    if isinstance(src, os.DirEntry):\n        st = src.stat(follow_symlinks=follow)\n    else:\n        st = lookup(\"stat\")(src, follow_symlinks=follow)\n    mode = stat.S_IMODE(st.st_mode)\n    lookup(\"utime\")(dst, ns=(st.st_atime_ns, st.st_mtime_ns),\n        follow_symlinks=follow)\n    # We must copy extended attributes before the file is (potentially)\n    # chmod()'ed read-only, otherwise setxattr() will error with -EACCES.\n    _copyxattr(src, dst, follow_symlinks=follow)\n    _chmod = lookup(\"chmod\")\n    if os.name == 'nt':\n        if follow:\n            if os.path.islink(dst):\n                dst = os.path.realpath(dst, strict=True)\n        else:\n            def _chmod(*args, **kwargs):\n                os.chmod(*args)\n    try:\n        _chmod(dst, mode, follow_symlinks=follow)\n    except NotImplementedError:\n        # if we got a NotImplementedError, it's because\n        #   * follow_symlinks=False,\n        #   * lchown() is unavailable, and\n        #   * either\n        #       * fchownat() is unavailable or\n        #       * fchownat() doesn't implement AT_SYMLINK_NOFOLLOW.\n        #         (it returned ENOSUP.)\n        # therefore we're out of options--we simply cannot chown the\n        # symlink.  give up, suppress the error.\n        # (which is what shutil always did in this circumstance.)\n        pass\n    if hasattr(st, 'st_flags'):\n        try:\n            lookup(\"chflags\")(dst, st.st_flags, follow_symlinks=follow)\n        except OSError as why:\n            for err in 'EOPNOTSUPP', 'ENOTSUP':\n                if hasattr(errno, err) and why.errno == getattr(errno, err):\n                    break\n            else:\n                raise\n\ndef copy(src, dst, *, follow_symlinks=True):\n    \"\"\"Copy data and mode bits (\"cp src dst\"). Return the file's destination.\n\n    The destination may be a directory.\n\n    If follow_symlinks is false, symlinks won't be followed. This\n    resembles GNU's \"cp -P src dst\".\n\n    If source and destination are the same file, a SameFileError will be\n    raised.\n\n    \"\"\"\n    if os.path.isdir(dst):\n        dst = os.path.join(dst, os.path.basename(src))\n    copyfile(src, dst, follow_symlinks=follow_symlinks)\n    copymode(src, dst, follow_symlinks=follow_symlinks)\n    return dst\n\ndef copy2(src, dst, *, follow_symlinks=True):\n    \"\"\"Copy data and metadata. Return the file's destination.\n\n    Metadata is copied with copystat(). Please see the copystat function\n    for more information.\n\n    The destination may be a directory.\n\n    If follow_symlinks is false, symlinks won't be followed. This\n    resembles GNU's \"cp -P src dst\".\n    \"\"\"\n    if os.path.isdir(dst):\n        dst = os.path.join(dst, os.path.basename(src))\n\n    if hasattr(_winapi, \"CopyFile2\"):\n        src_ = os.fsdecode(src)\n        dst_ = os.fsdecode(dst)\n        flags = _winapi.COPY_FILE_ALLOW_DECRYPTED_DESTINATION # for compat\n        if not follow_symlinks:\n            flags |= _winapi.COPY_FILE_COPY_SYMLINK\n        try:\n            _winapi.CopyFile2(src_, dst_, flags)\n            return dst\n        except OSError as exc:\n            if (exc.winerror == _winapi.ERROR_PRIVILEGE_NOT_HELD\n                and not follow_symlinks):\n                # Likely encountered a symlink we aren't allowed to create.\n                # Fall back on the old code\n                pass\n            elif exc.winerror == _winapi.ERROR_ACCESS_DENIED:\n                # Possibly encountered a hidden or readonly file we can't\n                # overwrite. Fall back on old code\n                pass\n            else:\n                raise\n\n    copyfile(src, dst, follow_symlinks=follow_symlinks)\n    copystat(src, dst, follow_symlinks=follow_symlinks)\n    return dst\n\ndef ignore_patterns(*patterns):\n    \"\"\"Function that can be used as copytree() ignore parameter.\n\n    Patterns is a sequence of glob-style patterns\n    that are used to exclude files\"\"\"\n    def _ignore_patterns(path, names):\n        ignored_names = []\n        for pattern in patterns:\n            ignored_names.extend(fnmatch.filter(names, pattern))\n        return set(ignored_names)\n    return _ignore_patterns\n\ndef _copytree(entries, src, dst, symlinks, ignore, copy_function,\n              ignore_dangling_symlinks, dirs_exist_ok=False):\n    if ignore is not None:\n        ignored_names = ignore(os.fspath(src), [x.name for x in entries])\n    else:\n        ignored_names = ()\n\n    os.makedirs(dst, exist_ok=dirs_exist_ok)\n    errors = []\n    use_srcentry = copy_function is copy2 or copy_function is copy\n\n    for srcentry in entries:\n        if srcentry.name in ignored_names:\n            continue\n        srcname = os.path.join(src, srcentry.name)\n        dstname = os.path.join(dst, srcentry.name)\n        srcobj = srcentry if use_srcentry else srcname\n        try:\n            is_symlink = srcentry.is_symlink()\n            if is_symlink and os.name == 'nt':\n                # Special check for directory junctions, which appear as\n                # symlinks but we want to recurse.\n                lstat = srcentry.stat(follow_symlinks=False)\n                if lstat.st_reparse_tag == stat.IO_REPARSE_TAG_MOUNT_POINT:\n                    is_symlink = False\n            if is_symlink:\n                linkto = os.readlink(srcname)\n                if symlinks:\n                    # We can't just leave it to `copy_function` because legacy\n                    # code with a custom `copy_function` may rely on copytree\n                    # doing the right thing.\n                    os.symlink(linkto, dstname)\n                    copystat(srcobj, dstname, follow_symlinks=not symlinks)\n                else:\n                    # ignore dangling symlink if the flag is on\n                    if not os.path.exists(linkto) and ignore_dangling_symlinks:\n                        continue\n                    # otherwise let the copy occur. copy2 will raise an error\n                    if srcentry.is_dir():\n                        copytree(srcobj, dstname, symlinks, ignore,\n                                 copy_function, ignore_dangling_symlinks,\n                                 dirs_exist_ok)\n                    else:\n                        copy_function(srcobj, dstname)\n            elif srcentry.is_dir():\n                copytree(srcobj, dstname, symlinks, ignore, copy_function,\n                         ignore_dangling_symlinks, dirs_exist_ok)\n            else:\n                # Will raise a SpecialFileError for unsupported file types\n                copy_function(srcobj, dstname)\n        # catch the Error from the recursive copytree so that we can\n        # continue with other files\n        except Error as err:\n            errors.extend(err.args[0])\n        except OSError as why:\n            errors.append((srcname, dstname, str(why)))\n    try:\n        copystat(src, dst)\n    except OSError as why:\n        # Copying file access times may fail on Windows\n        if getattr(why, 'winerror', None) is None:\n            errors.append((src, dst, str(why)))\n    if errors:\n        raise Error(errors)\n    return dst\n\ndef copytree(src, dst, symlinks=False, ignore=None, copy_function=copy2,\n             ignore_dangling_symlinks=False, dirs_exist_ok=False):\n    \"\"\"Recursively copy a directory tree and return the destination directory.\n\n    If exception(s) occur, an Error is raised with a list of reasons.\n\n    If the optional symlinks flag is true, symbolic links in the\n    source tree result in symbolic links in the destination tree; if\n    it is false, the contents of the files pointed to by symbolic\n    links are copied. If the file pointed to by the symlink doesn't\n    exist, an exception will be added in the list of errors raised in\n    an Error exception at the end of the copy process.\n\n    You can set the optional ignore_dangling_symlinks flag to true if you\n    want to silence this exception. Notice that this has no effect on\n    platforms that don't support os.symlink.\n\n    The optional ignore argument is a callable. If given, it\n    is called with the `src` parameter, which is the directory\n    being visited by copytree(), and `names` which is the list of\n    `src` contents, as returned by os.listdir():\n\n        callable(src, names) -> ignored_names\n\n    Since copytree() is called recursively, the callable will be\n    called once for each directory that is copied. It returns a\n    list of names relative to the `src` directory that should\n    not be copied.\n\n    The optional copy_function argument is a callable that will be used\n    to copy each file. It will be called with the source path and the\n    destination path as arguments. By default, copy2() is used, but any\n    function that supports the same signature (like copy()) can be used.\n\n    If dirs_exist_ok is false (the default) and `dst` already exists, a\n    `FileExistsError` is raised. If `dirs_exist_ok` is true, the copying\n    operation will continue if it encounters existing directories, and files\n    within the `dst` tree will be overwritten by corresponding files from the\n    `src` tree.\n    \"\"\"\n    sys.audit(\"shutil.copytree\", src, dst)\n    with os.scandir(src) as itr:\n        entries = list(itr)\n    return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n                     ignore=ignore, copy_function=copy_function,\n                     ignore_dangling_symlinks=ignore_dangling_symlinks,\n                     dirs_exist_ok=dirs_exist_ok)\n\nif hasattr(os.stat_result, 'st_file_attributes'):\n    def _rmtree_islink(path):\n        try:\n            st = os.lstat(path)\n            return (stat.S_ISLNK(st.st_mode) or\n                (st.st_file_attributes & stat.FILE_ATTRIBUTE_REPARSE_POINT\n                 and st.st_reparse_tag == stat.IO_REPARSE_TAG_MOUNT_POINT))\n        except OSError:\n            return False\nelse:\n    def _rmtree_islink(path):\n        return os.path.islink(path)\n\n# version vulnerable to race conditions\ndef _rmtree_unsafe(path, onexc):\n    def onerror(err):\n        onexc(os.scandir, err.filename, err)\n    results = os.walk(path, topdown=False, onerror=onerror, followlinks=os._walk_symlinks_as_files)\n    for dirpath, dirnames, filenames in results:\n        for name in dirnames:\n            fullname = os.path.join(dirpath, name)\n            try:\n                os.rmdir(fullname)\n            except OSError as err:\n                onexc(os.rmdir, fullname, err)\n        for name in filenames:\n            fullname = os.path.join(dirpath, name)\n            try:\n                os.unlink(fullname)\n            except OSError as err:\n                onexc(os.unlink, fullname, err)\n    try:\n        os.rmdir(path)\n    except OSError as err:\n        onexc(os.rmdir, path, err)\n\n# Version using fd-based APIs to protect against races\ndef _rmtree_safe_fd(stack, onexc):\n    # Each stack item has four elements:\n    # * func: The first operation to perform: os.lstat, os.close or os.rmdir.\n    #   Walking a directory starts with an os.lstat() to detect symlinks; in\n    #   this case, func is updated before subsequent operations and passed to\n    #   onexc() if an error occurs.\n    # * dirfd: Open file descriptor, or None if we're processing the top-level\n    #   directory given to rmtree() and the user didn't supply dir_fd.\n    # * path: Path of file to operate upon. This is passed to onexc() if an\n    #   error occurs.\n    # * orig_entry: os.DirEntry, or None if we're processing the top-level\n    #   directory given to rmtree(). We used the cached stat() of the entry to\n    #   save a call to os.lstat() when walking subdirectories.\n    func, dirfd, path, orig_entry = stack.pop()\n    name = path if orig_entry is None else orig_entry.name\n    try:\n        if func is os.close:\n            os.close(dirfd)\n            return\n        if func is os.rmdir:\n            os.rmdir(name, dir_fd=dirfd)\n            return\n\n        # Note: To guard against symlink races, we use the standard\n        # lstat()/open()/fstat() trick.\n        assert func is os.lstat\n        if orig_entry is None:\n            orig_st = os.lstat(name, dir_fd=dirfd)\n        else:\n            orig_st = orig_entry.stat(follow_symlinks=False)\n\n        func = os.open  # For error reporting.\n        topfd = os.open(name, os.O_RDONLY | os.O_NONBLOCK, dir_fd=dirfd)\n\n        func = os.path.islink  # For error reporting.\n        try:\n            if not os.path.samestat(orig_st, os.fstat(topfd)):\n                # Symlinks to directories are forbidden, see GH-46010.\n                raise OSError(\"Cannot call rmtree on a symbolic link\")\n            stack.append((os.rmdir, dirfd, path, orig_entry))\n        finally:\n            stack.append((os.close, topfd, path, orig_entry))\n\n        func = os.scandir  # For error reporting.\n        with os.scandir(topfd) as scandir_it:\n            entries = list(scandir_it)\n        for entry in entries:\n            fullname = os.path.join(path, entry.name)\n            try:\n                if entry.is_dir(follow_symlinks=False):\n                    # Traverse into sub-directory.\n                    stack.append((os.lstat, topfd, fullname, entry))\n                    continue\n            except OSError:\n                pass\n            try:\n                os.unlink(entry.name, dir_fd=topfd)\n            except OSError as err:\n                onexc(os.unlink, fullname, err)\n    except OSError as err:\n        err.filename = path\n        onexc(func, path, err)\n\n_use_fd_functions = ({os.open, os.stat, os.unlink, os.rmdir} <=\n                     os.supports_dir_fd and\n                     os.scandir in os.supports_fd and\n                     os.stat in os.supports_follow_symlinks)\n\ndef rmtree(path, ignore_errors=False, onerror=None, *, onexc=None, dir_fd=None):\n    \"\"\"Recursively delete a directory tree.\n\n    If dir_fd is not None, it should be a file descriptor open to a directory;\n    path will then be relative to that directory.\n    dir_fd may not be implemented on your platform.\n    If it is unavailable, using it will raise a NotImplementedError.\n\n    If ignore_errors is set, errors are ignored; otherwise, if onexc or\n    onerror is set, it is called to handle the error with arguments (func,\n    path, exc_info) where func is platform and implementation dependent;\n    path is the argument to that function that caused it to fail; and\n    the value of exc_info describes the exception. For onexc it is the\n    exception instance, and for onerror it is a tuple as returned by\n    sys.exc_info().  If ignore_errors is false and both onexc and\n    onerror are None, the exception is reraised.\n\n    onerror is deprecated and only remains for backwards compatibility.\n    If both onerror and onexc are set, onerror is ignored and onexc is used.\n    \"\"\"\n\n    sys.audit(\"shutil.rmtree\", path, dir_fd)\n    if ignore_errors:\n        def onexc(*args):\n            pass\n    elif onerror is None and onexc is None:\n        def onexc(*args):\n            raise\n    elif onexc is None:\n        if onerror is None:\n            def onexc(*args):\n                raise\n        else:\n            # delegate to onerror\n            def onexc(*args):\n                func, path, exc = args\n                if exc is None:\n                    exc_info = None, None, None\n                else:\n                    exc_info = type(exc), exc, exc.__traceback__\n                return onerror(func, path, exc_info)\n\n    if _use_fd_functions:\n        # While the unsafe rmtree works fine on bytes, the fd based does not.\n        if isinstance(path, bytes):\n            path = os.fsdecode(path)\n        stack = [(os.lstat, dir_fd, path, None)]\n        try:\n            while stack:\n                _rmtree_safe_fd(stack, onexc)\n        finally:\n            # Close any file descriptors still on the stack.\n            while stack:\n                func, fd, path, entry = stack.pop()\n                if func is not os.close:\n                    continue\n                try:\n                    os.close(fd)\n                except OSError as err:\n                    onexc(os.close, path, err)\n    else:\n        if dir_fd is not None:\n            raise NotImplementedError(\"dir_fd unavailable on this platform\")\n        try:\n            if _rmtree_islink(path):\n                # symlinks to directories are forbidden, see bug #1669\n                raise OSError(\"Cannot call rmtree on a symbolic link\")\n        except OSError as err:\n            onexc(os.path.islink, path, err)\n            # can't continue even if onexc hook returns\n            return\n        return _rmtree_unsafe(path, onexc)\n\n# Allow introspection of whether or not the hardening against symlink\n# attacks is supported on the current platform\nrmtree.avoids_symlink_attacks = _use_fd_functions\n\ndef _basename(path):\n    \"\"\"A basename() variant which first strips the trailing slash, if present.\n    Thus we always get the last component of the path, even for directories.\n\n    path: Union[PathLike, str]\n\n    e.g.\n    >>> os.path.basename('/bar/foo')\n    'foo'\n    >>> os.path.basename('/bar/foo/')\n    ''\n    >>> _basename('/bar/foo/')\n    'foo'\n    \"\"\"\n    path = os.fspath(path)\n    sep = os.path.sep + (os.path.altsep or '')\n    return os.path.basename(path.rstrip(sep))\n\ndef move(src, dst, copy_function=copy2):\n    \"\"\"Recursively move a file or directory to another location. This is\n    similar to the Unix \"mv\" command. Return the file or directory's\n    destination.\n\n    If dst is an existing directory or a symlink to a directory, then src is\n    moved inside that directory. The destination path in that directory must\n    not already exist.\n\n    If dst already exists but is not a directory, it may be overwritten\n    depending on os.rename() semantics.\n\n    If the destination is on our current filesystem, then rename() is used.\n    Otherwise, src is copied to the destination and then removed. Symlinks are\n    recreated under the new name if os.rename() fails because of cross\n    filesystem renames.\n\n    The optional `copy_function` argument is a callable that will be used\n    to copy the source or it will be delegated to `copytree`.\n    By default, copy2() is used, but any function that supports the same\n    signature (like copy()) can be used.\n\n    A lot more could be done here...  A look at a mv.c shows a lot of\n    the issues this implementation glosses over.\n\n    \"\"\"\n    sys.audit(\"shutil.move\", src, dst)\n    real_dst = dst\n    if os.path.isdir(dst):\n        if _samefile(src, dst) and not os.path.islink(src):\n            # We might be on a case insensitive filesystem,\n            # perform the rename anyway.\n            os.rename(src, dst)\n            return\n\n        # Using _basename instead of os.path.basename is important, as we must\n        # ignore any trailing slash to avoid the basename returning ''\n        real_dst = os.path.join(dst, _basename(src))\n\n        if os.path.exists(real_dst):\n            raise Error(\"Destination path '%s' already exists\" % real_dst)\n    try:\n        os.rename(src, real_dst)\n    except OSError:\n        if os.path.islink(src):\n            linkto = os.readlink(src)\n            os.symlink(linkto, real_dst)\n            os.unlink(src)\n        elif os.path.isdir(src):\n            if _destinsrc(src, dst):\n                raise Error(\"Cannot move a directory '%s' into itself\"\n                            \" '%s'.\" % (src, dst))\n            if (_is_immutable(src)\n                    or (not os.access(src, os.W_OK) and os.listdir(src)\n                        and sys.platform == 'darwin')):\n                raise PermissionError(\"Cannot move the non-empty directory \"\n                                      \"'%s': Lacking write permission to '%s'.\"\n                                      % (src, src))\n            copytree(src, real_dst, copy_function=copy_function,\n                     symlinks=True)\n            rmtree(src)\n        else:\n            copy_function(src, real_dst)\n            os.unlink(src)\n    return real_dst\n\ndef _destinsrc(src, dst):\n    src = os.path.abspath(src)\n    dst = os.path.abspath(dst)\n    if not src.endswith(os.path.sep):\n        src += os.path.sep\n    if not dst.endswith(os.path.sep):\n        dst += os.path.sep\n    return dst.startswith(src)\n\ndef _is_immutable(src):\n    st = _stat(src)\n    immutable_states = [stat.UF_IMMUTABLE, stat.SF_IMMUTABLE]\n    return hasattr(st, 'st_flags') and st.st_flags in immutable_states\n\ndef _get_gid(name):\n    \"\"\"Returns a gid, given a group name.\"\"\"\n    if name is None:\n        return None\n\n    try:\n        from grp import getgrnam\n    except ImportError:\n        return None\n\n    try:\n        result = getgrnam(name)\n    except KeyError:\n        result = None\n    if result is not None:\n        return result[2]\n    return None\n\ndef _get_uid(name):\n    \"\"\"Returns an uid, given a user name.\"\"\"\n    if name is None:\n        return None\n\n    try:\n        from pwd import getpwnam\n    except ImportError:\n        return None\n\n    try:\n        result = getpwnam(name)\n    except KeyError:\n        result = None\n    if result is not None:\n        return result[2]\n    return None\n\ndef _make_tarball(base_name, base_dir, compress=\"gzip\", verbose=0, dry_run=0,\n                  owner=None, group=None, logger=None, root_dir=None):\n    \"\"\"Create a (possibly compressed) tar file from all the files under\n    'base_dir'.\n\n    'compress' must be \"gzip\" (the default), \"bzip2\", \"xz\", or None.\n\n    'owner' and 'group' can be used to define an owner and a group for the\n    archive that is being built. If not provided, the current owner and group\n    will be used.\n\n    The output tar file will be named 'base_name' +  \".tar\", possibly plus\n    the appropriate compression extension (\".gz\", \".bz2\", or \".xz\").\n\n    Returns the output filename.\n    \"\"\"\n    if compress is None:\n        tar_compression = ''\n    elif _ZLIB_SUPPORTED and compress == 'gzip':\n        tar_compression = 'gz'\n    elif _BZ2_SUPPORTED and compress == 'bzip2':\n        tar_compression = 'bz2'\n    elif _LZMA_SUPPORTED and compress == 'xz':\n        tar_compression = 'xz'\n    else:\n        raise ValueError(\"bad value for 'compress', or compression format not \"\n                         \"supported : {0}\".format(compress))\n\n    import tarfile  # late import for breaking circular dependency\n\n    compress_ext = '.' + tar_compression if compress else ''\n    archive_name = base_name + '.tar' + compress_ext\n    archive_dir = os.path.dirname(archive_name)\n\n    if archive_dir and not os.path.exists(archive_dir):\n        if logger is not None:\n            logger.info(\"creating %s\", archive_dir)\n        if not dry_run:\n            os.makedirs(archive_dir)\n\n    # creating the tarball\n    if logger is not None:\n        logger.info('Creating tar archive')\n\n    uid = _get_uid(owner)\n    gid = _get_gid(group)\n\n    def _set_uid_gid(tarinfo):\n        if gid is not None:\n            tarinfo.gid = gid\n            tarinfo.gname = group\n        if uid is not None:\n            tarinfo.uid = uid\n            tarinfo.uname = owner\n        return tarinfo\n\n    if not dry_run:\n        tar = tarfile.open(archive_name, 'w|%s' % tar_compression)\n        arcname = base_dir\n        if root_dir is not None:\n            base_dir = os.path.join(root_dir, base_dir)\n        try:\n            tar.add(base_dir, arcname, filter=_set_uid_gid)\n        finally:\n            tar.close()\n\n    if root_dir is not None:\n        archive_name = os.path.abspath(archive_name)\n    return archive_name\n\ndef _make_zipfile(base_name, base_dir, verbose=0, dry_run=0,\n                  logger=None, owner=None, group=None, root_dir=None):\n    \"\"\"Create a zip file from all the files under 'base_dir'.\n\n    The output zip file will be named 'base_name' + \".zip\".  Returns the\n    name of the output zip file.\n    \"\"\"\n    import zipfile  # late import for breaking circular dependency\n\n    zip_filename = base_name + \".zip\"\n    archive_dir = os.path.dirname(base_name)\n\n    if archive_dir and not os.path.exists(archive_dir):\n        if logger is not None:\n            logger.info(\"creating %s\", archive_dir)\n        if not dry_run:\n            os.makedirs(archive_dir)\n\n    if logger is not None:\n        logger.info(\"creating '%s' and adding '%s' to it\",\n                    zip_filename, base_dir)\n\n    if not dry_run:\n        with zipfile.ZipFile(zip_filename, \"w\",\n                             compression=zipfile.ZIP_DEFLATED) as zf:\n            arcname = os.path.normpath(base_dir)\n            if root_dir is not None:\n                base_dir = os.path.join(root_dir, base_dir)\n            base_dir = os.path.normpath(base_dir)\n            if arcname != os.curdir:\n                zf.write(base_dir, arcname)\n                if logger is not None:\n                    logger.info(\"adding '%s'\", base_dir)\n            for dirpath, dirnames, filenames in os.walk(base_dir):\n                arcdirpath = dirpath\n                if root_dir is not None:\n                    arcdirpath = os.path.relpath(arcdirpath, root_dir)\n                arcdirpath = os.path.normpath(arcdirpath)\n                for name in sorted(dirnames):\n                    path = os.path.join(dirpath, name)\n                    arcname = os.path.join(arcdirpath, name)\n                    zf.write(path, arcname)\n                    if logger is not None:\n                        logger.info(\"adding '%s'\", path)\n                for name in filenames:\n                    path = os.path.join(dirpath, name)\n                    path = os.path.normpath(path)\n                    if os.path.isfile(path):\n                        arcname = os.path.join(arcdirpath, name)\n                        zf.write(path, arcname)\n                        if logger is not None:\n                            logger.info(\"adding '%s'\", path)\n\n    if root_dir is not None:\n        zip_filename = os.path.abspath(zip_filename)\n    return zip_filename\n\n_make_tarball.supports_root_dir = True\n_make_zipfile.supports_root_dir = True\n\n# Maps the name of the archive format to a tuple containing:\n# * the archiving function\n# * extra keyword arguments\n# * description\n_ARCHIVE_FORMATS = {\n    'tar':   (_make_tarball, [('compress', None)],\n              \"uncompressed tar file\"),\n}\n\nif _ZLIB_SUPPORTED:\n    _ARCHIVE_FORMATS['gztar'] = (_make_tarball, [('compress', 'gzip')],\n                                \"gzip'ed tar-file\")\n    _ARCHIVE_FORMATS['zip'] = (_make_zipfile, [], \"ZIP file\")\n\nif _BZ2_SUPPORTED:\n    _ARCHIVE_FORMATS['bztar'] = (_make_tarball, [('compress', 'bzip2')],\n                                \"bzip2'ed tar-file\")\n\nif _LZMA_SUPPORTED:\n    _ARCHIVE_FORMATS['xztar'] = (_make_tarball, [('compress', 'xz')],\n                                \"xz'ed tar-file\")\n\ndef get_archive_formats():\n    \"\"\"Returns a list of supported formats for archiving and unarchiving.\n\n    Each element of the returned sequence is a tuple (name, description)\n    \"\"\"\n    formats = [(name, registry[2]) for name, registry in\n               _ARCHIVE_FORMATS.items()]\n    formats.sort()\n    return formats\n\ndef register_archive_format(name, function, extra_args=None, description=''):\n    \"\"\"Registers an archive format.\n\n    name is the name of the format. function is the callable that will be\n    used to create archives. If provided, extra_args is a sequence of\n    (name, value) tuples that will be passed as arguments to the callable.\n    description can be provided to describe the format, and will be returned\n    by the get_archive_formats() function.\n    \"\"\"\n    if extra_args is None:\n        extra_args = []\n    if not callable(function):\n        raise TypeError('The %s object is not callable' % function)\n    if not isinstance(extra_args, (tuple, list)):\n        raise TypeError('extra_args needs to be a sequence')\n    for element in extra_args:\n        if not isinstance(element, (tuple, list)) or len(element) !=2:\n            raise TypeError('extra_args elements are : (arg_name, value)')\n\n    _ARCHIVE_FORMATS[name] = (function, extra_args, description)\n\ndef unregister_archive_format(name):\n    del _ARCHIVE_FORMATS[name]\n\ndef make_archive(base_name, format, root_dir=None, base_dir=None, verbose=0,\n                 dry_run=0, owner=None, group=None, logger=None):\n    \"\"\"Create an archive file (eg. zip or tar).\n\n    'base_name' is the name of the file to create, minus any format-specific\n    extension; 'format' is the archive format: one of \"zip\", \"tar\", \"gztar\",\n    \"bztar\", or \"xztar\".  Or any other registered format.\n\n    'root_dir' is a directory that will be the root directory of the\n    archive; ie. we typically chdir into 'root_dir' before creating the\n    archive.  'base_dir' is the directory where we start archiving from;\n    ie. 'base_dir' will be the common prefix of all files and\n    directories in the archive.  'root_dir' and 'base_dir' both default\n    to the current directory.  Returns the name of the archive file.\n\n    'owner' and 'group' are used when creating a tar archive. By default,\n    uses the current owner and group.\n    \"\"\"\n    sys.audit(\"shutil.make_archive\", base_name, format, root_dir, base_dir)\n    try:\n        format_info = _ARCHIVE_FORMATS[format]\n    except KeyError:\n        raise ValueError(\"unknown archive format '%s'\" % format) from None\n\n    kwargs = {'dry_run': dry_run, 'logger': logger,\n              'owner': owner, 'group': group}\n\n    func = format_info[0]\n    for arg, val in format_info[1]:\n        kwargs[arg] = val\n\n    if base_dir is None:\n        base_dir = os.curdir\n\n    supports_root_dir = getattr(func, 'supports_root_dir', False)\n    save_cwd = None\n    if root_dir is not None:\n        stmd = os.stat(root_dir).st_mode\n        if not stat.S_ISDIR(stmd):\n            raise NotADirectoryError(errno.ENOTDIR, 'Not a directory', root_dir)\n\n        if supports_root_dir:\n            # Support path-like base_name here for backwards-compatibility.\n            base_name = os.fspath(base_name)\n            kwargs['root_dir'] = root_dir\n        else:\n            save_cwd = os.getcwd()\n            if logger is not None:\n                logger.debug(\"changing into '%s'\", root_dir)\n            base_name = os.path.abspath(base_name)\n            if not dry_run:\n                os.chdir(root_dir)\n\n    try:\n        filename = func(base_name, base_dir, **kwargs)\n    finally:\n        if save_cwd is not None:\n            if logger is not None:\n                logger.debug(\"changing back to '%s'\", save_cwd)\n            os.chdir(save_cwd)\n\n    return filename\n\n\ndef get_unpack_formats():\n    \"\"\"Returns a list of supported formats for unpacking.\n\n    Each element of the returned sequence is a tuple\n    (name, extensions, description)\n    \"\"\"\n    formats = [(name, info[0], info[3]) for name, info in\n               _UNPACK_FORMATS.items()]\n    formats.sort()\n    return formats\n\ndef _check_unpack_options(extensions, function, extra_args):\n    \"\"\"Checks what gets registered as an unpacker.\"\"\"\n    # first make sure no other unpacker is registered for this extension\n    existing_extensions = {}\n    for name, info in _UNPACK_FORMATS.items():\n        for ext in info[0]:\n            existing_extensions[ext] = name\n\n    for extension in extensions:\n        if extension in existing_extensions:\n            msg = '%s is already registered for \"%s\"'\n            raise RegistryError(msg % (extension,\n                                       existing_extensions[extension]))\n\n    if not callable(function):\n        raise TypeError('The registered function must be a callable')\n\n\ndef register_unpack_format(name, extensions, function, extra_args=None,\n                           description=''):\n    \"\"\"Registers an unpack format.\n\n    `name` is the name of the format. `extensions` is a list of extensions\n    corresponding to the format.\n\n    `function` is the callable that will be\n    used to unpack archives. The callable will receive archives to unpack.\n    If it's unable to handle an archive, it needs to raise a ReadError\n    exception.\n\n    If provided, `extra_args` is a sequence of\n    (name, value) tuples that will be passed as arguments to the callable.\n    description can be provided to describe the format, and will be returned\n    by the get_unpack_formats() function.\n    \"\"\"\n    if extra_args is None:\n        extra_args = []\n    _check_unpack_options(extensions, function, extra_args)\n    _UNPACK_FORMATS[name] = extensions, function, extra_args, description\n\ndef unregister_unpack_format(name):\n    \"\"\"Removes the pack format from the registry.\"\"\"\n    del _UNPACK_FORMATS[name]\n\ndef _ensure_directory(path):\n    \"\"\"Ensure that the parent directory of `path` exists\"\"\"\n    dirname = os.path.dirname(path)\n    if not os.path.isdir(dirname):\n        os.makedirs(dirname)\n\ndef _unpack_zipfile(filename, extract_dir):\n    \"\"\"Unpack zip `filename` to `extract_dir`\n    \"\"\"\n    import zipfile  # late import for breaking circular dependency\n\n    if not zipfile.is_zipfile(filename):\n        raise ReadError(\"%s is not a zip file\" % filename)\n\n    zip = zipfile.ZipFile(filename)\n    try:\n        for info in zip.infolist():\n            name = info.filename\n\n            # don't extract absolute paths or ones with .. in them\n            if name.startswith('/') or '..' in name:\n                continue\n\n            targetpath = os.path.join(extract_dir, *name.split('/'))\n            if not targetpath:\n                continue\n\n            _ensure_directory(targetpath)\n            if not name.endswith('/'):\n                # file\n                with zip.open(name, 'r') as source, \\\n                        open(targetpath, 'wb') as target:\n                    copyfileobj(source, target)\n    finally:\n        zip.close()\n\ndef _unpack_tarfile(filename, extract_dir, *, filter=None):\n    \"\"\"Unpack tar/tar.gz/tar.bz2/tar.xz `filename` to `extract_dir`\n    \"\"\"\n    import tarfile  # late import for breaking circular dependency\n    try:\n        tarobj = tarfile.open(filename)\n    except tarfile.TarError:\n        raise ReadError(\n            \"%s is not a compressed or uncompressed tar file\" % filename)\n    try:\n        tarobj.extractall(extract_dir, filter=filter)\n    finally:\n        tarobj.close()\n\n# Maps the name of the unpack format to a tuple containing:\n# * extensions\n# * the unpacking function\n# * extra keyword arguments\n# * description\n_UNPACK_FORMATS = {\n    'tar':   (['.tar'], _unpack_tarfile, [], \"uncompressed tar file\"),\n    'zip':   (['.zip'], _unpack_zipfile, [], \"ZIP file\"),\n}\n\nif _ZLIB_SUPPORTED:\n    _UNPACK_FORMATS['gztar'] = (['.tar.gz', '.tgz'], _unpack_tarfile, [],\n                                \"gzip'ed tar-file\")\n\nif _BZ2_SUPPORTED:\n    _UNPACK_FORMATS['bztar'] = (['.tar.bz2', '.tbz2'], _unpack_tarfile, [],\n                                \"bzip2'ed tar-file\")\n\nif _LZMA_SUPPORTED:\n    _UNPACK_FORMATS['xztar'] = (['.tar.xz', '.txz'], _unpack_tarfile, [],\n                                \"xz'ed tar-file\")\n\ndef _find_unpack_format(filename):\n    for name, info in _UNPACK_FORMATS.items():\n        for extension in info[0]:\n            if filename.endswith(extension):\n                return name\n    return None\n\ndef unpack_archive(filename, extract_dir=None, format=None, *, filter=None):\n    \"\"\"Unpack an archive.\n\n    `filename` is the name of the archive.\n\n    `extract_dir` is the name of the target directory, where the archive\n    is unpacked. If not provided, the current working directory is used.\n\n    `format` is the archive format: one of \"zip\", \"tar\", \"gztar\", \"bztar\",\n    or \"xztar\".  Or any other registered format.  If not provided,\n    unpack_archive will use the filename extension and see if an unpacker\n    was registered for that extension.\n\n    In case none is found, a ValueError is raised.\n\n    If `filter` is given, it is passed to the underlying\n    extraction function.\n    \"\"\"\n    sys.audit(\"shutil.unpack_archive\", filename, extract_dir, format)\n\n    if extract_dir is None:\n        extract_dir = os.getcwd()\n\n    extract_dir = os.fspath(extract_dir)\n    filename = os.fspath(filename)\n\n    if filter is None:\n        filter_kwargs = {}\n    else:\n        filter_kwargs = {'filter': filter}\n    if format is not None:\n        try:\n            format_info = _UNPACK_FORMATS[format]\n        except KeyError:\n            raise ValueError(\"Unknown unpack format '{0}'\".format(format)) from None\n\n        func = format_info[1]\n        func(filename, extract_dir, **dict(format_info[2]), **filter_kwargs)\n    else:\n        # we need to look at the registered unpackers supported extensions\n        format = _find_unpack_format(filename)\n        if format is None:\n            raise ReadError(\"Unknown archive format '{0}'\".format(filename))\n\n        func = _UNPACK_FORMATS[format][1]\n        kwargs = dict(_UNPACK_FORMATS[format][2]) | filter_kwargs\n        func(filename, extract_dir, **kwargs)\n\n\nif hasattr(os, 'statvfs'):\n\n    __all__.append('disk_usage')\n    _ntuple_diskusage = collections.namedtuple('usage', 'total used free')\n    _ntuple_diskusage.total.__doc__ = 'Total space in bytes'\n    _ntuple_diskusage.used.__doc__ = 'Used space in bytes'\n    _ntuple_diskusage.free.__doc__ = 'Free space in bytes'\n\n    def disk_usage(path):\n        \"\"\"Return disk usage statistics about the given path.\n\n        Returned value is a named tuple with attributes 'total', 'used' and\n        'free', which are the amount of total, used and free space, in bytes.\n        \"\"\"\n        st = os.statvfs(path)\n        free = st.f_bavail * st.f_frsize\n        total = st.f_blocks * st.f_frsize\n        used = (st.f_blocks - st.f_bfree) * st.f_frsize\n        return _ntuple_diskusage(total, used, free)\n\nelif _WINDOWS:\n\n    __all__.append('disk_usage')\n    _ntuple_diskusage = collections.namedtuple('usage', 'total used free')\n\n    def disk_usage(path):\n        \"\"\"Return disk usage statistics about the given path.\n\n        Returned values is a named tuple with attributes 'total', 'used' and\n        'free', which are the amount of total, used and free space, in bytes.\n        \"\"\"\n        total, free = nt._getdiskusage(path)\n        used = total - free\n        return _ntuple_diskusage(total, used, free)\n\n\ndef chown(path, user=None, group=None):\n    \"\"\"Change owner user and group of the given path.\n\n    user and group can be the uid/gid or the user/group names, and in that case,\n    they are converted to their respective uid/gid.\n    \"\"\"\n    sys.audit('shutil.chown', path, user, group)\n\n    if user is None and group is None:\n        raise ValueError(\"user and/or group must be set\")\n\n    _user = user\n    _group = group\n\n    # -1 means don't change it\n    if user is None:\n        _user = -1\n    # user can either be an int (the uid) or a string (the system username)\n    elif isinstance(user, str):\n        _user = _get_uid(user)\n        if _user is None:\n            raise LookupError(\"no such user: {!r}\".format(user))\n\n    if group is None:\n        _group = -1\n    elif not isinstance(group, int):\n        _group = _get_gid(group)\n        if _group is None:\n            raise LookupError(\"no such group: {!r}\".format(group))\n\n    os.chown(path, _user, _group)\n\ndef get_terminal_size(fallback=(80, 24)):\n    \"\"\"Get the size of the terminal window.\n\n    For each of the two dimensions, the environment variable, COLUMNS\n    and LINES respectively, is checked. If the variable is defined and\n    the value is a positive integer, it is used.\n\n    When COLUMNS or LINES is not defined, which is the common case,\n    the terminal connected to sys.__stdout__ is queried\n    by invoking os.get_terminal_size.\n\n    If the terminal size cannot be successfully queried, either because\n    the system doesn't support querying, or because we are not\n    connected to a terminal, the value given in fallback parameter\n    is used. Fallback defaults to (80, 24) which is the default\n    size used by many terminal emulators.\n\n    The value returned is a named tuple of type os.terminal_size.\n    \"\"\"\n    # columns, lines are the working values\n    try:\n        columns = int(os.environ['COLUMNS'])\n    except (KeyError, ValueError):\n        columns = 0\n\n    try:\n        lines = int(os.environ['LINES'])\n    except (KeyError, ValueError):\n        lines = 0\n\n    # only query if necessary\n    if columns <= 0 or lines <= 0:\n        try:\n            size = os.get_terminal_size(sys.__stdout__.fileno())\n        except (AttributeError, ValueError, OSError):\n            # stdout is None, closed, detached, or not a terminal, or\n            # os.get_terminal_size() is unsupported\n            size = os.terminal_size(fallback)\n        if columns <= 0:\n            columns = size.columns or fallback[0]\n        if lines <= 0:\n            lines = size.lines or fallback[1]\n\n    return os.terminal_size((columns, lines))\n\n\n# Check that a given file can be accessed with the correct mode.\n# Additionally check that `file` is not a directory, as on Windows\n# directories pass the os.access check.\ndef _access_check(fn, mode):\n    return (os.path.exists(fn) and os.access(fn, mode)\n            and not os.path.isdir(fn))\n\n\ndef _win_path_needs_curdir(cmd, mode):\n    \"\"\"\n    On Windows, we can use NeedCurrentDirectoryForExePath to figure out\n    if we should add the cwd to PATH when searching for executables if\n    the mode is executable.\n    \"\"\"\n    return (not (mode & os.X_OK)) or _winapi.NeedCurrentDirectoryForExePath(\n                os.fsdecode(cmd))\n\n\ndef which(cmd, mode=os.F_OK | os.X_OK, path=None):\n    \"\"\"Given a command, mode, and a PATH string, return the path which\n    conforms to the given mode on the PATH, or None if there is no such\n    file.\n\n    `mode` defaults to os.F_OK | os.X_OK. `path` defaults to the result\n    of os.environ.get(\"PATH\"), or can be overridden with a custom search\n    path.\n\n    \"\"\"\n    use_bytes = isinstance(cmd, bytes)\n\n    # If we're given a path with a directory part, look it up directly rather\n    # than referring to PATH directories. This includes checking relative to\n    # the current directory, e.g. ./script\n    dirname, cmd = os.path.split(cmd)\n    if dirname:\n        path = [dirname]\n    else:\n        if path is None:\n            path = os.environ.get(\"PATH\", None)\n            if path is None:\n                try:\n                    path = os.confstr(\"CS_PATH\")\n                except (AttributeError, ValueError):\n                    # os.confstr() or CS_PATH is not available\n                    path = os.defpath\n            # bpo-35755: Don't use os.defpath if the PATH environment variable\n            # is set to an empty string\n\n        # PATH='' doesn't match, whereas PATH=':' looks in the current\n        # directory\n        if not path:\n            return None\n\n        if use_bytes:\n            path = os.fsencode(path)\n            path = path.split(os.fsencode(os.pathsep))\n        else:\n            path = os.fsdecode(path)\n            path = path.split(os.pathsep)\n\n        if sys.platform == \"win32\" and _win_path_needs_curdir(cmd, mode):\n            curdir = os.curdir\n            if use_bytes:\n                curdir = os.fsencode(curdir)\n            path.insert(0, curdir)\n\n    if sys.platform == \"win32\":\n        # PATHEXT is necessary to check on Windows.\n        pathext_source = os.getenv(\"PATHEXT\") or _WIN_DEFAULT_PATHEXT\n        pathext = pathext_source.split(os.pathsep)\n        pathext = [ext.rstrip('.') for ext in pathext if ext]\n\n        if use_bytes:\n            pathext = [os.fsencode(ext) for ext in pathext]\n\n        files = [cmd + ext for ext in pathext]\n\n        # If X_OK in mode, simulate the cmd.exe behavior: look at direct\n        # match if and only if the extension is in PATHEXT.\n        # If X_OK not in mode, simulate the first result of where.exe:\n        # always look at direct match before a PATHEXT match.\n        normcmd = cmd.upper()\n        if not (mode & os.X_OK) or any(normcmd.endswith(ext.upper()) for ext in pathext):\n            files.insert(0, cmd)\n    else:\n        # On other platforms you don't have things like PATHEXT to tell you\n        # what file suffixes are executable, so just pass on cmd as-is.\n        files = [cmd]\n\n    seen = set()\n    for dir in path:\n        normdir = os.path.normcase(dir)\n        if normdir not in seen:\n            seen.add(normdir)\n            for thefile in files:\n                name = os.path.join(dir, thefile)\n                if _access_check(name, mode):\n                    return name\n    return None\n", 1566], "/root/miniconda3/envs/gs-lightning/lib/python3.12/tempfile.py": ["\"\"\"Temporary files.\n\nThis module provides generic, low- and high-level interfaces for\ncreating temporary files and directories.  All of the interfaces\nprovided by this module can be used without fear of race conditions\nexcept for 'mktemp'.  'mktemp' is subject to race conditions and\nshould not be used; it is provided for backward compatibility only.\n\nThe default path names are returned as str.  If you supply bytes as\ninput, all return values will be in bytes.  Ex:\n\n    >>> tempfile.mkstemp()\n    (4, '/tmp/tmptpu9nin8')\n    >>> tempfile.mkdtemp(suffix=b'')\n    b'/tmp/tmppbi8f0hy'\n\nThis module also provides some data items to the user:\n\n  TMP_MAX  - maximum number of names that will be tried before\n             giving up.\n  tempdir  - If this is set to a string before the first use of\n             any routine from this module, it will be considered as\n             another candidate location to store temporary files.\n\"\"\"\n\n__all__ = [\n    \"NamedTemporaryFile\", \"TemporaryFile\", # high level safe interfaces\n    \"SpooledTemporaryFile\", \"TemporaryDirectory\",\n    \"mkstemp\", \"mkdtemp\",                  # low level safe interfaces\n    \"mktemp\",                              # deprecated unsafe interface\n    \"TMP_MAX\", \"gettempprefix\",            # constants\n    \"tempdir\", \"gettempdir\",\n    \"gettempprefixb\", \"gettempdirb\",\n   ]\n\n\n# Imports.\n\nimport functools as _functools\nimport warnings as _warnings\nimport io as _io\nimport os as _os\nimport shutil as _shutil\nimport errno as _errno\nfrom random import Random as _Random\nimport sys as _sys\nimport types as _types\nimport weakref as _weakref\nimport _thread\n_allocate_lock = _thread.allocate_lock\n\n_text_openflags = _os.O_RDWR | _os.O_CREAT | _os.O_EXCL\nif hasattr(_os, 'O_NOFOLLOW'):\n    _text_openflags |= _os.O_NOFOLLOW\n\n_bin_openflags = _text_openflags\nif hasattr(_os, 'O_BINARY'):\n    _bin_openflags |= _os.O_BINARY\n\nif hasattr(_os, 'TMP_MAX'):\n    TMP_MAX = _os.TMP_MAX\nelse:\n    TMP_MAX = 10000\n\n# This variable _was_ unused for legacy reasons, see issue 10354.\n# But as of 3.5 we actually use it at runtime so changing it would\n# have a possibly desirable side effect...  But we do not want to support\n# that as an API.  It is undocumented on purpose.  Do not depend on this.\ntemplate = \"tmp\"\n\n# Internal routines.\n\n_once_lock = _allocate_lock()\n\n\ndef _exists(fn):\n    try:\n        _os.lstat(fn)\n    except OSError:\n        return False\n    else:\n        return True\n\n\ndef _infer_return_type(*args):\n    \"\"\"Look at the type of all args and divine their implied return type.\"\"\"\n    return_type = None\n    for arg in args:\n        if arg is None:\n            continue\n\n        if isinstance(arg, _os.PathLike):\n            arg = _os.fspath(arg)\n\n        if isinstance(arg, bytes):\n            if return_type is str:\n                raise TypeError(\"Can't mix bytes and non-bytes in \"\n                                \"path components.\")\n            return_type = bytes\n        else:\n            if return_type is bytes:\n                raise TypeError(\"Can't mix bytes and non-bytes in \"\n                                \"path components.\")\n            return_type = str\n    if return_type is None:\n        if tempdir is None or isinstance(tempdir, str):\n            return str  # tempfile APIs return a str by default.\n        else:\n            # we could check for bytes but it'll fail later on anyway\n            return bytes\n    return return_type\n\n\ndef _sanitize_params(prefix, suffix, dir):\n    \"\"\"Common parameter processing for most APIs in this module.\"\"\"\n    output_type = _infer_return_type(prefix, suffix, dir)\n    if suffix is None:\n        suffix = output_type()\n    if prefix is None:\n        if output_type is str:\n            prefix = template\n        else:\n            prefix = _os.fsencode(template)\n    if dir is None:\n        if output_type is str:\n            dir = gettempdir()\n        else:\n            dir = gettempdirb()\n    return prefix, suffix, dir, output_type\n\n\nclass _RandomNameSequence:\n    \"\"\"An instance of _RandomNameSequence generates an endless\n    sequence of unpredictable strings which can safely be incorporated\n    into file names.  Each string is eight characters long.  Multiple\n    threads can safely use the same instance at the same time.\n\n    _RandomNameSequence is an iterator.\"\"\"\n\n    characters = \"abcdefghijklmnopqrstuvwxyz0123456789_\"\n\n    @property\n    def rng(self):\n        cur_pid = _os.getpid()\n        if cur_pid != getattr(self, '_rng_pid', None):\n            self._rng = _Random()\n            self._rng_pid = cur_pid\n        return self._rng\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return ''.join(self.rng.choices(self.characters, k=8))\n\ndef _candidate_tempdir_list():\n    \"\"\"Generate a list of candidate temporary directories which\n    _get_default_tempdir will try.\"\"\"\n\n    dirlist = []\n\n    # First, try the environment.\n    for envname in 'TMPDIR', 'TEMP', 'TMP':\n        dirname = _os.getenv(envname)\n        if dirname: dirlist.append(dirname)\n\n    # Failing that, try OS-specific locations.\n    if _os.name == 'nt':\n        dirlist.extend([ _os.path.expanduser(r'~\\AppData\\Local\\Temp'),\n                         _os.path.expandvars(r'%SYSTEMROOT%\\Temp'),\n                         r'c:\\temp', r'c:\\tmp', r'\\temp', r'\\tmp' ])\n    else:\n        dirlist.extend([ '/tmp', '/var/tmp', '/usr/tmp' ])\n\n    # As a last resort, the current directory.\n    try:\n        dirlist.append(_os.getcwd())\n    except (AttributeError, OSError):\n        dirlist.append(_os.curdir)\n\n    return dirlist\n\ndef _get_default_tempdir():\n    \"\"\"Calculate the default directory to use for temporary files.\n    This routine should be called exactly once.\n\n    We determine whether or not a candidate temp dir is usable by\n    trying to create and write to a file in that directory.  If this\n    is successful, the test file is deleted.  To prevent denial of\n    service, the name of the test file must be randomized.\"\"\"\n\n    namer = _RandomNameSequence()\n    dirlist = _candidate_tempdir_list()\n\n    for dir in dirlist:\n        if dir != _os.curdir:\n            dir = _os.path.abspath(dir)\n        # Try only a few names per directory.\n        for seq in range(100):\n            name = next(namer)\n            filename = _os.path.join(dir, name)\n            try:\n                fd = _os.open(filename, _bin_openflags, 0o600)\n                try:\n                    try:\n                        _os.write(fd, b'blat')\n                    finally:\n                        _os.close(fd)\n                finally:\n                    _os.unlink(filename)\n                return dir\n            except FileExistsError:\n                pass\n            except PermissionError:\n                # This exception is thrown when a directory with the chosen name\n                # already exists on windows.\n                if (_os.name == 'nt' and _os.path.isdir(dir) and\n                    _os.access(dir, _os.W_OK)):\n                    continue\n                break   # no point trying more names in this directory\n            except OSError:\n                break   # no point trying more names in this directory\n    raise FileNotFoundError(_errno.ENOENT,\n                            \"No usable temporary directory found in %s\" %\n                            dirlist)\n\n_name_sequence = None\n\ndef _get_candidate_names():\n    \"\"\"Common setup sequence for all user-callable interfaces.\"\"\"\n\n    global _name_sequence\n    if _name_sequence is None:\n        _once_lock.acquire()\n        try:\n            if _name_sequence is None:\n                _name_sequence = _RandomNameSequence()\n        finally:\n            _once_lock.release()\n    return _name_sequence\n\n\ndef _mkstemp_inner(dir, pre, suf, flags, output_type):\n    \"\"\"Code common to mkstemp, TemporaryFile, and NamedTemporaryFile.\"\"\"\n\n    dir = _os.path.abspath(dir)\n    names = _get_candidate_names()\n    if output_type is bytes:\n        names = map(_os.fsencode, names)\n\n    for seq in range(TMP_MAX):\n        name = next(names)\n        file = _os.path.join(dir, pre + name + suf)\n        _sys.audit(\"tempfile.mkstemp\", file)\n        try:\n            fd = _os.open(file, flags, 0o600)\n        except FileExistsError:\n            continue    # try again\n        except PermissionError:\n            # This exception is thrown when a directory with the chosen name\n            # already exists on windows.\n            if (_os.name == 'nt' and _os.path.isdir(dir) and\n                _os.access(dir, _os.W_OK)):\n                continue\n            else:\n                raise\n        return fd, file\n\n    raise FileExistsError(_errno.EEXIST,\n                          \"No usable temporary file name found\")\n\ndef _dont_follow_symlinks(func, path, *args):\n    # Pass follow_symlinks=False, unless not supported on this platform.\n    if func in _os.supports_follow_symlinks:\n        func(path, *args, follow_symlinks=False)\n    elif _os.name == 'nt' or not _os.path.islink(path):\n        func(path, *args)\n\ndef _resetperms(path):\n    try:\n        chflags = _os.chflags\n    except AttributeError:\n        pass\n    else:\n        _dont_follow_symlinks(chflags, path, 0)\n    _dont_follow_symlinks(_os.chmod, path, 0o700)\n\n\n# User visible interfaces.\n\ndef gettempprefix():\n    \"\"\"The default prefix for temporary directories as string.\"\"\"\n    return _os.fsdecode(template)\n\ndef gettempprefixb():\n    \"\"\"The default prefix for temporary directories as bytes.\"\"\"\n    return _os.fsencode(template)\n\ntempdir = None\n\ndef _gettempdir():\n    \"\"\"Private accessor for tempfile.tempdir.\"\"\"\n    global tempdir\n    if tempdir is None:\n        _once_lock.acquire()\n        try:\n            if tempdir is None:\n                tempdir = _get_default_tempdir()\n        finally:\n            _once_lock.release()\n    return tempdir\n\ndef gettempdir():\n    \"\"\"Returns tempfile.tempdir as str.\"\"\"\n    return _os.fsdecode(_gettempdir())\n\ndef gettempdirb():\n    \"\"\"Returns tempfile.tempdir as bytes.\"\"\"\n    return _os.fsencode(_gettempdir())\n\ndef mkstemp(suffix=None, prefix=None, dir=None, text=False):\n    \"\"\"User-callable function to create and return a unique temporary\n    file.  The return value is a pair (fd, name) where fd is the\n    file descriptor returned by os.open, and name is the filename.\n\n    If 'suffix' is not None, the file name will end with that suffix,\n    otherwise there will be no suffix.\n\n    If 'prefix' is not None, the file name will begin with that prefix,\n    otherwise a default prefix is used.\n\n    If 'dir' is not None, the file will be created in that directory,\n    otherwise a default directory is used.\n\n    If 'text' is specified and true, the file is opened in text\n    mode.  Else (the default) the file is opened in binary mode.\n\n    If any of 'suffix', 'prefix' and 'dir' are not None, they must be the\n    same type.  If they are bytes, the returned name will be bytes; str\n    otherwise.\n\n    The file is readable and writable only by the creating user ID.\n    If the operating system uses permission bits to indicate whether a\n    file is executable, the file is executable by no one. The file\n    descriptor is not inherited by children of this process.\n\n    Caller is responsible for deleting the file when done with it.\n    \"\"\"\n\n    prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)\n\n    if text:\n        flags = _text_openflags\n    else:\n        flags = _bin_openflags\n\n    return _mkstemp_inner(dir, prefix, suffix, flags, output_type)\n\n\ndef mkdtemp(suffix=None, prefix=None, dir=None):\n    \"\"\"User-callable function to create and return a unique temporary\n    directory.  The return value is the pathname of the directory.\n\n    Arguments are as for mkstemp, except that the 'text' argument is\n    not accepted.\n\n    The directory is readable, writable, and searchable only by the\n    creating user.\n\n    Caller is responsible for deleting the directory when done with it.\n    \"\"\"\n\n    prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)\n\n    names = _get_candidate_names()\n    if output_type is bytes:\n        names = map(_os.fsencode, names)\n\n    for seq in range(TMP_MAX):\n        name = next(names)\n        file = _os.path.join(dir, prefix + name + suffix)\n        _sys.audit(\"tempfile.mkdtemp\", file)\n        try:\n            _os.mkdir(file, 0o700)\n        except FileExistsError:\n            continue    # try again\n        except PermissionError:\n            # This exception is thrown when a directory with the chosen name\n            # already exists on windows.\n            if (_os.name == 'nt' and _os.path.isdir(dir) and\n                _os.access(dir, _os.W_OK)):\n                continue\n            else:\n                raise\n        return _os.path.abspath(file)\n\n    raise FileExistsError(_errno.EEXIST,\n                          \"No usable temporary directory name found\")\n\ndef mktemp(suffix=\"\", prefix=template, dir=None):\n    \"\"\"User-callable function to return a unique temporary file name.  The\n    file is not created.\n\n    Arguments are similar to mkstemp, except that the 'text' argument is\n    not accepted, and suffix=None, prefix=None and bytes file names are not\n    supported.\n\n    THIS FUNCTION IS UNSAFE AND SHOULD NOT BE USED.  The file name may\n    refer to a file that did not exist at some point, but by the time\n    you get around to creating it, someone else may have beaten you to\n    the punch.\n    \"\"\"\n\n##    from warnings import warn as _warn\n##    _warn(\"mktemp is a potential security risk to your program\",\n##          RuntimeWarning, stacklevel=2)\n\n    if dir is None:\n        dir = gettempdir()\n\n    names = _get_candidate_names()\n    for seq in range(TMP_MAX):\n        name = next(names)\n        file = _os.path.join(dir, prefix + name + suffix)\n        if not _exists(file):\n            return file\n\n    raise FileExistsError(_errno.EEXIST,\n                          \"No usable temporary filename found\")\n\n\nclass _TemporaryFileCloser:\n    \"\"\"A separate object allowing proper closing of a temporary file's\n    underlying file object, without adding a __del__ method to the\n    temporary file.\"\"\"\n\n    cleanup_called = False\n    close_called = False\n\n    def __init__(self, file, name, delete=True, delete_on_close=True):\n        self.file = file\n        self.name = name\n        self.delete = delete\n        self.delete_on_close = delete_on_close\n\n    def cleanup(self, windows=(_os.name == 'nt'), unlink=_os.unlink):\n        if not self.cleanup_called:\n            self.cleanup_called = True\n            try:\n                if not self.close_called:\n                    self.close_called = True\n                    self.file.close()\n            finally:\n                # Windows provides delete-on-close as a primitive, in which\n                # case the file was deleted by self.file.close().\n                if self.delete and not (windows and self.delete_on_close):\n                    try:\n                        unlink(self.name)\n                    except FileNotFoundError:\n                        pass\n\n    def close(self):\n        if not self.close_called:\n            self.close_called = True\n            try:\n                self.file.close()\n            finally:\n                if self.delete and self.delete_on_close:\n                    self.cleanup()\n\n    def __del__(self):\n        self.cleanup()\n\n\nclass _TemporaryFileWrapper:\n    \"\"\"Temporary file wrapper\n\n    This class provides a wrapper around files opened for\n    temporary use.  In particular, it seeks to automatically\n    remove the file when it is no longer needed.\n    \"\"\"\n\n    def __init__(self, file, name, delete=True, delete_on_close=True):\n        self.file = file\n        self.name = name\n        self._closer = _TemporaryFileCloser(file, name, delete,\n                                            delete_on_close)\n\n    def __getattr__(self, name):\n        # Attribute lookups are delegated to the underlying file\n        # and cached for non-numeric results\n        # (i.e. methods are cached, closed and friends are not)\n        file = self.__dict__['file']\n        a = getattr(file, name)\n        if hasattr(a, '__call__'):\n            func = a\n            @_functools.wraps(func)\n            def func_wrapper(*args, **kwargs):\n                return func(*args, **kwargs)\n            # Avoid closing the file as long as the wrapper is alive,\n            # see issue #18879.\n            func_wrapper._closer = self._closer\n            a = func_wrapper\n        if not isinstance(a, int):\n            setattr(self, name, a)\n        return a\n\n    # The underlying __enter__ method returns the wrong object\n    # (self.file) so override it to return the wrapper\n    def __enter__(self):\n        self.file.__enter__()\n        return self\n\n    # Need to trap __exit__ as well to ensure the file gets\n    # deleted when used in a with statement\n    def __exit__(self, exc, value, tb):\n        result = self.file.__exit__(exc, value, tb)\n        self._closer.cleanup()\n        return result\n\n    def close(self):\n        \"\"\"\n        Close the temporary file, possibly deleting it.\n        \"\"\"\n        self._closer.close()\n\n    # iter() doesn't use __getattr__ to find the __iter__ method\n    def __iter__(self):\n        # Don't return iter(self.file), but yield from it to avoid closing\n        # file as long as it's being used as iterator (see issue #23700).  We\n        # can't use 'yield from' here because iter(file) returns the file\n        # object itself, which has a close method, and thus the file would get\n        # closed when the generator is finalized, due to PEP380 semantics.\n        for line in self.file:\n            yield line\n\ndef NamedTemporaryFile(mode='w+b', buffering=-1, encoding=None,\n                       newline=None, suffix=None, prefix=None,\n                       dir=None, delete=True, *, errors=None,\n                       delete_on_close=True):\n    \"\"\"Create and return a temporary file.\n    Arguments:\n    'prefix', 'suffix', 'dir' -- as for mkstemp.\n    'mode' -- the mode argument to io.open (default \"w+b\").\n    'buffering' -- the buffer size argument to io.open (default -1).\n    'encoding' -- the encoding argument to io.open (default None)\n    'newline' -- the newline argument to io.open (default None)\n    'delete' -- whether the file is automatically deleted (default True).\n    'delete_on_close' -- if 'delete', whether the file is deleted on close\n       (default True) or otherwise either on context manager exit\n       (if context manager was used) or on object finalization. .\n    'errors' -- the errors argument to io.open (default None)\n    The file is created as mkstemp() would do it.\n\n    Returns an object with a file-like interface; the name of the file\n    is accessible as its 'name' attribute.  The file will be automatically\n    deleted when it is closed unless the 'delete' argument is set to False.\n\n    On POSIX, NamedTemporaryFiles cannot be automatically deleted if\n    the creating process is terminated abruptly with a SIGKILL signal.\n    Windows can delete the file even in this case.\n    \"\"\"\n\n    prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)\n\n    flags = _bin_openflags\n\n    # Setting O_TEMPORARY in the flags causes the OS to delete\n    # the file when it is closed.  This is only supported by Windows.\n    if _os.name == 'nt' and delete and delete_on_close:\n        flags |= _os.O_TEMPORARY\n\n    if \"b\" not in mode:\n        encoding = _io.text_encoding(encoding)\n\n    name = None\n    def opener(*args):\n        nonlocal name\n        fd, name = _mkstemp_inner(dir, prefix, suffix, flags, output_type)\n        return fd\n    try:\n        file = _io.open(dir, mode, buffering=buffering,\n                        newline=newline, encoding=encoding, errors=errors,\n                        opener=opener)\n        try:\n            raw = getattr(file, 'buffer', file)\n            raw = getattr(raw, 'raw', raw)\n            raw.name = name\n            return _TemporaryFileWrapper(file, name, delete, delete_on_close)\n        except:\n            file.close()\n            raise\n    except:\n        if name is not None and not (\n            _os.name == 'nt' and delete and delete_on_close):\n            _os.unlink(name)\n        raise\n\nif _os.name != 'posix' or _sys.platform == 'cygwin':\n    # On non-POSIX and Cygwin systems, assume that we cannot unlink a file\n    # while it is open.\n    TemporaryFile = NamedTemporaryFile\n\nelse:\n    # Is the O_TMPFILE flag available and does it work?\n    # The flag is set to False if os.open(dir, os.O_TMPFILE) raises an\n    # IsADirectoryError exception\n    _O_TMPFILE_WORKS = hasattr(_os, 'O_TMPFILE')\n\n    def TemporaryFile(mode='w+b', buffering=-1, encoding=None,\n                      newline=None, suffix=None, prefix=None,\n                      dir=None, *, errors=None):\n        \"\"\"Create and return a temporary file.\n        Arguments:\n        'prefix', 'suffix', 'dir' -- as for mkstemp.\n        'mode' -- the mode argument to io.open (default \"w+b\").\n        'buffering' -- the buffer size argument to io.open (default -1).\n        'encoding' -- the encoding argument to io.open (default None)\n        'newline' -- the newline argument to io.open (default None)\n        'errors' -- the errors argument to io.open (default None)\n        The file is created as mkstemp() would do it.\n\n        Returns an object with a file-like interface.  The file has no\n        name, and will cease to exist when it is closed.\n        \"\"\"\n        global _O_TMPFILE_WORKS\n\n        if \"b\" not in mode:\n            encoding = _io.text_encoding(encoding)\n\n        prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)\n\n        flags = _bin_openflags\n        if _O_TMPFILE_WORKS:\n            fd = None\n            def opener(*args):\n                nonlocal fd\n                flags2 = (flags | _os.O_TMPFILE) & ~_os.O_CREAT\n                fd = _os.open(dir, flags2, 0o600)\n                return fd\n            try:\n                file = _io.open(dir, mode, buffering=buffering,\n                                newline=newline, encoding=encoding,\n                                errors=errors, opener=opener)\n                raw = getattr(file, 'buffer', file)\n                raw = getattr(raw, 'raw', raw)\n                raw.name = fd\n                return file\n            except IsADirectoryError:\n                # Linux kernel older than 3.11 ignores the O_TMPFILE flag:\n                # O_TMPFILE is read as O_DIRECTORY. Trying to open a directory\n                # with O_RDWR|O_DIRECTORY fails with IsADirectoryError, a\n                # directory cannot be open to write. Set flag to False to not\n                # try again.\n                _O_TMPFILE_WORKS = False\n            except OSError:\n                # The filesystem of the directory does not support O_TMPFILE.\n                # For example, OSError(95, 'Operation not supported').\n                #\n                # On Linux kernel older than 3.11, trying to open a regular\n                # file (or a symbolic link to a regular file) with O_TMPFILE\n                # fails with NotADirectoryError, because O_TMPFILE is read as\n                # O_DIRECTORY.\n                pass\n            # Fallback to _mkstemp_inner().\n\n        fd = None\n        def opener(*args):\n            nonlocal fd\n            fd, name = _mkstemp_inner(dir, prefix, suffix, flags, output_type)\n            try:\n                _os.unlink(name)\n            except BaseException as e:\n                _os.close(fd)\n                raise\n            return fd\n        file = _io.open(dir, mode, buffering=buffering,\n                        newline=newline, encoding=encoding, errors=errors,\n                        opener=opener)\n        raw = getattr(file, 'buffer', file)\n        raw = getattr(raw, 'raw', raw)\n        raw.name = fd\n        return file\n\nclass SpooledTemporaryFile(_io.IOBase):\n    \"\"\"Temporary file wrapper, specialized to switch from BytesIO\n    or StringIO to a real file when it exceeds a certain size or\n    when a fileno is needed.\n    \"\"\"\n    _rolled = False\n\n    def __init__(self, max_size=0, mode='w+b', buffering=-1,\n                 encoding=None, newline=None,\n                 suffix=None, prefix=None, dir=None, *, errors=None):\n        if 'b' in mode:\n            self._file = _io.BytesIO()\n        else:\n            encoding = _io.text_encoding(encoding)\n            self._file = _io.TextIOWrapper(_io.BytesIO(),\n                            encoding=encoding, errors=errors,\n                            newline=newline)\n        self._max_size = max_size\n        self._rolled = False\n        self._TemporaryFileArgs = {'mode': mode, 'buffering': buffering,\n                                   'suffix': suffix, 'prefix': prefix,\n                                   'encoding': encoding, 'newline': newline,\n                                   'dir': dir, 'errors': errors}\n\n    __class_getitem__ = classmethod(_types.GenericAlias)\n\n    def _check(self, file):\n        if self._rolled: return\n        max_size = self._max_size\n        if max_size and file.tell() > max_size:\n            self.rollover()\n\n    def rollover(self):\n        if self._rolled: return\n        file = self._file\n        newfile = self._file = TemporaryFile(**self._TemporaryFileArgs)\n        del self._TemporaryFileArgs\n\n        pos = file.tell()\n        if hasattr(newfile, 'buffer'):\n            newfile.buffer.write(file.detach().getvalue())\n        else:\n            newfile.write(file.getvalue())\n        newfile.seek(pos, 0)\n\n        self._rolled = True\n\n    # The method caching trick from NamedTemporaryFile\n    # won't work here, because _file may change from a\n    # BytesIO/StringIO instance to a real file. So we list\n    # all the methods directly.\n\n    # Context management protocol\n    def __enter__(self):\n        if self._file.closed:\n            raise ValueError(\"Cannot enter context with closed file\")\n        return self\n\n    def __exit__(self, exc, value, tb):\n        self._file.close()\n\n    # file protocol\n    def __iter__(self):\n        return self._file.__iter__()\n\n    def __del__(self):\n        if not self.closed:\n            _warnings.warn(\n                \"Unclosed file {!r}\".format(self),\n                ResourceWarning,\n                stacklevel=2,\n                source=self\n            )\n            self.close()\n\n    def close(self):\n        self._file.close()\n\n    @property\n    def closed(self):\n        return self._file.closed\n\n    @property\n    def encoding(self):\n        return self._file.encoding\n\n    @property\n    def errors(self):\n        return self._file.errors\n\n    def fileno(self):\n        self.rollover()\n        return self._file.fileno()\n\n    def flush(self):\n        self._file.flush()\n\n    def isatty(self):\n        return self._file.isatty()\n\n    @property\n    def mode(self):\n        try:\n            return self._file.mode\n        except AttributeError:\n            return self._TemporaryFileArgs['mode']\n\n    @property\n    def name(self):\n        try:\n            return self._file.name\n        except AttributeError:\n            return None\n\n    @property\n    def newlines(self):\n        return self._file.newlines\n\n    def readable(self):\n        return self._file.readable()\n\n    def read(self, *args):\n        return self._file.read(*args)\n\n    def read1(self, *args):\n        return self._file.read1(*args)\n\n    def readinto(self, b):\n        return self._file.readinto(b)\n\n    def readinto1(self, b):\n        return self._file.readinto1(b)\n\n    def readline(self, *args):\n        return self._file.readline(*args)\n\n    def readlines(self, *args):\n        return self._file.readlines(*args)\n\n    def seekable(self):\n        return self._file.seekable()\n\n    def seek(self, *args):\n        return self._file.seek(*args)\n\n    def tell(self):\n        return self._file.tell()\n\n    def truncate(self, size=None):\n        if size is None:\n            return self._file.truncate()\n        else:\n            if size > self._max_size:\n                self.rollover()\n            return self._file.truncate(size)\n\n    def writable(self):\n        return self._file.writable()\n\n    def write(self, s):\n        file = self._file\n        rv = file.write(s)\n        self._check(file)\n        return rv\n\n    def writelines(self, iterable):\n        if self._max_size == 0 or self._rolled:\n            return self._file.writelines(iterable)\n\n        it = iter(iterable)\n        for line in it:\n            self.write(line)\n            if self._rolled:\n                return self._file.writelines(it)\n\n    def detach(self):\n        return self._file.detach()\n\n\nclass TemporaryDirectory:\n    \"\"\"Create and return a temporary directory.  This has the same\n    behavior as mkdtemp but can be used as a context manager.  For\n    example:\n\n        with TemporaryDirectory() as tmpdir:\n            ...\n\n    Upon exiting the context, the directory and everything contained\n    in it are removed (unless delete=False is passed or an exception\n    is raised during cleanup and ignore_cleanup_errors is not True).\n\n    Optional Arguments:\n        suffix - A str suffix for the directory name.  (see mkdtemp)\n        prefix - A str prefix for the directory name.  (see mkdtemp)\n        dir - A directory to create this temp dir in.  (see mkdtemp)\n        ignore_cleanup_errors - False; ignore exceptions during cleanup?\n        delete - True; whether the directory is automatically deleted.\n    \"\"\"\n\n    def __init__(self, suffix=None, prefix=None, dir=None,\n                 ignore_cleanup_errors=False, *, delete=True):\n        self.name = mkdtemp(suffix, prefix, dir)\n        self._ignore_cleanup_errors = ignore_cleanup_errors\n        self._delete = delete\n        self._finalizer = _weakref.finalize(\n            self, self._cleanup, self.name,\n            warn_message=\"Implicitly cleaning up {!r}\".format(self),\n            ignore_errors=self._ignore_cleanup_errors, delete=self._delete)\n\n    @classmethod\n    def _rmtree(cls, name, ignore_errors=False, repeated=False):\n        def onexc(func, path, exc):\n            if isinstance(exc, PermissionError):\n                if repeated and path == name:\n                    if ignore_errors:\n                        return\n                    raise\n\n                try:\n                    if path != name:\n                        _resetperms(_os.path.dirname(path))\n                    _resetperms(path)\n\n                    try:\n                        _os.unlink(path)\n                    except IsADirectoryError:\n                        cls._rmtree(path, ignore_errors=ignore_errors)\n                    except PermissionError:\n                        # The PermissionError handler was originally added for\n                        # FreeBSD in directories, but it seems that it is raised\n                        # on Windows too.\n                        # bpo-43153: Calling _rmtree again may\n                        # raise NotADirectoryError and mask the PermissionError.\n                        # So we must re-raise the current PermissionError if\n                        # path is not a directory.\n                        if not _os.path.isdir(path) or _os.path.isjunction(path):\n                            if ignore_errors:\n                                return\n                            raise\n                        cls._rmtree(path, ignore_errors=ignore_errors,\n                                    repeated=(path == name))\n                except FileNotFoundError:\n                    pass\n            elif isinstance(exc, FileNotFoundError):\n                pass\n            else:\n                if not ignore_errors:\n                    raise\n\n        _shutil.rmtree(name, onexc=onexc)\n\n    @classmethod\n    def _cleanup(cls, name, warn_message, ignore_errors=False, delete=True):\n        if delete:\n            cls._rmtree(name, ignore_errors=ignore_errors)\n            _warnings.warn(warn_message, ResourceWarning)\n\n    def __repr__(self):\n        return \"<{} {!r}>\".format(self.__class__.__name__, self.name)\n\n    def __enter__(self):\n        return self.name\n\n    def __exit__(self, exc, value, tb):\n        if self._delete:\n            self.cleanup()\n\n    def cleanup(self):\n        if self._finalizer.detach() or _os.path.exists(self.name):\n            self._rmtree(self.name, ignore_errors=self._ignore_cleanup_errors)\n\n    __class_getitem__ = classmethod(_types.GenericAlias)\n", 956], "/root/miniconda3/envs/gs-lightning/lib/python3.12/textwrap.py": ["\"\"\"Text wrapping and filling.\n\"\"\"\n\n# Copyright (C) 1999-2001 Gregory P. Ward.\n# Copyright (C) 2002, 2003 Python Software Foundation.\n# Written by Greg Ward <gward@python.net>\n\nimport re\n\n__all__ = ['TextWrapper', 'wrap', 'fill', 'dedent', 'indent', 'shorten']\n\n# Hardcode the recognized whitespace characters to the US-ASCII\n# whitespace characters.  The main reason for doing this is that\n# some Unicode spaces (like \\u00a0) are non-breaking whitespaces.\n_whitespace = '\\t\\n\\x0b\\x0c\\r '\n\nclass TextWrapper:\n    \"\"\"\n    Object for wrapping/filling text.  The public interface consists of\n    the wrap() and fill() methods; the other methods are just there for\n    subclasses to override in order to tweak the default behaviour.\n    If you want to completely replace the main wrapping algorithm,\n    you'll probably have to override _wrap_chunks().\n\n    Several instance attributes control various aspects of wrapping:\n      width (default: 70)\n        the maximum width of wrapped lines (unless break_long_words\n        is false)\n      initial_indent (default: \"\")\n        string that will be prepended to the first line of wrapped\n        output.  Counts towards the line's width.\n      subsequent_indent (default: \"\")\n        string that will be prepended to all lines save the first\n        of wrapped output; also counts towards each line's width.\n      expand_tabs (default: true)\n        Expand tabs in input text to spaces before further processing.\n        Each tab will become 0 .. 'tabsize' spaces, depending on its position\n        in its line.  If false, each tab is treated as a single character.\n      tabsize (default: 8)\n        Expand tabs in input text to 0 .. 'tabsize' spaces, unless\n        'expand_tabs' is false.\n      replace_whitespace (default: true)\n        Replace all whitespace characters in the input text by spaces\n        after tab expansion.  Note that if expand_tabs is false and\n        replace_whitespace is true, every tab will be converted to a\n        single space!\n      fix_sentence_endings (default: false)\n        Ensure that sentence-ending punctuation is always followed\n        by two spaces.  Off by default because the algorithm is\n        (unavoidably) imperfect.\n      break_long_words (default: true)\n        Break words longer than 'width'.  If false, those words will not\n        be broken, and some lines might be longer than 'width'.\n      break_on_hyphens (default: true)\n        Allow breaking hyphenated words. If true, wrapping will occur\n        preferably on whitespaces and right after hyphens part of\n        compound words.\n      drop_whitespace (default: true)\n        Drop leading and trailing whitespace from lines.\n      max_lines (default: None)\n        Truncate wrapped lines.\n      placeholder (default: ' [...]')\n        Append to the last line of truncated text.\n    \"\"\"\n\n    unicode_whitespace_trans = dict.fromkeys(map(ord, _whitespace), ord(' '))\n\n    # This funky little regex is just the trick for splitting\n    # text up into word-wrappable chunks.  E.g.\n    #   \"Hello there -- you goof-ball, use the -b option!\"\n    # splits into\n    #   Hello/ /there/ /--/ /you/ /goof-/ball,/ /use/ /the/ /-b/ /option!\n    # (after stripping out empty strings).\n    word_punct = r'[\\w!\"\\'&.,?]'\n    letter = r'[^\\d\\W]'\n    whitespace = r'[%s]' % re.escape(_whitespace)\n    nowhitespace = '[^' + whitespace[1:]\n    wordsep_re = re.compile(r'''\n        ( # any whitespace\n          %(ws)s+\n        | # em-dash between words\n          (?<=%(wp)s) -{2,} (?=\\w)\n        | # word, possibly hyphenated\n          %(nws)s+? (?:\n            # hyphenated word\n              -(?: (?<=%(lt)s{2}-) | (?<=%(lt)s-%(lt)s-))\n              (?= %(lt)s -? %(lt)s)\n            | # end of word\n              (?=%(ws)s|\\Z)\n            | # em-dash\n              (?<=%(wp)s) (?=-{2,}\\w)\n            )\n        )''' % {'wp': word_punct, 'lt': letter,\n                'ws': whitespace, 'nws': nowhitespace},\n        re.VERBOSE)\n    del word_punct, letter, nowhitespace\n\n    # This less funky little regex just split on recognized spaces. E.g.\n    #   \"Hello there -- you goof-ball, use the -b option!\"\n    # splits into\n    #   Hello/ /there/ /--/ /you/ /goof-ball,/ /use/ /the/ /-b/ /option!/\n    wordsep_simple_re = re.compile(r'(%s+)' % whitespace)\n    del whitespace\n\n    # XXX this is not locale- or charset-aware -- string.lowercase\n    # is US-ASCII only (and therefore English-only)\n    sentence_end_re = re.compile(r'[a-z]'             # lowercase letter\n                                 r'[\\.\\!\\?]'          # sentence-ending punct.\n                                 r'[\\\"\\']?'           # optional end-of-quote\n                                 r'\\Z')               # end of chunk\n\n    def __init__(self,\n                 width=70,\n                 initial_indent=\"\",\n                 subsequent_indent=\"\",\n                 expand_tabs=True,\n                 replace_whitespace=True,\n                 fix_sentence_endings=False,\n                 break_long_words=True,\n                 drop_whitespace=True,\n                 break_on_hyphens=True,\n                 tabsize=8,\n                 *,\n                 max_lines=None,\n                 placeholder=' [...]'):\n        self.width = width\n        self.initial_indent = initial_indent\n        self.subsequent_indent = subsequent_indent\n        self.expand_tabs = expand_tabs\n        self.replace_whitespace = replace_whitespace\n        self.fix_sentence_endings = fix_sentence_endings\n        self.break_long_words = break_long_words\n        self.drop_whitespace = drop_whitespace\n        self.break_on_hyphens = break_on_hyphens\n        self.tabsize = tabsize\n        self.max_lines = max_lines\n        self.placeholder = placeholder\n\n\n    # -- Private methods -----------------------------------------------\n    # (possibly useful for subclasses to override)\n\n    def _munge_whitespace(self, text):\n        \"\"\"_munge_whitespace(text : string) -> string\n\n        Munge whitespace in text: expand tabs and convert all other\n        whitespace characters to spaces.  Eg. \" foo\\\\tbar\\\\n\\\\nbaz\"\n        becomes \" foo    bar  baz\".\n        \"\"\"\n        if self.expand_tabs:\n            text = text.expandtabs(self.tabsize)\n        if self.replace_whitespace:\n            text = text.translate(self.unicode_whitespace_trans)\n        return text\n\n\n    def _split(self, text):\n        \"\"\"_split(text : string) -> [string]\n\n        Split the text to wrap into indivisible chunks.  Chunks are\n        not quite the same as words; see _wrap_chunks() for full\n        details.  As an example, the text\n          Look, goof-ball -- use the -b option!\n        breaks into the following chunks:\n          'Look,', ' ', 'goof-', 'ball', ' ', '--', ' ',\n          'use', ' ', 'the', ' ', '-b', ' ', 'option!'\n        if break_on_hyphens is True, or in:\n          'Look,', ' ', 'goof-ball', ' ', '--', ' ',\n          'use', ' ', 'the', ' ', '-b', ' ', option!'\n        otherwise.\n        \"\"\"\n        if self.break_on_hyphens is True:\n            chunks = self.wordsep_re.split(text)\n        else:\n            chunks = self.wordsep_simple_re.split(text)\n        chunks = [c for c in chunks if c]\n        return chunks\n\n    def _fix_sentence_endings(self, chunks):\n        \"\"\"_fix_sentence_endings(chunks : [string])\n\n        Correct for sentence endings buried in 'chunks'.  Eg. when the\n        original text contains \"... foo.\\\\nBar ...\", munge_whitespace()\n        and split() will convert that to [..., \"foo.\", \" \", \"Bar\", ...]\n        which has one too few spaces; this method simply changes the one\n        space to two.\n        \"\"\"\n        i = 0\n        patsearch = self.sentence_end_re.search\n        while i < len(chunks)-1:\n            if chunks[i+1] == \" \" and patsearch(chunks[i]):\n                chunks[i+1] = \"  \"\n                i += 2\n            else:\n                i += 1\n\n    def _handle_long_word(self, reversed_chunks, cur_line, cur_len, width):\n        \"\"\"_handle_long_word(chunks : [string],\n                             cur_line : [string],\n                             cur_len : int, width : int)\n\n        Handle a chunk of text (most likely a word, not whitespace) that\n        is too long to fit in any line.\n        \"\"\"\n        # Figure out when indent is larger than the specified width, and make\n        # sure at least one character is stripped off on every pass\n        if width < 1:\n            space_left = 1\n        else:\n            space_left = width - cur_len\n\n        # If we're allowed to break long words, then do so: put as much\n        # of the next chunk onto the current line as will fit.\n        if self.break_long_words:\n            end = space_left\n            chunk = reversed_chunks[-1]\n            if self.break_on_hyphens and len(chunk) > space_left:\n                # break after last hyphen, but only if there are\n                # non-hyphens before it\n                hyphen = chunk.rfind('-', 0, space_left)\n                if hyphen > 0 and any(c != '-' for c in chunk[:hyphen]):\n                    end = hyphen + 1\n            cur_line.append(chunk[:end])\n            reversed_chunks[-1] = chunk[end:]\n\n        # Otherwise, we have to preserve the long word intact.  Only add\n        # it to the current line if there's nothing already there --\n        # that minimizes how much we violate the width constraint.\n        elif not cur_line:\n            cur_line.append(reversed_chunks.pop())\n\n        # If we're not allowed to break long words, and there's already\n        # text on the current line, do nothing.  Next time through the\n        # main loop of _wrap_chunks(), we'll wind up here again, but\n        # cur_len will be zero, so the next line will be entirely\n        # devoted to the long word that we can't handle right now.\n\n    def _wrap_chunks(self, chunks):\n        \"\"\"_wrap_chunks(chunks : [string]) -> [string]\n\n        Wrap a sequence of text chunks and return a list of lines of\n        length 'self.width' or less.  (If 'break_long_words' is false,\n        some lines may be longer than this.)  Chunks correspond roughly\n        to words and the whitespace between them: each chunk is\n        indivisible (modulo 'break_long_words'), but a line break can\n        come between any two chunks.  Chunks should not have internal\n        whitespace; ie. a chunk is either all whitespace or a \"word\".\n        Whitespace chunks will be removed from the beginning and end of\n        lines, but apart from that whitespace is preserved.\n        \"\"\"\n        lines = []\n        if self.width <= 0:\n            raise ValueError(\"invalid width %r (must be > 0)\" % self.width)\n        if self.max_lines is not None:\n            if self.max_lines > 1:\n                indent = self.subsequent_indent\n            else:\n                indent = self.initial_indent\n            if len(indent) + len(self.placeholder.lstrip()) > self.width:\n                raise ValueError(\"placeholder too large for max width\")\n\n        # Arrange in reverse order so items can be efficiently popped\n        # from a stack of chucks.\n        chunks.reverse()\n\n        while chunks:\n\n            # Start the list of chunks that will make up the current line.\n            # cur_len is just the length of all the chunks in cur_line.\n            cur_line = []\n            cur_len = 0\n\n            # Figure out which static string will prefix this line.\n            if lines:\n                indent = self.subsequent_indent\n            else:\n                indent = self.initial_indent\n\n            # Maximum width for this line.\n            width = self.width - len(indent)\n\n            # First chunk on line is whitespace -- drop it, unless this\n            # is the very beginning of the text (ie. no lines started yet).\n            if self.drop_whitespace and chunks[-1].strip() == '' and lines:\n                del chunks[-1]\n\n            while chunks:\n                l = len(chunks[-1])\n\n                # Can at least squeeze this chunk onto the current line.\n                if cur_len + l <= width:\n                    cur_line.append(chunks.pop())\n                    cur_len += l\n\n                # Nope, this line is full.\n                else:\n                    break\n\n            # The current line is full, and the next chunk is too big to\n            # fit on *any* line (not just this one).\n            if chunks and len(chunks[-1]) > width:\n                self._handle_long_word(chunks, cur_line, cur_len, width)\n                cur_len = sum(map(len, cur_line))\n\n            # If the last chunk on this line is all whitespace, drop it.\n            if self.drop_whitespace and cur_line and cur_line[-1].strip() == '':\n                cur_len -= len(cur_line[-1])\n                del cur_line[-1]\n\n            if cur_line:\n                if (self.max_lines is None or\n                    len(lines) + 1 < self.max_lines or\n                    (not chunks or\n                     self.drop_whitespace and\n                     len(chunks) == 1 and\n                     not chunks[0].strip()) and cur_len <= width):\n                    # Convert current line back to a string and store it in\n                    # list of all lines (return value).\n                    lines.append(indent + ''.join(cur_line))\n                else:\n                    while cur_line:\n                        if (cur_line[-1].strip() and\n                            cur_len + len(self.placeholder) <= width):\n                            cur_line.append(self.placeholder)\n                            lines.append(indent + ''.join(cur_line))\n                            break\n                        cur_len -= len(cur_line[-1])\n                        del cur_line[-1]\n                    else:\n                        if lines:\n                            prev_line = lines[-1].rstrip()\n                            if (len(prev_line) + len(self.placeholder) <=\n                                    self.width):\n                                lines[-1] = prev_line + self.placeholder\n                                break\n                        lines.append(indent + self.placeholder.lstrip())\n                    break\n\n        return lines\n\n    def _split_chunks(self, text):\n        text = self._munge_whitespace(text)\n        return self._split(text)\n\n    # -- Public interface ----------------------------------------------\n\n    def wrap(self, text):\n        \"\"\"wrap(text : string) -> [string]\n\n        Reformat the single paragraph in 'text' so it fits in lines of\n        no more than 'self.width' columns, and return a list of wrapped\n        lines.  Tabs in 'text' are expanded with string.expandtabs(),\n        and all other whitespace characters (including newline) are\n        converted to space.\n        \"\"\"\n        chunks = self._split_chunks(text)\n        if self.fix_sentence_endings:\n            self._fix_sentence_endings(chunks)\n        return self._wrap_chunks(chunks)\n\n    def fill(self, text):\n        \"\"\"fill(text : string) -> string\n\n        Reformat the single paragraph in 'text' to fit in lines of no\n        more than 'self.width' columns, and return a new string\n        containing the entire wrapped paragraph.\n        \"\"\"\n        return \"\\n\".join(self.wrap(text))\n\n\n# -- Convenience interface ---------------------------------------------\n\ndef wrap(text, width=70, **kwargs):\n    \"\"\"Wrap a single paragraph of text, returning a list of wrapped lines.\n\n    Reformat the single paragraph in 'text' so it fits in lines of no\n    more than 'width' columns, and return a list of wrapped lines.  By\n    default, tabs in 'text' are expanded with string.expandtabs(), and\n    all other whitespace characters (including newline) are converted to\n    space.  See TextWrapper class for available keyword args to customize\n    wrapping behaviour.\n    \"\"\"\n    w = TextWrapper(width=width, **kwargs)\n    return w.wrap(text)\n\ndef fill(text, width=70, **kwargs):\n    \"\"\"Fill a single paragraph of text, returning a new string.\n\n    Reformat the single paragraph in 'text' to fit in lines of no more\n    than 'width' columns, and return a new string containing the entire\n    wrapped paragraph.  As with wrap(), tabs are expanded and other\n    whitespace characters converted to space.  See TextWrapper class for\n    available keyword args to customize wrapping behaviour.\n    \"\"\"\n    w = TextWrapper(width=width, **kwargs)\n    return w.fill(text)\n\ndef shorten(text, width, **kwargs):\n    \"\"\"Collapse and truncate the given text to fit in the given width.\n\n    The text first has its whitespace collapsed.  If it then fits in\n    the *width*, it is returned as is.  Otherwise, as many words\n    as possible are joined and then the placeholder is appended::\n\n        >>> textwrap.shorten(\"Hello  world!\", width=12)\n        'Hello world!'\n        >>> textwrap.shorten(\"Hello  world!\", width=11)\n        'Hello [...]'\n    \"\"\"\n    w = TextWrapper(width=width, max_lines=1, **kwargs)\n    return w.fill(' '.join(text.strip().split()))\n\n\n# -- Loosely related functionality -------------------------------------\n\n_whitespace_only_re = re.compile('^[ \\t]+$', re.MULTILINE)\n_leading_whitespace_re = re.compile('(^[ \\t]*)(?:[^ \\t\\n])', re.MULTILINE)\n\ndef dedent(text):\n    \"\"\"Remove any common leading whitespace from every line in `text`.\n\n    This can be used to make triple-quoted strings line up with the left\n    edge of the display, while still presenting them in the source code\n    in indented form.\n\n    Note that tabs and spaces are both treated as whitespace, but they\n    are not equal: the lines \"  hello\" and \"\\\\thello\" are\n    considered to have no common leading whitespace.\n\n    Entirely blank lines are normalized to a newline character.\n    \"\"\"\n    # Look for the longest leading string of spaces and tabs common to\n    # all lines.\n    margin = None\n    text = _whitespace_only_re.sub('', text)\n    indents = _leading_whitespace_re.findall(text)\n    for indent in indents:\n        if margin is None:\n            margin = indent\n\n        # Current line more deeply indented than previous winner:\n        # no change (previous winner is still on top).\n        elif indent.startswith(margin):\n            pass\n\n        # Current line consistent with and no deeper than previous winner:\n        # it's the new winner.\n        elif margin.startswith(indent):\n            margin = indent\n\n        # Find the largest common whitespace between current line and previous\n        # winner.\n        else:\n            for i, (x, y) in enumerate(zip(margin, indent)):\n                if x != y:\n                    margin = margin[:i]\n                    break\n\n    # sanity check (testing/debugging only)\n    if 0 and margin:\n        for line in text.split(\"\\n\"):\n            assert not line or line.startswith(margin), \\\n                   \"line = %r, margin = %r\" % (line, margin)\n\n    if margin:\n        text = re.sub(r'(?m)^' + margin, '', text)\n    return text\n\n\ndef indent(text, prefix, predicate=None):\n    \"\"\"Adds 'prefix' to the beginning of selected lines in 'text'.\n\n    If 'predicate' is provided, 'prefix' will only be added to the lines\n    where 'predicate(line)' is True. If 'predicate' is not provided,\n    it will default to adding 'prefix' to all non-empty lines that do not\n    consist solely of whitespace characters.\n    \"\"\"\n    if predicate is None:\n        def predicate(line):\n            return line.strip()\n\n    def prefixed_lines():\n        for line in text.splitlines(True):\n            yield (prefix + line if predicate(line) else line)\n    return ''.join(prefixed_lines())\n\n\nif __name__ == \"__main__\":\n    #print dedent(\"\\tfoo\\n\\tbar\")\n    #print dedent(\"  \\thello there\\n  \\t  how are you?\")\n    print(dedent(\"Hello there.\\n  This is indented.\"))\n", 491], "/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py": ["# Copyright 2001-2022 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\"\"\"\nLogging package for Python. Based on PEP 282 and comments thereto in\ncomp.lang.python.\n\nCopyright (C) 2001-2022 Vinay Sajip. All Rights Reserved.\n\nTo use, simply 'import logging' and log away!\n\"\"\"\n\nimport sys, os, time, io, re, traceback, warnings, weakref, collections.abc\n\nfrom types import GenericAlias\nfrom string import Template\nfrom string import Formatter as StrFormatter\n\n\n__all__ = ['BASIC_FORMAT', 'BufferingFormatter', 'CRITICAL', 'DEBUG', 'ERROR',\n           'FATAL', 'FileHandler', 'Filter', 'Formatter', 'Handler', 'INFO',\n           'LogRecord', 'Logger', 'LoggerAdapter', 'NOTSET', 'NullHandler',\n           'StreamHandler', 'WARN', 'WARNING', 'addLevelName', 'basicConfig',\n           'captureWarnings', 'critical', 'debug', 'disable', 'error',\n           'exception', 'fatal', 'getLevelName', 'getLogger', 'getLoggerClass',\n           'info', 'log', 'makeLogRecord', 'setLoggerClass', 'shutdown',\n           'warn', 'warning', 'getLogRecordFactory', 'setLogRecordFactory',\n           'lastResort', 'raiseExceptions', 'getLevelNamesMapping',\n           'getHandlerByName', 'getHandlerNames']\n\nimport threading\n\n__author__  = \"Vinay Sajip <vinay_sajip@red-dove.com>\"\n__status__  = \"production\"\n# The following module attributes are no longer updated.\n__version__ = \"0.5.1.2\"\n__date__    = \"07 February 2010\"\n\n#---------------------------------------------------------------------------\n#   Miscellaneous module data\n#---------------------------------------------------------------------------\n\n#\n#_startTime is used as the base when calculating the relative time of events\n#\n_startTime = time.time()\n\n#\n#raiseExceptions is used to see if exceptions during handling should be\n#propagated\n#\nraiseExceptions = True\n\n#\n# If you don't want threading information in the log, set this to False\n#\nlogThreads = True\n\n#\n# If you don't want multiprocessing information in the log, set this to False\n#\nlogMultiprocessing = True\n\n#\n# If you don't want process information in the log, set this to False\n#\nlogProcesses = True\n\n#\n# If you don't want asyncio task information in the log, set this to False\n#\nlogAsyncioTasks = True\n\n#---------------------------------------------------------------------------\n#   Level related stuff\n#---------------------------------------------------------------------------\n#\n# Default levels and level names, these can be replaced with any positive set\n# of values having corresponding names. There is a pseudo-level, NOTSET, which\n# is only really there as a lower limit for user-defined levels. Handlers and\n# loggers are initialized with NOTSET so that they will log all messages, even\n# at user-defined levels.\n#\n\nCRITICAL = 50\nFATAL = CRITICAL\nERROR = 40\nWARNING = 30\nWARN = WARNING\nINFO = 20\nDEBUG = 10\nNOTSET = 0\n\n_levelToName = {\n    CRITICAL: 'CRITICAL',\n    ERROR: 'ERROR',\n    WARNING: 'WARNING',\n    INFO: 'INFO',\n    DEBUG: 'DEBUG',\n    NOTSET: 'NOTSET',\n}\n_nameToLevel = {\n    'CRITICAL': CRITICAL,\n    'FATAL': FATAL,\n    'ERROR': ERROR,\n    'WARN': WARNING,\n    'WARNING': WARNING,\n    'INFO': INFO,\n    'DEBUG': DEBUG,\n    'NOTSET': NOTSET,\n}\n\ndef getLevelNamesMapping():\n    return _nameToLevel.copy()\n\ndef getLevelName(level):\n    \"\"\"\n    Return the textual or numeric representation of logging level 'level'.\n\n    If the level is one of the predefined levels (CRITICAL, ERROR, WARNING,\n    INFO, DEBUG) then you get the corresponding string. If you have\n    associated levels with names using addLevelName then the name you have\n    associated with 'level' is returned.\n\n    If a numeric value corresponding to one of the defined levels is passed\n    in, the corresponding string representation is returned.\n\n    If a string representation of the level is passed in, the corresponding\n    numeric value is returned.\n\n    If no matching numeric or string value is passed in, the string\n    'Level %s' % level is returned.\n    \"\"\"\n    # See Issues #22386, #27937 and #29220 for why it's this way\n    result = _levelToName.get(level)\n    if result is not None:\n        return result\n    result = _nameToLevel.get(level)\n    if result is not None:\n        return result\n    return \"Level %s\" % level\n\ndef addLevelName(level, levelName):\n    \"\"\"\n    Associate 'levelName' with 'level'.\n\n    This is used when converting levels to text during message formatting.\n    \"\"\"\n    _acquireLock()\n    try:    #unlikely to cause an exception, but you never know...\n        _levelToName[level] = levelName\n        _nameToLevel[levelName] = level\n    finally:\n        _releaseLock()\n\nif hasattr(sys, \"_getframe\"):\n    currentframe = lambda: sys._getframe(1)\nelse: #pragma: no cover\n    def currentframe():\n        \"\"\"Return the frame object for the caller's stack frame.\"\"\"\n        try:\n            raise Exception\n        except Exception as exc:\n            return exc.__traceback__.tb_frame.f_back\n\n#\n# _srcfile is used when walking the stack to check when we've got the first\n# caller stack frame, by skipping frames whose filename is that of this\n# module's source. It therefore should contain the filename of this module's\n# source file.\n#\n# Ordinarily we would use __file__ for this, but frozen modules don't always\n# have __file__ set, for some reason (see Issue #21736). Thus, we get the\n# filename from a handy code object from a function defined in this module.\n# (There's no particular reason for picking addLevelName.)\n#\n\n_srcfile = os.path.normcase(addLevelName.__code__.co_filename)\n\n# _srcfile is only used in conjunction with sys._getframe().\n# Setting _srcfile to None will prevent findCaller() from being called. This\n# way, you can avoid the overhead of fetching caller information.\n\n# The following is based on warnings._is_internal_frame. It makes sure that\n# frames of the import mechanism are skipped when logging at module level and\n# using a stacklevel value greater than one.\ndef _is_internal_frame(frame):\n    \"\"\"Signal whether the frame is a CPython or logging module internal.\"\"\"\n    filename = os.path.normcase(frame.f_code.co_filename)\n    return filename == _srcfile or (\n        \"importlib\" in filename and \"_bootstrap\" in filename\n    )\n\n\ndef _checkLevel(level):\n    if isinstance(level, int):\n        rv = level\n    elif str(level) == level:\n        if level not in _nameToLevel:\n            raise ValueError(\"Unknown level: %r\" % level)\n        rv = _nameToLevel[level]\n    else:\n        raise TypeError(\"Level not an integer or a valid string: %r\"\n                        % (level,))\n    return rv\n\n#---------------------------------------------------------------------------\n#   Thread-related stuff\n#---------------------------------------------------------------------------\n\n#\n#_lock is used to serialize access to shared data structures in this module.\n#This needs to be an RLock because fileConfig() creates and configures\n#Handlers, and so might arbitrary user threads. Since Handler code updates the\n#shared dictionary _handlers, it needs to acquire the lock. But if configuring,\n#the lock would already have been acquired - so we need an RLock.\n#The same argument applies to Loggers and Manager.loggerDict.\n#\n_lock = threading.RLock()\n\ndef _acquireLock():\n    \"\"\"\n    Acquire the module-level lock for serializing access to shared data.\n\n    This should be released with _releaseLock().\n    \"\"\"\n    if _lock:\n        _lock.acquire()\n\ndef _releaseLock():\n    \"\"\"\n    Release the module-level lock acquired by calling _acquireLock().\n    \"\"\"\n    if _lock:\n        _lock.release()\n\n\n# Prevent a held logging lock from blocking a child from logging.\n\nif not hasattr(os, 'register_at_fork'):  # Windows and friends.\n    def _register_at_fork_reinit_lock(instance):\n        pass  # no-op when os.register_at_fork does not exist.\nelse:\n    # A collection of instances with a _at_fork_reinit method (logging.Handler)\n    # to be called in the child after forking.  The weakref avoids us keeping\n    # discarded Handler instances alive.\n    _at_fork_reinit_lock_weakset = weakref.WeakSet()\n\n    def _register_at_fork_reinit_lock(instance):\n        _acquireLock()\n        try:\n            _at_fork_reinit_lock_weakset.add(instance)\n        finally:\n            _releaseLock()\n\n    def _after_at_fork_child_reinit_locks():\n        for handler in _at_fork_reinit_lock_weakset:\n            handler._at_fork_reinit()\n\n        # _acquireLock() was called in the parent before forking.\n        # The lock is reinitialized to unlocked state.\n        _lock._at_fork_reinit()\n\n    os.register_at_fork(before=_acquireLock,\n                        after_in_child=_after_at_fork_child_reinit_locks,\n                        after_in_parent=_releaseLock)\n\n\n#---------------------------------------------------------------------------\n#   The logging record\n#---------------------------------------------------------------------------\n\nclass LogRecord(object):\n    \"\"\"\n    A LogRecord instance represents an event being logged.\n\n    LogRecord instances are created every time something is logged. They\n    contain all the information pertinent to the event being logged. The\n    main information passed in is in msg and args, which are combined\n    using str(msg) % args to create the message field of the record. The\n    record also includes information such as when the record was created,\n    the source line where the logging call was made, and any exception\n    information to be logged.\n    \"\"\"\n    def __init__(self, name, level, pathname, lineno,\n                 msg, args, exc_info, func=None, sinfo=None, **kwargs):\n        \"\"\"\n        Initialize a logging record with interesting information.\n        \"\"\"\n        ct = time.time()\n        self.name = name\n        self.msg = msg\n        #\n        # The following statement allows passing of a dictionary as a sole\n        # argument, so that you can do something like\n        #  logging.debug(\"a %(a)d b %(b)s\", {'a':1, 'b':2})\n        # Suggested by Stefan Behnel.\n        # Note that without the test for args[0], we get a problem because\n        # during formatting, we test to see if the arg is present using\n        # 'if self.args:'. If the event being logged is e.g. 'Value is %d'\n        # and if the passed arg fails 'if self.args:' then no formatting\n        # is done. For example, logger.warning('Value is %d', 0) would log\n        # 'Value is %d' instead of 'Value is 0'.\n        # For the use case of passing a dictionary, this should not be a\n        # problem.\n        # Issue #21172: a request was made to relax the isinstance check\n        # to hasattr(args[0], '__getitem__'). However, the docs on string\n        # formatting still seem to suggest a mapping object is required.\n        # Thus, while not removing the isinstance check, it does now look\n        # for collections.abc.Mapping rather than, as before, dict.\n        if (args and len(args) == 1 and isinstance(args[0], collections.abc.Mapping)\n            and args[0]):\n            args = args[0]\n        self.args = args\n        self.levelname = getLevelName(level)\n        self.levelno = level\n        self.pathname = pathname\n        try:\n            self.filename = os.path.basename(pathname)\n            self.module = os.path.splitext(self.filename)[0]\n        except (TypeError, ValueError, AttributeError):\n            self.filename = pathname\n            self.module = \"Unknown module\"\n        self.exc_info = exc_info\n        self.exc_text = None      # used to cache the traceback text\n        self.stack_info = sinfo\n        self.lineno = lineno\n        self.funcName = func\n        self.created = ct\n        self.msecs = int((ct - int(ct)) * 1000) + 0.0  # see gh-89047\n        self.relativeCreated = (self.created - _startTime) * 1000\n        if logThreads:\n            self.thread = threading.get_ident()\n            self.threadName = threading.current_thread().name\n        else: # pragma: no cover\n            self.thread = None\n            self.threadName = None\n        if not logMultiprocessing: # pragma: no cover\n            self.processName = None\n        else:\n            self.processName = 'MainProcess'\n            mp = sys.modules.get('multiprocessing')\n            if mp is not None:\n                # Errors may occur if multiprocessing has not finished loading\n                # yet - e.g. if a custom import hook causes third-party code\n                # to run when multiprocessing calls import. See issue 8200\n                # for an example\n                try:\n                    self.processName = mp.current_process().name\n                except Exception: #pragma: no cover\n                    pass\n        if logProcesses and hasattr(os, 'getpid'):\n            self.process = os.getpid()\n        else:\n            self.process = None\n\n        self.taskName = None\n        if logAsyncioTasks:\n            asyncio = sys.modules.get('asyncio')\n            if asyncio:\n                try:\n                    self.taskName = asyncio.current_task().get_name()\n                except Exception:\n                    pass\n\n    def __repr__(self):\n        return '<LogRecord: %s, %s, %s, %s, \"%s\">'%(self.name, self.levelno,\n            self.pathname, self.lineno, self.msg)\n\n    def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        msg = str(self.msg)\n        if self.args:\n            msg = msg % self.args\n        return msg\n\n#\n#   Determine which class to use when instantiating log records.\n#\n_logRecordFactory = LogRecord\n\ndef setLogRecordFactory(factory):\n    \"\"\"\n    Set the factory to be used when instantiating a log record.\n\n    :param factory: A callable which will be called to instantiate\n    a log record.\n    \"\"\"\n    global _logRecordFactory\n    _logRecordFactory = factory\n\ndef getLogRecordFactory():\n    \"\"\"\n    Return the factory to be used when instantiating a log record.\n    \"\"\"\n\n    return _logRecordFactory\n\ndef makeLogRecord(dict):\n    \"\"\"\n    Make a LogRecord whose attributes are defined by the specified dictionary,\n    This function is useful for converting a logging event received over\n    a socket connection (which is sent as a dictionary) into a LogRecord\n    instance.\n    \"\"\"\n    rv = _logRecordFactory(None, None, \"\", 0, \"\", (), None, None)\n    rv.__dict__.update(dict)\n    return rv\n\n\n#---------------------------------------------------------------------------\n#   Formatter classes and functions\n#---------------------------------------------------------------------------\n_str_formatter = StrFormatter()\ndel StrFormatter\n\n\nclass PercentStyle(object):\n\n    default_format = '%(message)s'\n    asctime_format = '%(asctime)s'\n    asctime_search = '%(asctime)'\n    validation_pattern = re.compile(r'%\\(\\w+\\)[#0+ -]*(\\*|\\d+)?(\\.(\\*|\\d+))?[diouxefgcrsa%]', re.I)\n\n    def __init__(self, fmt, *, defaults=None):\n        self._fmt = fmt or self.default_format\n        self._defaults = defaults\n\n    def usesTime(self):\n        return self._fmt.find(self.asctime_search) >= 0\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it matches the correct style\"\"\"\n        if not self.validation_pattern.search(self._fmt):\n            raise ValueError(\"Invalid format '%s' for '%s' style\" % (self._fmt, self.default_format[0]))\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._fmt % values\n\n    def format(self, record):\n        try:\n            return self._format(record)\n        except KeyError as e:\n            raise ValueError('Formatting field not found in record: %s' % e)\n\n\nclass StrFormatStyle(PercentStyle):\n    default_format = '{message}'\n    asctime_format = '{asctime}'\n    asctime_search = '{asctime'\n\n    fmt_spec = re.compile(r'^(.?[<>=^])?[+ -]?#?0?(\\d+|{\\w+})?[,_]?(\\.(\\d+|{\\w+}))?[bcdefgnosx%]?$', re.I)\n    field_spec = re.compile(r'^(\\d+|\\w+)(\\.\\w+|\\[[^]]+\\])*$')\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._fmt.format(**values)\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it is the correct string formatting style\"\"\"\n        fields = set()\n        try:\n            for _, fieldname, spec, conversion in _str_formatter.parse(self._fmt):\n                if fieldname:\n                    if not self.field_spec.match(fieldname):\n                        raise ValueError('invalid field name/expression: %r' % fieldname)\n                    fields.add(fieldname)\n                if conversion and conversion not in 'rsa':\n                    raise ValueError('invalid conversion: %r' % conversion)\n                if spec and not self.fmt_spec.match(spec):\n                    raise ValueError('bad specifier: %r' % spec)\n        except ValueError as e:\n            raise ValueError('invalid format: %s' % e)\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n\nclass StringTemplateStyle(PercentStyle):\n    default_format = '${message}'\n    asctime_format = '${asctime}'\n    asctime_search = '${asctime}'\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._tpl = Template(self._fmt)\n\n    def usesTime(self):\n        fmt = self._fmt\n        return fmt.find('$asctime') >= 0 or fmt.find(self.asctime_search) >= 0\n\n    def validate(self):\n        pattern = Template.pattern\n        fields = set()\n        for m in pattern.finditer(self._fmt):\n            d = m.groupdict()\n            if d['named']:\n                fields.add(d['named'])\n            elif d['braced']:\n                fields.add(d['braced'])\n            elif m.group(0) == '$':\n                raise ValueError('invalid format: bare \\'$\\' not allowed')\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._tpl.substitute(**values)\n\n\nBASIC_FORMAT = \"%(levelname)s:%(name)s:%(message)s\"\n\n_STYLES = {\n    '%': (PercentStyle, BASIC_FORMAT),\n    '{': (StrFormatStyle, '{levelname}:{name}:{message}'),\n    '$': (StringTemplateStyle, '${levelname}:${name}:${message}'),\n}\n\nclass Formatter(object):\n    \"\"\"\n    Formatter instances are used to convert a LogRecord to text.\n\n    Formatters need to know how a LogRecord is constructed. They are\n    responsible for converting a LogRecord to (usually) a string which can\n    be interpreted by either a human or an external system. The base Formatter\n    allows a formatting string to be specified. If none is supplied, the\n    style-dependent default value, \"%(message)s\", \"{message}\", or\n    \"${message}\", is used.\n\n    The Formatter can be initialized with a format string which makes use of\n    knowledge of the LogRecord attributes - e.g. the default value mentioned\n    above makes use of the fact that the user's message and arguments are pre-\n    formatted into a LogRecord's message attribute. Currently, the useful\n    attributes in a LogRecord are described by:\n\n    %(name)s            Name of the logger (logging channel)\n    %(levelno)s         Numeric logging level for the message (DEBUG, INFO,\n                        WARNING, ERROR, CRITICAL)\n    %(levelname)s       Text logging level for the message (\"DEBUG\", \"INFO\",\n                        \"WARNING\", \"ERROR\", \"CRITICAL\")\n    %(pathname)s        Full pathname of the source file where the logging\n                        call was issued (if available)\n    %(filename)s        Filename portion of pathname\n    %(module)s          Module (name portion of filename)\n    %(lineno)d          Source line number where the logging call was issued\n                        (if available)\n    %(funcName)s        Function name\n    %(created)f         Time when the LogRecord was created (time.time()\n                        return value)\n    %(asctime)s         Textual time when the LogRecord was created\n    %(msecs)d           Millisecond portion of the creation time\n    %(relativeCreated)d Time in milliseconds when the LogRecord was created,\n                        relative to the time the logging module was loaded\n                        (typically at application startup time)\n    %(thread)d          Thread ID (if available)\n    %(threadName)s      Thread name (if available)\n    %(taskName)s        Task name (if available)\n    %(process)d         Process ID (if available)\n    %(message)s         The result of record.getMessage(), computed just as\n                        the record is emitted\n    \"\"\"\n\n    converter = time.localtime\n\n    def __init__(self, fmt=None, datefmt=None, style='%', validate=True, *,\n                 defaults=None):\n        \"\"\"\n        Initialize the formatter with specified format strings.\n\n        Initialize the formatter either with the specified format string, or a\n        default as described above. Allow for specialized date formatting with\n        the optional datefmt argument. If datefmt is omitted, you get an\n        ISO8601-like (or RFC 3339-like) format.\n\n        Use a style parameter of '%', '{' or '$' to specify that you want to\n        use one of %-formatting, :meth:`str.format` (``{}``) formatting or\n        :class:`string.Template` formatting in your format string.\n\n        .. versionchanged:: 3.2\n           Added the ``style`` parameter.\n        \"\"\"\n        if style not in _STYLES:\n            raise ValueError('Style must be one of: %s' % ','.join(\n                             _STYLES.keys()))\n        self._style = _STYLES[style][0](fmt, defaults=defaults)\n        if validate:\n            self._style.validate()\n\n        self._fmt = self._style._fmt\n        self.datefmt = datefmt\n\n    default_time_format = '%Y-%m-%d %H:%M:%S'\n    default_msec_format = '%s,%03d'\n\n    def formatTime(self, record, datefmt=None):\n        \"\"\"\n        Return the creation time of the specified LogRecord as formatted text.\n\n        This method should be called from format() by a formatter which\n        wants to make use of a formatted time. This method can be overridden\n        in formatters to provide for any specific requirement, but the\n        basic behaviour is as follows: if datefmt (a string) is specified,\n        it is used with time.strftime() to format the creation time of the\n        record. Otherwise, an ISO8601-like (or RFC 3339-like) format is used.\n        The resulting string is returned. This function uses a user-configurable\n        function to convert the creation time to a tuple. By default,\n        time.localtime() is used; to change this for a particular formatter\n        instance, set the 'converter' attribute to a function with the same\n        signature as time.localtime() or time.gmtime(). To change it for all\n        formatters, for example if you want all logging times to be shown in GMT,\n        set the 'converter' attribute in the Formatter class.\n        \"\"\"\n        ct = self.converter(record.created)\n        if datefmt:\n            s = time.strftime(datefmt, ct)\n        else:\n            s = time.strftime(self.default_time_format, ct)\n            if self.default_msec_format:\n                s = self.default_msec_format % (s, record.msecs)\n        return s\n\n    def formatException(self, ei):\n        \"\"\"\n        Format and return the specified exception information as a string.\n\n        This default implementation just uses\n        traceback.print_exception()\n        \"\"\"\n        sio = io.StringIO()\n        tb = ei[2]\n        # See issues #9427, #1553375. Commented out for now.\n        #if getattr(self, 'fullstack', False):\n        #    traceback.print_stack(tb.tb_frame.f_back, file=sio)\n        traceback.print_exception(ei[0], ei[1], tb, None, sio)\n        s = sio.getvalue()\n        sio.close()\n        if s[-1:] == \"\\n\":\n            s = s[:-1]\n        return s\n\n    def usesTime(self):\n        \"\"\"\n        Check if the format uses the creation time of the record.\n        \"\"\"\n        return self._style.usesTime()\n\n    def formatMessage(self, record):\n        return self._style.format(record)\n\n    def formatStack(self, stack_info):\n        \"\"\"\n        This method is provided as an extension point for specialized\n        formatting of stack information.\n\n        The input data is a string as returned from a call to\n        :func:`traceback.print_stack`, but with the last trailing newline\n        removed.\n\n        The base implementation just returns the value passed in.\n        \"\"\"\n        return stack_info\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record as text.\n\n        The record's attribute dictionary is used as the operand to a\n        string formatting operation which yields the returned string.\n        Before formatting the dictionary, a couple of preparatory steps\n        are carried out. The message attribute of the record is computed\n        using LogRecord.getMessage(). If the formatting string uses the\n        time (as determined by a call to usesTime(), formatTime() is\n        called to format the event time. If there is exception information,\n        it is formatted using formatException() and appended to the message.\n        \"\"\"\n        record.message = record.getMessage()\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n        s = self.formatMessage(record)\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + record.exc_text\n        if record.stack_info:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + self.formatStack(record.stack_info)\n        return s\n\n#\n#   The default formatter to use when no other is specified\n#\n_defaultFormatter = Formatter()\n\nclass BufferingFormatter(object):\n    \"\"\"\n    A formatter suitable for formatting a number of records.\n    \"\"\"\n    def __init__(self, linefmt=None):\n        \"\"\"\n        Optionally specify a formatter which will be used to format each\n        individual record.\n        \"\"\"\n        if linefmt:\n            self.linefmt = linefmt\n        else:\n            self.linefmt = _defaultFormatter\n\n    def formatHeader(self, records):\n        \"\"\"\n        Return the header string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def formatFooter(self, records):\n        \"\"\"\n        Return the footer string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def format(self, records):\n        \"\"\"\n        Format the specified records and return the result as a string.\n        \"\"\"\n        rv = \"\"\n        if len(records) > 0:\n            rv = rv + self.formatHeader(records)\n            for record in records:\n                rv = rv + self.linefmt.format(record)\n            rv = rv + self.formatFooter(records)\n        return rv\n\n#---------------------------------------------------------------------------\n#   Filter classes and functions\n#---------------------------------------------------------------------------\n\nclass Filter(object):\n    \"\"\"\n    Filter instances are used to perform arbitrary filtering of LogRecords.\n\n    Loggers and Handlers can optionally use Filter instances to filter\n    records as desired. The base filter class only allows events which are\n    below a certain point in the logger hierarchy. For example, a filter\n    initialized with \"A.B\" will allow events logged by loggers \"A.B\",\n    \"A.B.C\", \"A.B.C.D\", \"A.B.D\" etc. but not \"A.BB\", \"B.A.B\" etc. If\n    initialized with the empty string, all events are passed.\n    \"\"\"\n    def __init__(self, name=''):\n        \"\"\"\n        Initialize a filter.\n\n        Initialize with the name of the logger which, together with its\n        children, will have its events allowed through the filter. If no\n        name is specified, allow every event.\n        \"\"\"\n        self.name = name\n        self.nlen = len(name)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if the specified record is to be logged.\n\n        Returns True if the record should be logged, or False otherwise.\n        If deemed appropriate, the record may be modified in-place.\n        \"\"\"\n        if self.nlen == 0:\n            return True\n        elif self.name == record.name:\n            return True\n        elif record.name.find(self.name, 0, self.nlen) != 0:\n            return False\n        return (record.name[self.nlen] == \".\")\n\nclass Filterer(object):\n    \"\"\"\n    A base class for loggers and handlers which allows them to share\n    common code.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the list of filters to be an empty list.\n        \"\"\"\n        self.filters = []\n\n    def addFilter(self, filter):\n        \"\"\"\n        Add the specified filter to this handler.\n        \"\"\"\n        if not (filter in self.filters):\n            self.filters.append(filter)\n\n    def removeFilter(self, filter):\n        \"\"\"\n        Remove the specified filter from this handler.\n        \"\"\"\n        if filter in self.filters:\n            self.filters.remove(filter)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if a record is loggable by consulting all the filters.\n\n        The default is to allow the record to be logged; any filter can veto\n        this by returning a false value.\n        If a filter attached to a handler returns a log record instance,\n        then that instance is used in place of the original log record in\n        any further processing of the event by that handler.\n        If a filter returns any other true value, the original log record\n        is used in any further processing of the event by that handler.\n\n        If none of the filters return false values, this method returns\n        a log record.\n        If any of the filters return a false value, this method returns\n        a false value.\n\n        .. versionchanged:: 3.2\n\n           Allow filters to be just callables.\n\n        .. versionchanged:: 3.12\n           Allow filters to return a LogRecord instead of\n           modifying it in place.\n        \"\"\"\n        for f in self.filters:\n            if hasattr(f, 'filter'):\n                result = f.filter(record)\n            else:\n                result = f(record) # assume callable - will raise if not\n            if not result:\n                return False\n            if isinstance(result, LogRecord):\n                record = result\n        return record\n\n#---------------------------------------------------------------------------\n#   Handler classes and functions\n#---------------------------------------------------------------------------\n\n_handlers = weakref.WeakValueDictionary()  #map of handler names to handlers\n_handlerList = [] # added to allow handlers to be removed in reverse of order initialized\n\ndef _removeHandlerRef(wr):\n    \"\"\"\n    Remove a handler reference from the internal cleanup list.\n    \"\"\"\n    # This function can be called during module teardown, when globals are\n    # set to None. It can also be called from another thread. So we need to\n    # pre-emptively grab the necessary globals and check if they're None,\n    # to prevent race conditions and failures during interpreter shutdown.\n    acquire, release, handlers = _acquireLock, _releaseLock, _handlerList\n    if acquire and release and handlers:\n        acquire()\n        try:\n            handlers.remove(wr)\n        except ValueError:\n            pass\n        finally:\n            release()\n\ndef _addHandlerRef(handler):\n    \"\"\"\n    Add a handler to the internal cleanup list using a weak reference.\n    \"\"\"\n    _acquireLock()\n    try:\n        _handlerList.append(weakref.ref(handler, _removeHandlerRef))\n    finally:\n        _releaseLock()\n\n\ndef getHandlerByName(name):\n    \"\"\"\n    Get a handler with the specified *name*, or None if there isn't one with\n    that name.\n    \"\"\"\n    return _handlers.get(name)\n\n\ndef getHandlerNames():\n    \"\"\"\n    Return all known handler names as an immutable set.\n    \"\"\"\n    result = set(_handlers.keys())\n    return frozenset(result)\n\n\nclass Handler(Filterer):\n    \"\"\"\n    Handler instances dispatch logging events to specific destinations.\n\n    The base handler class. Acts as a placeholder which defines the Handler\n    interface. Handlers can optionally use Formatter instances to format\n    records as desired. By default, no formatter is specified; in this case,\n    the 'raw' message as determined by record.message is logged.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initializes the instance - basically setting the formatter to None\n        and the filter list to empty.\n        \"\"\"\n        Filterer.__init__(self)\n        self._name = None\n        self.level = _checkLevel(level)\n        self.formatter = None\n        self._closed = False\n        # Add the handler to the global _handlerList (for cleanup on shutdown)\n        _addHandlerRef(self)\n        self.createLock()\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, name):\n        _acquireLock()\n        try:\n            if self._name in _handlers:\n                del _handlers[self._name]\n            self._name = name\n            if name:\n                _handlers[name] = self\n        finally:\n            _releaseLock()\n\n    name = property(get_name, set_name)\n\n    def createLock(self):\n        \"\"\"\n        Acquire a thread lock for serializing access to the underlying I/O.\n        \"\"\"\n        self.lock = threading.RLock()\n        _register_at_fork_reinit_lock(self)\n\n    def _at_fork_reinit(self):\n        self.lock._at_fork_reinit()\n\n    def acquire(self):\n        \"\"\"\n        Acquire the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.acquire()\n\n    def release(self):\n        \"\"\"\n        Release the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.release()\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this handler.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record.\n\n        If a formatter is set, use it. Otherwise, use the default formatter\n        for the module.\n        \"\"\"\n        if self.formatter:\n            fmt = self.formatter\n        else:\n            fmt = _defaultFormatter\n        return fmt.format(record)\n\n    def emit(self, record):\n        \"\"\"\n        Do whatever it takes to actually log the specified logging record.\n\n        This version is intended to be implemented by subclasses and so\n        raises a NotImplementedError.\n        \"\"\"\n        raise NotImplementedError('emit must be implemented '\n                                  'by Handler subclasses')\n\n    def handle(self, record):\n        \"\"\"\n        Conditionally emit the specified logging record.\n\n        Emission depends on filters which may have been added to the handler.\n        Wrap the actual emission of the record with acquisition/release of\n        the I/O thread lock.\n\n        Returns an instance of the log record that was emitted\n        if it passed all filters, otherwise a false value is returned.\n        \"\"\"\n        rv = self.filter(record)\n        if isinstance(rv, LogRecord):\n            record = rv\n        if rv:\n            self.acquire()\n            try:\n                self.emit(record)\n            finally:\n                self.release()\n        return rv\n\n    def setFormatter(self, fmt):\n        \"\"\"\n        Set the formatter for this handler.\n        \"\"\"\n        self.formatter = fmt\n\n    def flush(self):\n        \"\"\"\n        Ensure all logging output has been flushed.\n\n        This version does nothing and is intended to be implemented by\n        subclasses.\n        \"\"\"\n        pass\n\n    def close(self):\n        \"\"\"\n        Tidy up any resources used by the handler.\n\n        This version removes the handler from an internal map of handlers,\n        _handlers, which is used for handler lookup by name. Subclasses\n        should ensure that this gets called from overridden close()\n        methods.\n        \"\"\"\n        #get the module data lock, as we're updating a shared structure.\n        _acquireLock()\n        try:    #unlikely to raise an exception, but you never know...\n            self._closed = True\n            if self._name and self._name in _handlers:\n                del _handlers[self._name]\n        finally:\n            _releaseLock()\n\n    def handleError(self, record):\n        \"\"\"\n        Handle errors which occur during an emit() call.\n\n        This method should be called from handlers when an exception is\n        encountered during an emit() call. If raiseExceptions is false,\n        exceptions get silently ignored. This is what is mostly wanted\n        for a logging system - most users will not care about errors in\n        the logging system, they are more interested in application errors.\n        You could, however, replace this with a custom handler if you wish.\n        The record which was being processed is passed in to this method.\n        \"\"\"\n        if raiseExceptions and sys.stderr:  # see issue 13807\n            t, v, tb = sys.exc_info()\n            try:\n                sys.stderr.write('--- Logging error ---\\n')\n                traceback.print_exception(t, v, tb, None, sys.stderr)\n                sys.stderr.write('Call stack:\\n')\n                # Walk the stack frame up until we're out of logging,\n                # so as to print the calling context.\n                frame = tb.tb_frame\n                while (frame and os.path.dirname(frame.f_code.co_filename) ==\n                       __path__[0]):\n                    frame = frame.f_back\n                if frame:\n                    traceback.print_stack(frame, file=sys.stderr)\n                else:\n                    # couldn't find the right stack frame, for some reason\n                    sys.stderr.write('Logged from file %s, line %s\\n' % (\n                                     record.filename, record.lineno))\n                # Issue 18671: output logging message and arguments\n                try:\n                    sys.stderr.write('Message: %r\\n'\n                                     'Arguments: %s\\n' % (record.msg,\n                                                          record.args))\n                except RecursionError:  # See issue 36272\n                    raise\n                except Exception:\n                    sys.stderr.write('Unable to print the message and arguments'\n                                     ' - possible formatting error.\\nUse the'\n                                     ' traceback above to help find the error.\\n'\n                                    )\n            except OSError: #pragma: no cover\n                pass    # see issue 5971\n            finally:\n                del t, v, tb\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s (%s)>' % (self.__class__.__name__, level)\n\nclass StreamHandler(Handler):\n    \"\"\"\n    A handler class which writes logging records, appropriately formatted,\n    to a stream. Note that this class does not close the stream, as\n    sys.stdout or sys.stderr may be used.\n    \"\"\"\n\n    terminator = '\\n'\n\n    def __init__(self, stream=None):\n        \"\"\"\n        Initialize the handler.\n\n        If stream is not specified, sys.stderr is used.\n        \"\"\"\n        Handler.__init__(self)\n        if stream is None:\n            stream = sys.stderr\n        self.stream = stream\n\n    def flush(self):\n        \"\"\"\n        Flushes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream and hasattr(self.stream, \"flush\"):\n                self.stream.flush()\n        finally:\n            self.release()\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If a formatter is specified, it is used to format the record.\n        The record is then written to the stream with a trailing newline.  If\n        exception information is present, it is formatted using\n        traceback.print_exception and appended to the stream.  If the stream\n        has an 'encoding' attribute, it is used to determine how to do the\n        output to the stream.\n        \"\"\"\n        try:\n            msg = self.format(record)\n            stream = self.stream\n            # issue 35046: merged two stream.writes into one.\n            stream.write(msg + self.terminator)\n            self.flush()\n        except RecursionError:  # See issue 36272\n            raise\n        except Exception:\n            self.handleError(record)\n\n    def setStream(self, stream):\n        \"\"\"\n        Sets the StreamHandler's stream to the specified value,\n        if it is different.\n\n        Returns the old stream, if the stream was changed, or None\n        if it wasn't.\n        \"\"\"\n        if stream is self.stream:\n            result = None\n        else:\n            result = self.stream\n            self.acquire()\n            try:\n                self.flush()\n                self.stream = stream\n            finally:\n                self.release()\n        return result\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        name = getattr(self.stream, 'name', '')\n        #  bpo-36015: name can be an int\n        name = str(name)\n        if name:\n            name += ' '\n        return '<%s %s(%s)>' % (self.__class__.__name__, name, level)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass FileHandler(StreamHandler):\n    \"\"\"\n    A handler class which writes formatted logging records to disk files.\n    \"\"\"\n    def __init__(self, filename, mode='a', encoding=None, delay=False, errors=None):\n        \"\"\"\n        Open the specified file and use it as the stream for logging.\n        \"\"\"\n        # Issue #27493: add support for Path objects to be passed in\n        filename = os.fspath(filename)\n        #keep the absolute path, otherwise derived classes which use this\n        #may come a cropper when the current directory changes\n        self.baseFilename = os.path.abspath(filename)\n        self.mode = mode\n        self.encoding = encoding\n        if \"b\" not in mode:\n            self.encoding = io.text_encoding(encoding)\n        self.errors = errors\n        self.delay = delay\n        # bpo-26789: FileHandler keeps a reference to the builtin open()\n        # function to be able to open or reopen the file during Python\n        # finalization.\n        self._builtin_open = open\n        if delay:\n            #We don't open the stream, but we still need to call the\n            #Handler constructor to set level, formatter, lock etc.\n            Handler.__init__(self)\n            self.stream = None\n        else:\n            StreamHandler.__init__(self, self._open())\n\n    def close(self):\n        \"\"\"\n        Closes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            try:\n                if self.stream:\n                    try:\n                        self.flush()\n                    finally:\n                        stream = self.stream\n                        self.stream = None\n                        if hasattr(stream, \"close\"):\n                            stream.close()\n            finally:\n                # Issue #19523: call unconditionally to\n                # prevent a handler leak when delay is set\n                # Also see Issue #42378: we also rely on\n                # self._closed being set to True there\n                StreamHandler.close(self)\n        finally:\n            self.release()\n\n    def _open(self):\n        \"\"\"\n        Open the current base file with the (original) mode and encoding.\n        Return the resulting stream.\n        \"\"\"\n        open_func = self._builtin_open\n        return open_func(self.baseFilename, self.mode,\n                         encoding=self.encoding, errors=self.errors)\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If the stream was not opened because 'delay' was specified in the\n        constructor, open it before calling the superclass's emit.\n\n        If stream is not open, current mode is 'w' and `_closed=True`, record\n        will not be emitted (see Issue #42378).\n        \"\"\"\n        if self.stream is None:\n            if self.mode != 'w' or not self._closed:\n                self.stream = self._open()\n        if self.stream:\n            StreamHandler.emit(self, record)\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.baseFilename, level)\n\n\nclass _StderrHandler(StreamHandler):\n    \"\"\"\n    This class is like a StreamHandler using sys.stderr, but always uses\n    whatever sys.stderr is currently set to rather than the value of\n    sys.stderr at handler construction time.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initialize the handler.\n        \"\"\"\n        Handler.__init__(self, level)\n\n    @property\n    def stream(self):\n        return sys.stderr\n\n\n_defaultLastResort = _StderrHandler(WARNING)\nlastResort = _defaultLastResort\n\n#---------------------------------------------------------------------------\n#   Manager classes and functions\n#---------------------------------------------------------------------------\n\nclass PlaceHolder(object):\n    \"\"\"\n    PlaceHolder instances are used in the Manager logger hierarchy to take\n    the place of nodes for which no loggers have been defined. This class is\n    intended for internal use only and not as part of the public API.\n    \"\"\"\n    def __init__(self, alogger):\n        \"\"\"\n        Initialize with the specified logger being a child of this placeholder.\n        \"\"\"\n        self.loggerMap = { alogger : None }\n\n    def append(self, alogger):\n        \"\"\"\n        Add the specified logger as a child of this placeholder.\n        \"\"\"\n        if alogger not in self.loggerMap:\n            self.loggerMap[alogger] = None\n\n#\n#   Determine which class to use when instantiating loggers.\n#\n\ndef setLoggerClass(klass):\n    \"\"\"\n    Set the class to be used when instantiating a logger. The class should\n    define __init__() such that only a name argument is required, and the\n    __init__() should call Logger.__init__()\n    \"\"\"\n    if klass != Logger:\n        if not issubclass(klass, Logger):\n            raise TypeError(\"logger not derived from logging.Logger: \"\n                            + klass.__name__)\n    global _loggerClass\n    _loggerClass = klass\n\ndef getLoggerClass():\n    \"\"\"\n    Return the class to be used when instantiating a logger.\n    \"\"\"\n    return _loggerClass\n\nclass Manager(object):\n    \"\"\"\n    There is [under normal circumstances] just one Manager instance, which\n    holds the hierarchy of loggers.\n    \"\"\"\n    def __init__(self, rootnode):\n        \"\"\"\n        Initialize the manager with the root node of the logger hierarchy.\n        \"\"\"\n        self.root = rootnode\n        self.disable = 0\n        self.emittedNoHandlerWarning = False\n        self.loggerDict = {}\n        self.loggerClass = None\n        self.logRecordFactory = None\n\n    @property\n    def disable(self):\n        return self._disable\n\n    @disable.setter\n    def disable(self, value):\n        self._disable = _checkLevel(value)\n\n    def getLogger(self, name):\n        \"\"\"\n        Get a logger with the specified name (channel name), creating it\n        if it doesn't yet exist. This name is a dot-separated hierarchical\n        name, such as \"a\", \"a.b\", \"a.b.c\" or similar.\n\n        If a PlaceHolder existed for the specified name [i.e. the logger\n        didn't exist but a child of it did], replace it with the created\n        logger and fix up the parent/child references which pointed to the\n        placeholder to now point to the logger.\n        \"\"\"\n        rv = None\n        if not isinstance(name, str):\n            raise TypeError('A logger name must be a string')\n        _acquireLock()\n        try:\n            if name in self.loggerDict:\n                rv = self.loggerDict[name]\n                if isinstance(rv, PlaceHolder):\n                    ph = rv\n                    rv = (self.loggerClass or _loggerClass)(name)\n                    rv.manager = self\n                    self.loggerDict[name] = rv\n                    self._fixupChildren(ph, rv)\n                    self._fixupParents(rv)\n            else:\n                rv = (self.loggerClass or _loggerClass)(name)\n                rv.manager = self\n                self.loggerDict[name] = rv\n                self._fixupParents(rv)\n        finally:\n            _releaseLock()\n        return rv\n\n    def setLoggerClass(self, klass):\n        \"\"\"\n        Set the class to be used when instantiating a logger with this Manager.\n        \"\"\"\n        if klass != Logger:\n            if not issubclass(klass, Logger):\n                raise TypeError(\"logger not derived from logging.Logger: \"\n                                + klass.__name__)\n        self.loggerClass = klass\n\n    def setLogRecordFactory(self, factory):\n        \"\"\"\n        Set the factory to be used when instantiating a log record with this\n        Manager.\n        \"\"\"\n        self.logRecordFactory = factory\n\n    def _fixupParents(self, alogger):\n        \"\"\"\n        Ensure that there are either loggers or placeholders all the way\n        from the specified logger to the root of the logger hierarchy.\n        \"\"\"\n        name = alogger.name\n        i = name.rfind(\".\")\n        rv = None\n        while (i > 0) and not rv:\n            substr = name[:i]\n            if substr not in self.loggerDict:\n                self.loggerDict[substr] = PlaceHolder(alogger)\n            else:\n                obj = self.loggerDict[substr]\n                if isinstance(obj, Logger):\n                    rv = obj\n                else:\n                    assert isinstance(obj, PlaceHolder)\n                    obj.append(alogger)\n            i = name.rfind(\".\", 0, i - 1)\n        if not rv:\n            rv = self.root\n        alogger.parent = rv\n\n    def _fixupChildren(self, ph, alogger):\n        \"\"\"\n        Ensure that children of the placeholder ph are connected to the\n        specified logger.\n        \"\"\"\n        name = alogger.name\n        namelen = len(name)\n        for c in ph.loggerMap.keys():\n            #The if means ... if not c.parent.name.startswith(nm)\n            if c.parent.name[:namelen] != name:\n                alogger.parent = c.parent\n                c.parent = alogger\n\n    def _clear_cache(self):\n        \"\"\"\n        Clear the cache for all loggers in loggerDict\n        Called when level changes are made\n        \"\"\"\n\n        _acquireLock()\n        for logger in self.loggerDict.values():\n            if isinstance(logger, Logger):\n                logger._cache.clear()\n        self.root._cache.clear()\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n#   Logger classes and functions\n#---------------------------------------------------------------------------\n\nclass Logger(Filterer):\n    \"\"\"\n    Instances of the Logger class represent a single logging channel. A\n    \"logging channel\" indicates an area of an application. Exactly how an\n    \"area\" is defined is up to the application developer. Since an\n    application can have any number of areas, logging channels are identified\n    by a unique string. Application areas can be nested (e.g. an area\n    of \"input processing\" might include sub-areas \"read CSV files\", \"read\n    XLS files\" and \"read Gnumeric files\"). To cater for this natural nesting,\n    channel names are organized into a namespace hierarchy where levels are\n    separated by periods, much like the Java or Python package namespace. So\n    in the instance given above, channel names might be \"input\" for the upper\n    level, and \"input.csv\", \"input.xls\" and \"input.gnu\" for the sub-levels.\n    There is no arbitrary limit to the depth of nesting.\n    \"\"\"\n    def __init__(self, name, level=NOTSET):\n        \"\"\"\n        Initialize the logger with a name and an optional level.\n        \"\"\"\n        Filterer.__init__(self)\n        self.name = name\n        self.level = _checkLevel(level)\n        self.parent = None\n        self.propagate = True\n        self.handlers = []\n        self.disabled = False\n        self._cache = {}\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this logger.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n        self.manager._clear_cache()\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'DEBUG'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.debug(\"Houston, we have a %s\", \"thorny problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(DEBUG):\n            self._log(DEBUG, msg, args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'INFO'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.info(\"Houston, we have a %s\", \"notable problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(INFO):\n            self._log(INFO, msg, args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'WARNING'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.warning(\"Houston, we have a %s\", \"bit of a problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(WARNING):\n            self._log(WARNING, msg, args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'ERROR'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.error(\"Houston, we have a %s\", \"major problem\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(ERROR):\n            self._log(ERROR, msg, args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Convenience method for logging an ERROR with exception information.\n        \"\"\"\n        self.error(msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'CRITICAL'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.critical(\"Houston, we have a %s\", \"major disaster\", exc_info=True)\n        \"\"\"\n        if self.isEnabledFor(CRITICAL):\n            self._log(CRITICAL, msg, args, **kwargs)\n\n    def fatal(self, msg, *args, **kwargs):\n        \"\"\"\n        Don't use this method, use critical() instead.\n        \"\"\"\n        self.critical(msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with the integer severity 'level'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.log(level, \"We have a %s\", \"mysterious problem\", exc_info=True)\n        \"\"\"\n        if not isinstance(level, int):\n            if raiseExceptions:\n                raise TypeError(\"level must be an integer\")\n            else:\n                return\n        if self.isEnabledFor(level):\n            self._log(level, msg, args, **kwargs)\n\n    def findCaller(self, stack_info=False, stacklevel=1):\n        \"\"\"\n        Find the stack frame of the caller so that we can note the source\n        file name, line number and function name.\n        \"\"\"\n        f = currentframe()\n        #On some versions of IronPython, currentframe() returns None if\n        #IronPython isn't run with -X:Frames.\n        if f is None:\n            return \"(unknown file)\", 0, \"(unknown function)\", None\n        while stacklevel > 0:\n            next_f = f.f_back\n            if next_f is None:\n                ## We've got options here.\n                ## If we want to use the last (deepest) frame:\n                break\n                ## If we want to mimic the warnings module:\n                #return (\"sys\", 1, \"(unknown function)\", None)\n                ## If we want to be pedantic:\n                #raise ValueError(\"call stack is not deep enough\")\n            f = next_f\n            if not _is_internal_frame(f):\n                stacklevel -= 1\n        co = f.f_code\n        sinfo = None\n        if stack_info:\n            with io.StringIO() as sio:\n                sio.write(\"Stack (most recent call last):\\n\")\n                traceback.print_stack(f, file=sio)\n                sinfo = sio.getvalue()\n                if sinfo[-1] == '\\n':\n                    sinfo = sinfo[:-1]\n        return co.co_filename, f.f_lineno, co.co_name, sinfo\n\n    def makeRecord(self, name, level, fn, lno, msg, args, exc_info,\n                   func=None, extra=None, sinfo=None):\n        \"\"\"\n        A factory method which can be overridden in subclasses to create\n        specialized LogRecords.\n        \"\"\"\n        rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,\n                             sinfo)\n        if extra is not None:\n            for key in extra:\n                if (key in [\"message\", \"asctime\"]) or (key in rv.__dict__):\n                    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\n                rv.__dict__[key] = extra[key]\n        return rv\n\n    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False,\n             stacklevel=1):\n        \"\"\"\n        Low-level logging routine which creates a LogRecord and then calls\n        all the handlers of this logger to handle the record.\n        \"\"\"\n        sinfo = None\n        if _srcfile:\n            #IronPython doesn't track Python frames, so findCaller raises an\n            #exception on some versions of IronPython. We trap it here so that\n            #IronPython can use logging.\n            try:\n                fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\n            except ValueError: # pragma: no cover\n                fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        else: # pragma: no cover\n            fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        if exc_info:\n            if isinstance(exc_info, BaseException):\n                exc_info = (type(exc_info), exc_info, exc_info.__traceback__)\n            elif not isinstance(exc_info, tuple):\n                exc_info = sys.exc_info()\n        record = self.makeRecord(self.name, level, fn, lno, msg, args,\n                                 exc_info, func, extra, sinfo)\n        self.handle(record)\n\n    def handle(self, record):\n        \"\"\"\n        Call the handlers for the specified record.\n\n        This method is used for unpickled records received from a socket, as\n        well as those created locally. Logger-level filtering is applied.\n        \"\"\"\n        if self.disabled:\n            return\n        maybe_record = self.filter(record)\n        if not maybe_record:\n            return\n        if isinstance(maybe_record, LogRecord):\n            record = maybe_record\n        self.callHandlers(record)\n\n    def addHandler(self, hdlr):\n        \"\"\"\n        Add the specified handler to this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if not (hdlr in self.handlers):\n                self.handlers.append(hdlr)\n        finally:\n            _releaseLock()\n\n    def removeHandler(self, hdlr):\n        \"\"\"\n        Remove the specified handler from this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if hdlr in self.handlers:\n                self.handlers.remove(hdlr)\n        finally:\n            _releaseLock()\n\n    def hasHandlers(self):\n        \"\"\"\n        See if this logger has any handlers configured.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. Return True if a handler was found, else False.\n        Stop searching up the hierarchy whenever a logger with the \"propagate\"\n        attribute set to zero is found - that will be the last logger which\n        is checked for the existence of handlers.\n        \"\"\"\n        c = self\n        rv = False\n        while c:\n            if c.handlers:\n                rv = True\n                break\n            if not c.propagate:\n                break\n            else:\n                c = c.parent\n        return rv\n\n    def callHandlers(self, record):\n        \"\"\"\n        Pass a record to all relevant handlers.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. If no handler was found, output a one-off error\n        message to sys.stderr. Stop searching up the hierarchy whenever a\n        logger with the \"propagate\" attribute set to zero is found - that\n        will be the last logger whose handlers are called.\n        \"\"\"\n        c = self\n        found = 0\n        while c:\n            for hdlr in c.handlers:\n                found = found + 1\n                if record.levelno >= hdlr.level:\n                    hdlr.handle(record)\n            if not c.propagate:\n                c = None    #break out\n            else:\n                c = c.parent\n        if (found == 0):\n            if lastResort:\n                if record.levelno >= lastResort.level:\n                    lastResort.handle(record)\n            elif raiseExceptions and not self.manager.emittedNoHandlerWarning:\n                sys.stderr.write(\"No handlers could be found for logger\"\n                                 \" \\\"%s\\\"\\n\" % self.name)\n                self.manager.emittedNoHandlerWarning = True\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for this logger.\n\n        Loop through this logger and its parents in the logger hierarchy,\n        looking for a non-zero logging level. Return the first one found.\n        \"\"\"\n        logger = self\n        while logger:\n            if logger.level:\n                return logger.level\n            logger = logger.parent\n        return NOTSET\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        if self.disabled:\n            return False\n\n        try:\n            return self._cache[level]\n        except KeyError:\n            _acquireLock()\n            try:\n                if self.manager.disable >= level:\n                    is_enabled = self._cache[level] = False\n                else:\n                    is_enabled = self._cache[level] = (\n                        level >= self.getEffectiveLevel()\n                    )\n            finally:\n                _releaseLock()\n            return is_enabled\n\n    def getChild(self, suffix):\n        \"\"\"\n        Get a logger which is a descendant to this one.\n\n        This is a convenience method, such that\n\n        logging.getLogger('abc').getChild('def.ghi')\n\n        is the same as\n\n        logging.getLogger('abc.def.ghi')\n\n        It's useful, for example, when the parent logger is named using\n        __name__ rather than a literal string.\n        \"\"\"\n        if self.root is not self:\n            suffix = '.'.join((self.name, suffix))\n        return self.manager.getLogger(suffix)\n\n    def getChildren(self):\n\n        def _hierlevel(logger):\n            if logger is logger.manager.root:\n                return 0\n            return 1 + logger.name.count('.')\n\n        d = self.manager.loggerDict\n        _acquireLock()\n        try:\n            # exclude PlaceHolders - the last check is to ensure that lower-level\n            # descendants aren't returned - if there are placeholders, a logger's\n            # parent field might point to a grandparent or ancestor thereof.\n            return set(item for item in d.values()\n                       if isinstance(item, Logger) and item.parent is self and\n                       _hierlevel(item) == 1 + _hierlevel(item.parent))\n        finally:\n            _releaseLock()\n\n    def __repr__(self):\n        level = getLevelName(self.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.name, level)\n\n    def __reduce__(self):\n        if getLogger(self.name) is not self:\n            import pickle\n            raise pickle.PicklingError('logger cannot be pickled')\n        return getLogger, (self.name,)\n\n\nclass RootLogger(Logger):\n    \"\"\"\n    A root logger is not that different to any other logger, except that\n    it must have a logging level and there is only one instance of it in\n    the hierarchy.\n    \"\"\"\n    def __init__(self, level):\n        \"\"\"\n        Initialize the logger with the name \"root\".\n        \"\"\"\n        Logger.__init__(self, \"root\", level)\n\n    def __reduce__(self):\n        return getLogger, ()\n\n_loggerClass = Logger\n\nclass LoggerAdapter(object):\n    \"\"\"\n    An adapter for loggers which makes it easier to specify contextual\n    information in logging output.\n    \"\"\"\n\n    def __init__(self, logger, extra=None):\n        \"\"\"\n        Initialize the adapter with a logger and a dict-like object which\n        provides contextual information. This constructor signature allows\n        easy stacking of LoggerAdapters, if so desired.\n\n        You can effectively pass keyword arguments as shown in the\n        following example:\n\n        adapter = LoggerAdapter(someLogger, dict(p1=v1, p2=\"v2\"))\n        \"\"\"\n        self.logger = logger\n        self.extra = extra\n\n    def process(self, msg, kwargs):\n        \"\"\"\n        Process the logging message and keyword arguments passed in to\n        a logging call to insert contextual information. You can either\n        manipulate the message itself, the keyword args or both. Return\n        the message and kwargs modified (or not) to suit your needs.\n\n        Normally, you'll only need to override this one method in a\n        LoggerAdapter subclass for your specific needs.\n        \"\"\"\n        kwargs[\"extra\"] = self.extra\n        return msg, kwargs\n\n    #\n    # Boilerplate convenience methods\n    #\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a debug call to the underlying logger.\n        \"\"\"\n        self.log(DEBUG, msg, *args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an info call to the underlying logger.\n        \"\"\"\n        self.log(INFO, msg, *args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a warning call to the underlying logger.\n        \"\"\"\n        self.log(WARNING, msg, *args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an error call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Delegate an exception call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a critical call to the underlying logger.\n        \"\"\"\n        self.log(CRITICAL, msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a log call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        if self.isEnabledFor(level):\n            msg, kwargs = self.process(msg, kwargs)\n            self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        return self.logger.isEnabledFor(level)\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the specified level on the underlying logger.\n        \"\"\"\n        self.logger.setLevel(level)\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for the underlying logger.\n        \"\"\"\n        return self.logger.getEffectiveLevel()\n\n    def hasHandlers(self):\n        \"\"\"\n        See if the underlying logger has any handlers.\n        \"\"\"\n        return self.logger.hasHandlers()\n\n    def _log(self, level, msg, args, **kwargs):\n        \"\"\"\n        Low-level log implementation, proxied to allow nested logger adapters.\n        \"\"\"\n        return self.logger._log(level, msg, args, **kwargs)\n\n    @property\n    def manager(self):\n        return self.logger.manager\n\n    @manager.setter\n    def manager(self, value):\n        self.logger.manager = value\n\n    @property\n    def name(self):\n        return self.logger.name\n\n    def __repr__(self):\n        logger = self.logger\n        level = getLevelName(logger.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, logger.name, level)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\nroot = RootLogger(WARNING)\nLogger.root = root\nLogger.manager = Manager(Logger.root)\n\n#---------------------------------------------------------------------------\n# Configuration classes and functions\n#---------------------------------------------------------------------------\n\ndef basicConfig(**kwargs):\n    \"\"\"\n    Do basic configuration for the logging system.\n\n    This function does nothing if the root logger already has handlers\n    configured, unless the keyword argument *force* is set to ``True``.\n    It is a convenience method intended for use by simple scripts\n    to do one-shot configuration of the logging package.\n\n    The default behaviour is to create a StreamHandler which writes to\n    sys.stderr, set a formatter using the BASIC_FORMAT format string, and\n    add the handler to the root logger.\n\n    A number of optional keyword arguments may be specified, which can alter\n    the default behaviour.\n\n    filename  Specifies that a FileHandler be created, using the specified\n              filename, rather than a StreamHandler.\n    filemode  Specifies the mode to open the file, if filename is specified\n              (if filemode is unspecified, it defaults to 'a').\n    format    Use the specified format string for the handler.\n    datefmt   Use the specified date/time format.\n    style     If a format string is specified, use this to specify the\n              type of format string (possible values '%', '{', '$', for\n              %-formatting, :meth:`str.format` and :class:`string.Template`\n              - defaults to '%').\n    level     Set the root logger level to the specified level.\n    stream    Use the specified stream to initialize the StreamHandler. Note\n              that this argument is incompatible with 'filename' - if both\n              are present, 'stream' is ignored.\n    handlers  If specified, this should be an iterable of already created\n              handlers, which will be added to the root logger. Any handler\n              in the list which does not have a formatter assigned will be\n              assigned the formatter created in this function.\n    force     If this keyword  is specified as true, any existing handlers\n              attached to the root logger are removed and closed, before\n              carrying out the configuration as specified by the other\n              arguments.\n    encoding  If specified together with a filename, this encoding is passed to\n              the created FileHandler, causing it to be used when the file is\n              opened.\n    errors    If specified together with a filename, this value is passed to the\n              created FileHandler, causing it to be used when the file is\n              opened in text mode. If not specified, the default value is\n              `backslashreplace`.\n\n    Note that you could specify a stream created using open(filename, mode)\n    rather than passing the filename and mode in. However, it should be\n    remembered that StreamHandler does not close its stream (since it may be\n    using sys.stdout or sys.stderr), whereas FileHandler closes its stream\n    when the handler is closed.\n\n    .. versionchanged:: 3.2\n       Added the ``style`` parameter.\n\n    .. versionchanged:: 3.3\n       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for\n       incompatible arguments (e.g. ``handlers`` specified together with\n       ``filename``/``filemode``, or ``filename``/``filemode`` specified\n       together with ``stream``, or ``handlers`` specified together with\n       ``stream``.\n\n    .. versionchanged:: 3.8\n       Added the ``force`` parameter.\n\n    .. versionchanged:: 3.9\n       Added the ``encoding`` and ``errors`` parameters.\n    \"\"\"\n    # Add thread safety in case someone mistakenly calls\n    # basicConfig() from multiple threads\n    _acquireLock()\n    try:\n        force = kwargs.pop('force', False)\n        encoding = kwargs.pop('encoding', None)\n        errors = kwargs.pop('errors', 'backslashreplace')\n        if force:\n            for h in root.handlers[:]:\n                root.removeHandler(h)\n                h.close()\n        if len(root.handlers) == 0:\n            handlers = kwargs.pop(\"handlers\", None)\n            if handlers is None:\n                if \"stream\" in kwargs and \"filename\" in kwargs:\n                    raise ValueError(\"'stream' and 'filename' should not be \"\n                                     \"specified together\")\n            else:\n                if \"stream\" in kwargs or \"filename\" in kwargs:\n                    raise ValueError(\"'stream' or 'filename' should not be \"\n                                     \"specified together with 'handlers'\")\n            if handlers is None:\n                filename = kwargs.pop(\"filename\", None)\n                mode = kwargs.pop(\"filemode\", 'a')\n                if filename:\n                    if 'b' in mode:\n                        errors = None\n                    else:\n                        encoding = io.text_encoding(encoding)\n                    h = FileHandler(filename, mode,\n                                    encoding=encoding, errors=errors)\n                else:\n                    stream = kwargs.pop(\"stream\", None)\n                    h = StreamHandler(stream)\n                handlers = [h]\n            dfs = kwargs.pop(\"datefmt\", None)\n            style = kwargs.pop(\"style\", '%')\n            if style not in _STYLES:\n                raise ValueError('Style must be one of: %s' % ','.join(\n                                 _STYLES.keys()))\n            fs = kwargs.pop(\"format\", _STYLES[style][1])\n            fmt = Formatter(fs, dfs, style)\n            for h in handlers:\n                if h.formatter is None:\n                    h.setFormatter(fmt)\n                root.addHandler(h)\n            level = kwargs.pop(\"level\", None)\n            if level is not None:\n                root.setLevel(level)\n            if kwargs:\n                keys = ', '.join(kwargs.keys())\n                raise ValueError('Unrecognised argument(s): %s' % keys)\n    finally:\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n# Utility functions at module level.\n# Basically delegate everything to the root logger.\n#---------------------------------------------------------------------------\n\ndef getLogger(name=None):\n    \"\"\"\n    Return a logger with the specified name, creating it if necessary.\n\n    If no name is specified, return the root logger.\n    \"\"\"\n    if not name or isinstance(name, str) and name == root.name:\n        return root\n    return Logger.manager.getLogger(name)\n\ndef critical(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'CRITICAL' on the root logger. If the logger\n    has no handlers, call basicConfig() to add a console handler with a\n    pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.critical(msg, *args, **kwargs)\n\ndef fatal(msg, *args, **kwargs):\n    \"\"\"\n    Don't use this function, use critical() instead.\n    \"\"\"\n    critical(msg, *args, **kwargs)\n\ndef error(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.error(msg, *args, **kwargs)\n\ndef exception(msg, *args, exc_info=True, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger, with exception\n    information. If the logger has no handlers, basicConfig() is called to add\n    a console handler with a pre-defined format.\n    \"\"\"\n    error(msg, *args, exc_info=exc_info, **kwargs)\n\ndef warning(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'WARNING' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.warning(msg, *args, **kwargs)\n\ndef warn(msg, *args, **kwargs):\n    warnings.warn(\"The 'warn' function is deprecated, \"\n        \"use 'warning' instead\", DeprecationWarning, 2)\n    warning(msg, *args, **kwargs)\n\ndef info(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'INFO' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.info(msg, *args, **kwargs)\n\ndef debug(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'DEBUG' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.debug(msg, *args, **kwargs)\n\ndef log(level, msg, *args, **kwargs):\n    \"\"\"\n    Log 'msg % args' with the integer severity 'level' on the root logger. If\n    the logger has no handlers, call basicConfig() to add a console handler\n    with a pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.log(level, msg, *args, **kwargs)\n\ndef disable(level=CRITICAL):\n    \"\"\"\n    Disable all logging calls of severity 'level' and below.\n    \"\"\"\n    root.manager.disable = level\n    root.manager._clear_cache()\n\ndef shutdown(handlerList=_handlerList):\n    \"\"\"\n    Perform any cleanup actions in the logging system (e.g. flushing\n    buffers).\n\n    Should be called at application exit.\n    \"\"\"\n    for wr in reversed(handlerList[:]):\n        #errors might occur, for example, if files are locked\n        #we just ignore them if raiseExceptions is not set\n        try:\n            h = wr()\n            if h:\n                try:\n                    h.acquire()\n                    # MemoryHandlers might not want to be flushed on close,\n                    # but circular imports prevent us scoping this to just\n                    # those handlers.  hence the default to True.\n                    if getattr(h, 'flushOnClose', True):\n                        h.flush()\n                    h.close()\n                except (OSError, ValueError):\n                    # Ignore errors which might be caused\n                    # because handlers have been closed but\n                    # references to them are still around at\n                    # application exit.\n                    pass\n                finally:\n                    h.release()\n        except: # ignore everything, as we're shutting down\n            if raiseExceptions:\n                raise\n            #else, swallow\n\n#Let's try and shutdown automatically on application exit...\nimport atexit\natexit.register(shutdown)\n\n# Null handler\n\nclass NullHandler(Handler):\n    \"\"\"\n    This handler does nothing. It's intended to be used to avoid the\n    \"No handlers could be found for logger XXX\" one-off warning. This is\n    important for library code, which may contain code to log events. If a user\n    of the library does not configure logging, the one-off warning might be\n    produced; to avoid this, the library developer simply needs to instantiate\n    a NullHandler and add it to the top-level logger of the library module or\n    package.\n    \"\"\"\n    def handle(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def emit(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def createLock(self):\n        self.lock = None\n\n    def _at_fork_reinit(self):\n        pass\n\n# Warnings integration\n\n_warnings_showwarning = None\n\ndef _showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"\n    Implementation of showwarnings which redirects to logging, which will first\n    check to see if the file parameter is None. If a file is specified, it will\n    delegate to the original warnings implementation of showwarning. Otherwise,\n    it will call warnings.formatwarning and will log the resulting string to a\n    warnings logger named \"py.warnings\" with level logging.WARNING.\n    \"\"\"\n    if file is not None:\n        if _warnings_showwarning is not None:\n            _warnings_showwarning(message, category, filename, lineno, file, line)\n    else:\n        s = warnings.formatwarning(message, category, filename, lineno, line)\n        logger = getLogger(\"py.warnings\")\n        if not logger.handlers:\n            logger.addHandler(NullHandler())\n        # bpo-46557: Log str(s) as msg instead of logger.warning(\"%s\", s)\n        # since some log aggregation tools group logs by the msg arg\n        logger.warning(str(s))\n\ndef captureWarnings(capture):\n    \"\"\"\n    If capture is true, redirect all warnings to the logging package.\n    If capture is False, ensure that warnings are not redirected to logging\n    but to their original destinations.\n    \"\"\"\n    global _warnings_showwarning\n    if capture:\n        if _warnings_showwarning is None:\n            _warnings_showwarning = warnings.showwarning\n            warnings.showwarning = _showwarning\n    else:\n        if _warnings_showwarning is not None:\n            warnings.showwarning = _warnings_showwarning\n            _warnings_showwarning = None\n", 2345], "/data/wangyingqi/code/pytorch/torch/_dynamo/eval_frame.py": ["# mypy: disable-error-code=\"method-assign\"\n\n\"\"\"\nThis module implements the core frame evaluation handler for TorchDynamo's compilation system.\nThe eval frame handler intercepts Python bytecode execution at runtime to enable dynamic\ncompilation and optimization of PyTorch code.\n\nKey components defined here:\n- Frame evaluation handlers that intercept and analyze Python execution frames\n- Guards management for tracking dependencies and invalidating compiled code\n- Optimization contexts and decorators (optimize, run_once, disable, etc.)\n- Export functionality for saving optimized graphs\n- Backend compiler integrations and callback management\n\nFunctions in this file are responsible for modifying the eval frame handler at RUNTIME.\nTherefore, all functions in this file are hot and performance-critical. Functions that\nonly execute at compile time should be placed in torch._dynamo.convert_frame.\n\nThe eval frame handler is the core mechanism that enables TorchDynamo to dynamically\nintercept, analyze and optimize PyTorch code during execution. It works by registering\na custom frame evaluation function that gets called for every Python frame, allowing\nus to detect PyTorch operations and trigger compilation as needed.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport atexit\nimport contextlib\nimport functools\nimport inspect\nimport logging\nimport os\nimport sys\nimport sysconfig\nimport textwrap\nimport threading\nimport traceback\nimport types\nimport warnings\nimport weakref\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom os.path import dirname, join\nfrom typing import Any, Callable, NamedTuple, Optional, TYPE_CHECKING, Union\nfrom unittest.mock import patch\n\nimport sympy\n\nimport torch\nimport torch.fx\nimport torch.utils._pytree as pytree\nimport torch.utils.checkpoint\nfrom torch import _guards\n\n# see discussion at https://github.com/pytorch/pytorch/issues/120699\nfrom torch._C._dynamo.eval_frame import (  # noqa: F401\n    reset_code,\n    set_code_exec_strategy,\n    set_eval_frame,\n    set_guard_complete_hook,\n    set_guard_error_hook,\n    set_skip_guard_eval_unsafe,\n    unsupported,\n)\nfrom torch._dispatch.python import enable_python_dispatcher\nfrom torch._dynamo.types import ConvertFrameReturn, FrameAction, FrameExecStrategy\nfrom torch._export.utils import _compiling_state_context\nfrom torch._subclasses.fake_tensor import unset_fake_temporarily\nfrom torch._utils_internal import justknobs_check, log_export_usage\nfrom torch.export.dynamic_shapes import (\n    _combine_args,\n    _DimHint,\n    _DimHintType,\n    _IntWrapper,\n    _process_dynamic_shapes,\n    _RelaxedConstraint,\n    Constraint,\n)\nfrom torch.fx import GraphModule\nfrom torch.fx.experimental._dynamism import (\n    clone_and_convert_to_meta,\n    track_dynamism_across_examples,\n)\nfrom torch.fx.experimental.proxy_tensor import make_fx\nfrom torch.fx.experimental.symbolic_shapes import (\n    ConstraintViolationError,\n    DimDynamic,\n    ShapeEnv,\n    StatelessSymbolicContext,\n)\nfrom torch.fx.graph import _PyTreeCodeGen, _PyTreeInfo\n\nfrom . import config, convert_frame, distributed, external_utils, trace_rules, utils\nfrom .backends.registry import CompilerFn, lookup_backend\nfrom .code_context import code_context\nfrom .exc import (\n    CondOpArgsMismatchError,\n    ShortenTraceback,\n    Unsupported,\n    UserError,\n    UserErrorType,\n)\nfrom .hooks import Hooks\nfrom .mutation_guard import install_generation_tagging_init\nfrom .utils import (\n    _get_error_on_graph_break,\n    _set_error_on_graph_break,\n    common_constant_types,\n    compile_times,\n)\n\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable, Sequence\n\n    from torch._dynamo.package import CompilePackage\n    from torch._dynamo.repro.after_dynamo import WrapBackendDebug\n    from torch._subclasses import fake_tensor\n    from torch.fx.node import Argument, Node, Target\n\n    from .types import (\n        CacheEntry,\n        DynamoCallback,\n        DynamoFrameType,\n        GuardFail,\n        GuardFilterEntry,\n    )\n\n\nlog = logging.getLogger(__name__)\n\n\nalways_optimize_code_objects = utils.ExactWeakKeyDictionary()\nnull_context = contextlib.nullcontext\n\n\n# See https://github.com/python/typing/pull/240\nclass Unset(Enum):\n    token = 0\n\n\ncached_backends: dict[int, CompilerFn] = {}\n\nunset = Unset.token\n\n\ndef _maybe_set_eval_frame(callback: DynamoCallback) -> DynamoCallback:\n    # A wrapper on set_eval_frame that is guarded by a Justknob.\n    # Users can disable torchDynamo by setting the JK to False.\n    if not justknobs_check(\"pytorch/compiler:enable_compiler_set_eval_frame\"):\n        torch._dynamo.utils.warn_once(\n            \"Dynamo disabled by Justknob: enable_compiler_set_eval_frame, skipping set_eval_frame\"\n        )\n        return callback\n    else:\n        return set_eval_frame(callback)\n\n\n@dataclass\nclass DynamoStance:\n    stance: str = \"default\"\n    skip_guard_eval_unsafe: bool = False\n    backend: Union[str, Callable[..., Any], None] = None\n\n\n_stance = DynamoStance()\n\n\ndef _set_stance(stance: DynamoStance) -> DynamoStance:\n    global _stance\n\n    from torch._C._dynamo.eval_frame import get_eval_frame_callback\n\n    callback = get_eval_frame_callback()\n\n    if callback is not False and callback is not None:\n        raise RuntimeError(\"attempted to set_stance in a torch.compile region\")\n\n    prior = _stance\n    _stance = stance\n    return prior\n\n\n_set_stance._dynamo_forbidden = True  # type: ignore[attr-defined]\n\n_EXAMPLE_INPUTS: Optional[dict[str, list[Any]]] = None\n\n\ndef get_example_inputs(key: str) -> list[Any]:\n    global _EXAMPLE_INPUTS\n    if _EXAMPLE_INPUTS is None:\n        _EXAMPLE_INPUTS = {}\n\n    if key not in _EXAMPLE_INPUTS:\n        _EXAMPLE_INPUTS[key] = []\n\n    return _EXAMPLE_INPUTS[key]\n\n\ndef _callback_from_stance(callback: DynamoCallback) -> DynamoCallback:\n    if _stance.stance == \"default\":\n        # force_backend\n        if _stance.backend is not None and callback not in (False, None):\n            callback = _create_wrapped_callback(get_compiler_fn(_stance.backend))\n\n        return callback\n    elif _stance.stance == \"eager_then_compile\":\n        if callback not in (False, None):\n            return _create_delayed_compile_callback(callback, _stance.stance)\n        return callback\n    elif _stance.stance == \"aot_eager_then_compile\":\n        if callback not in (False, None):\n            return _create_delayed_compile_callback(callback, _stance.stance)\n        return callback\n    elif _stance.stance == \"force_eager\":\n        # disable\n        return None\n    elif _stance.stance == \"eager_on_recompile\":\n        # run mode\n        return False\n    elif _stance.stance == \"fail_on_recompile\":\n        if callback in (False, None):\n            return callback\n\n        def fail_callback(\n            frame: DynamoFrameType, *args: Any, **kwargs: Any\n        ) -> ConvertFrameReturn:\n            if trace_rules.check(frame.f_code):\n                return ConvertFrameReturn()\n\n            from torch._C._dynamo.eval_frame import _debug_get_precompile_entries\n\n            message = (\n                \"Detected recompile when torch.compile stance is 'fail_on_recompile'. \"\n                + f\"filename: '{frame.f_code.co_filename}', \"\n                + f\"function name: '{frame.f_code.co_name}', \"\n                + f\"line number: {frame.f_lineno}\"\n            )\n            precompile_entries = _debug_get_precompile_entries(frame.f_code)\n            if len(precompile_entries) > 0:\n                message += \"\\nFailed on the following precompiled guards: \"\n                for entry in precompile_entries:\n                    message += f\"\\n{entry.guard_manager}{entry.guard_manager.check_verbose(frame.f_locals)}\"  # type: ignore[attr-defined]\n            raise RuntimeError(message)\n\n        # to prevent cache miss due to different backend\n        fail_callback._torchdynamo_orig_backend = callback  # type: ignore[attr-defined]\n\n        return fail_callback\n    else:\n        raise RuntimeError(f\"invalid torch.compile stance '{_stance}'\")\n\n\ndef _create_wrapped_callback(\n    compiler_fn: CompilerFn,\n) -> convert_frame.CatchErrorsWrapper:\n    hooks = Hooks()\n    return convert_frame.catch_errors_wrapper(\n        convert_frame.convert_frame(  # type: ignore[arg-type]\n            compiler_fn,\n            hooks,\n        ),\n        hooks,\n    )\n\n\ndef _get_or_add_example_inputs(frame: DynamoFrameType) -> list[Any]:\n    key = frame.f_code.co_filename + str(frame.f_code.co_firstlineno)\n    example_inputs = get_example_inputs(key)\n\n    if len(example_inputs) < 2:\n        example_inputs.append(clone_and_convert_to_meta(frame.f_locals))\n\n    return example_inputs\n\n\ndef _create_delayed_compile_callback(\n    callback: DynamoCallback, stance: str\n) -> Callable[..., Any]:\n    def callback_fn(*args: Any, **kwargs: Any) -> convert_frame.ConvertFrameReturn:\n        frame = args[0]\n        example_inputs = _get_or_add_example_inputs(frame)\n\n        if len(example_inputs) == 1:\n            if stance == \"eager_then_compile\":\n                return ConvertFrameReturn(\n                    frame_exec_strategy=FrameExecStrategy(\n                        FrameAction.DEFAULT, FrameAction.DEFAULT\n                    )\n                )\n            elif stance == \"aot_eager_then_compile\":\n                aot_eager_fn = get_compiler_fn(\"aot_eager\")\n                return _create_wrapped_callback(aot_eager_fn)(*args, **kwargs)\n\n        dynamism = track_dynamism_across_examples(example_inputs)\n        code_context.get_context(frame.f_code)[\"dynamism\"] = dynamism\n        compiler_fn = callback._torchdynamo_orig_backend._torchdynamo_orig_backend  # type: ignore[union-attr]\n        return _create_wrapped_callback(compiler_fn)(*args, **kwargs)\n\n    # to prevent cache miss due to different backend\n    callback_fn._torchdynamo_orig_backend = callback  # type: ignore[attr-defined]\n\n    return callback_fn\n\n\ndef _is_skip_guard_eval_unsafe_stance() -> bool:\n    return _stance.skip_guard_eval_unsafe\n\n\ndef _reset_guarded_backend_cache() -> None:\n    global cached_backends\n    for backend in cached_backends.values():\n        if hasattr(backend, \"reset\"):\n            backend.reset()\n    cached_backends.clear()\n\n\nDONT_WRAP_FILES = {\n    # For tracing into fx modules\n    inspect.getsourcefile(GraphModule),\n    join(dirname(dirname(__file__)), \"onnx/_internal/fx/dynamo_graph_extractor.py\"),\n}\n\n\ndef _debug_get_cache_entry_list(\n    code: Union[types.CodeType, Callable[..., Any]],\n) -> list[CacheEntry]:\n    \"\"\"\n    Given a code object or a callable object, retrieve the cache entries\n     stored in this code.\n    \"\"\"\n    if callable(code):\n        code = code.__code__\n    return torch._C._dynamo.eval_frame._debug_get_cache_entry_list(code)\n\n\nclass OptimizedModule(torch.nn.Module):\n    \"\"\"\n    Wraps the original nn.Module object and later patches its\n    forward method to optimized self.forward method.\n    \"\"\"\n\n    _torchdynamo_orig_callable: Callable[..., Any]\n    get_compiler_config: Callable[[], Any]\n\n    _opt_mod_attributes = {\n        \"_orig_mod\",\n        \"dynamo_ctx\",\n        \"_torchdynamo_orig_callable\",\n        \"get_compiler_config\",\n        \"forward\",\n        \"_forward\",\n        \"__dict__\",\n        \"named_children_walk\",\n        \"_super_module_initialized\",\n    }\n\n    def __init__(self, mod: torch.nn.Module, dynamo_ctx: _TorchDynamoContext) -> None:\n        # NOTE: this must go first, because attribute reads/writes of `self`\n        # uses `_orig_mod`, and sometimes users override `Module.__init__` to\n        # do attribute reads/writes on `self`.\n        #\n        # We also can't use regular setattr because `super().__setattr__` will\n        # complain for module value before `super().__init__()`\n        object.__setattr__(self, \"_orig_mod\", mod)\n        self._super_module_initialized = False\n        super().__init__()\n        self._super_module_initialized = True\n\n        # Installs the params/buffer\n        self._orig_mod = mod  # `super().__setattr__` will register this module\n        self.dynamo_ctx = dynamo_ctx\n        self._initialize()\n        self.training = self._orig_mod.training\n\n    def _initialize(self) -> None:\n        # Do this stuff in constructor to lower overhead slightly\n        if isinstance(self.dynamo_ctx, DisableContext):\n            # No need to check trace rules\n            self.forward = self.dynamo_ctx(self._orig_mod.__call__)\n        elif config.wrap_top_frame or (\n            isinstance(self._orig_mod.forward, types.MethodType)\n            and (\n                trace_rules.check(self._orig_mod.forward)\n                or getattr(self._orig_mod, \"_is_fsdp_managed_module\", False)\n            )\n        ):\n            # This may be a torch.nn.* instance in trace_rules.py which\n            # won't trigger a frame evaluation workaround to add an extra\n            # frame we can capture\n            self.forward = self.dynamo_ctx(external_utils.wrap_inline(self._orig_mod))\n        else:\n            # Invoke hooks outside of dynamo then pickup the inner frame\n            self.forward = self.dynamo_ctx(self._orig_mod.__call__)\n\n        if hasattr(self._orig_mod, \"_initialize_hook\"):\n            self._forward = self.forward\n            self.forward = self._call_lazy_check\n\n    def __call__(self, *args: Any, **kwargs: Any) -> Any:\n        if torch.nn.modules.module._has_any_global_hook():\n            warnings.warn(\n                \"Using `torch.compile(module)` when there are global hooks on \"\n                \"modules (e.g., from `register_module_forward_hook`); this will\"\n                \" cause the hooks to fire an extra time for the \"\n                \"`OptimizedModule` created by `torch.compile(module)`. If this \"\n                \"causes undesired behavior, please try using `module.compile()`\"\n                \", or use the per-module hooks instead\",\n                stacklevel=2,\n            )\n        return super().__call__(*args, **kwargs)\n\n    def __reduce__(\n        self,\n    ) -> tuple[type[OptimizedModule], tuple[torch.nn.Module, _TorchDynamoContext]]:\n        return (self.__class__, (self._orig_mod, self.dynamo_ctx))\n\n    def __getstate__(self) -> dict[str, Any]:\n        state = dict(self.__dict__)\n        state.pop(\"forward\", None)\n        state.pop(\"__call__\", None)\n        return state\n\n    def __setstate__(self, state: dict[str, Any]) -> None:\n        self.__dict__ = state\n        self._initialize()\n\n    @property\n    def training(self) -> bool:\n        return self._orig_mod.training\n\n    @training.setter\n    def training(self, value: bool) -> None:\n        # Ignore the `training` mutation in `super().__init__()`, since that's\n        # setting the default on `nn.Module`, but we are mirroring the\n        # `training` attr in `self._orig_mod`.\n        if self._super_module_initialized:\n            self._orig_mod.training = value\n\n    def __getattr__(self, name: str) -> Any:\n        if name == \"_orig_mod\":\n            return self._modules[\"_orig_mod\"]\n        return getattr(self._orig_mod, name)\n\n    def __setattr__(self, name: str, val: Any) -> None:\n        # Allow patching over class attributes\n        if hasattr(type(self), name):\n            return super().__setattr__(name, val)\n\n        if name in OptimizedModule._opt_mod_attributes:\n            return super().__setattr__(name, val)\n        return setattr(self._orig_mod, name, val)\n\n    def __delattr__(self, name: str) -> None:\n        # This mirrors `__setattr__`\n        if hasattr(type(self), name):\n            return super().__delattr__(name)\n\n        if name in OptimizedModule._opt_mod_attributes:\n            return super().__delattr__(name)\n        return delattr(self._orig_mod, name)\n\n    def _call_lazy_check(self, *args: Any, **kwargs: Any) -> Any:\n        if (\n            hasattr(self._orig_mod, \"_initialize_hook\")\n            and hasattr(self._orig_mod, \"_infer_parameters\")\n            and callable(self._orig_mod._infer_parameters)\n        ):\n            # In the case of a lazy module, we want to run\n            # the pre-hooks which initialize it.\n            # Afterwards, lazy module deletes its pre-hooks\n            # to avoid treating it as lazy on subsequent recompile.\n            self._orig_mod._infer_parameters(self._orig_mod, args, kwargs)\n        return self._forward(*args, **kwargs)\n\n    def __dir__(self) -> list[str]:\n        orig_mod_attrs = self._orig_mod.__dir__()\n        return orig_mod_attrs + [\n            attr for attr in super().__dir__() if attr not in orig_mod_attrs\n        ]\n\n\ndef remove_from_cache(f: Any) -> None:\n    \"\"\"\n    Make sure f.__code__ is not cached to force a recompile\n    \"\"\"\n    if isinstance(f, types.CodeType):\n        reset_code(f)\n    elif hasattr(f, \"__code__\"):\n        reset_code(f.__code__)\n    elif hasattr(getattr(f, \"forward\", None), \"__code__\"):\n        reset_code(f.forward.__code__)\n    else:\n        from . import reset  # type: ignore[attr-defined]\n\n        reset()\n        log.warning(\"could not determine __code__ for %s\", f)\n\n\ndef nothing() -> None:\n    pass\n\n\ndef always_false() -> bool:\n    return False\n\n\ndef innermost_fn(\n    fn: Callable[..., Any], unaltered_fn_attr: str = \"_torchdynamo_orig_callable\"\n) -> Callable[..., Any]:\n    \"\"\"\n    In case of nesting of _TorchDynamoContext calls, find the innermost\n    function. TorchDynamo caches on fn.__code__ object, so its necessary to find\n    the innermost function to pass on the optimize, run, disable etc.\n    \"\"\"\n    unaltered_fn = fn\n    while hasattr(unaltered_fn, unaltered_fn_attr):\n        unaltered_fn = getattr(unaltered_fn, unaltered_fn_attr)\n        assert callable(unaltered_fn), (\n            f\"A callable function is expected, but {type(unaltered_fn)} is provided.\"\n        )\n    return unaltered_fn\n\n\ndef make_set_enable_dynamic(enable: bool) -> Any:\n    assert isinstance(enable, bool)\n    if enable:\n        # Assume everything is dynamic by default\n        return config._make_closure_patcher(assume_static_by_default=False)\n    else:\n        return config._make_closure_patcher(\n            automatic_dynamic_shapes=False, assume_static_by_default=True\n        )\n\n\n# A thread local storage that serves to store information as Dynamo traces\n# through a user provided function.\nclass DynamoTLS(threading.local):\n    # Each string is a summary of a frame Dynamo attempted to trace, stored in\n    # temporal order.\n    traced_frame_infos: list[str] = []\n\n\ndynamo_tls = DynamoTLS()\n\n\ndef clear_dynamo_tls() -> None:\n    dynamo_tls.traced_frame_infos.clear()\n\n\n@atexit.register\ndef _log_traced_frames() -> None:\n    \"\"\"\n    At program exit, log all of the frames Dynamo has attempted to trace from,\n    excluding the continuation frames generated by Dynamo.\n    \"\"\"\n    msg = \"\\n\".join(dynamo_tls.traced_frame_infos)\n    msg = textwrap.indent(msg, \"  * \")\n    msg = f\"TorchDynamo attempted to trace the following frames: [\\n{msg}\\n]\"\n    log.info(msg)\n\n\ndef guard_collectives_hook(guard_eval_result: bool) -> bool:\n    import torch.distributed as dist\n    from torch._dynamo.utils import dynamo_timed\n\n    # guard_eval_result == True  ==>  cache hit\n    if pg := distributed.get_guard_pg():\n        with dynamo_timed(\n            \"guard_collective\", log_pt2_compile_event=False, log_waitcounter=True\n        ):\n            log.debug(\"guard_collective %s\", guard_eval_result)\n            # TODO: a bit awkward to time, this isn't inside of the dynamo compile region\n            all_results = [None] * pg.size()\n            dist.all_gather_object(all_results, guard_eval_result, group=pg)\n            # True = everyone hit, OK to run\n            # False = someone missed, force recompile everywhere\n            res = all(all_results)\n            log.debug(\"guard_collective %s -> %s\", guard_eval_result, res)\n            return res\n    return guard_eval_result\n\n\n_not_set = object()\n\n\nclass _TorchDynamoContext:\n    def __init__(\n        self,\n        callback: DynamoCallback,\n        on_enter: Callable[[], Any] = nothing,\n        backend_ctx_ctor: Callable[\n            [], contextlib.AbstractContextManager[Any]\n        ] = null_context,\n        patch_fn: Callable[[], Any] = nothing,\n        first_ctx: bool = False,\n        *,\n        error_on_graph_break: bool = False,\n        export: bool = False,\n        dynamic: Optional[bool] = None,\n        compiler_config: Optional[Any] = None,\n        package: Optional[CompilePackage] = None,\n    ) -> None:\n        super().__init__()\n        assert callable(callback) or callback is False or callback is None\n        self.callback: DynamoCallback = callback\n        self._backend_ctx_ctor = backend_ctx_ctor\n        self.prior: Union[Unset, DynamoCallback] = unset\n        self.first_ctx = first_ctx\n        self.error_on_graph_break = error_on_graph_break\n        self.export = export\n        self._dynamic = dynamic\n        self.compiler_config = compiler_config\n        self.cleanup_fns: list[Callable[[], Any]] = []\n        self.enter_exit_hooks = []\n        self._package = package\n        patch_fn()\n\n        # Save the backends so that we can reset them during torch._dynamo.reset\n        backend = innermost_fn(callback, unaltered_fn_attr=\"_torchdynamo_orig_backend\")  # type: ignore[arg-type]\n        cached_backends.setdefault(id(backend), backend)  # type: ignore[arg-type]\n\n        if dynamic is not None:\n            self.enter_exit_hooks.append(make_set_enable_dynamic(dynamic))\n\n        if on_enter is not nothing:\n            # this case is not common\n            def call_on_enter() -> Callable[[], None]:\n                on_enter()\n                return nothing\n\n            self.enter_exit_hooks.append(call_on_enter)\n\n        if backend_ctx_ctor is not contextlib.nullcontext:\n            # this case is not common\n            def call_backend_ctx() -> functools.partial[Optional[bool]]:\n                ctx = backend_ctx_ctor()\n                ctx.__enter__()\n                return functools.partial(ctx.__exit__, None, None, None)\n\n            self.enter_exit_hooks.append(call_backend_ctx)\n\n    def __enter__(self) -> None:\n        if config.raise_on_ctx_manager_usage:\n            raise RuntimeError(\n                \"torch._dynamo.optimize(...) is used with a context manager. \"\n                \"Please refer to https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html \"\n                \"to use torch._dynamo.optimize(...) as an annotation/decorator. \"\n            )\n        self.prior = set_eval_frame(None)\n        self.cleanup_fns = [enter() for enter in self.enter_exit_hooks]\n        self.prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n            _is_skip_guard_eval_unsafe_stance()\n        )\n        _maybe_set_eval_frame(_callback_from_stance(self.callback))\n\n    def __exit__(\n        self,\n        exc_type: Optional[type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[types.TracebackType],\n    ) -> Optional[bool]:\n        assert self.prior is not unset\n        set_eval_frame(None)\n        set_skip_guard_eval_unsafe(self.prior_skip_guard_eval_unsafe)\n        for cleanup in self.cleanup_fns:\n            cleanup()\n        self.cleanup_fns.clear()\n        _maybe_set_eval_frame(_callback_from_stance(self.prior))\n        self.prior = unset\n        return None\n\n    def __call__(self, fn: Any) -> Any:\n        # public api for compiler config/options\n        def get_compiler_config() -> Any:\n            return self.compiler_config\n\n        from .package import DynamoCache\n\n        # If self._package is lazily initialized, we should check the dynamo cache now\n        if config.caching_precompile:\n            assert self._package is not None\n            if not self._package.is_initialized():\n                result = DynamoCache.load(fn)\n                if result is None:\n                    # Create a fresh CompilePackage\n                    self._package.initialize(fn, None, ignore_inlined_sources=False)\n                else:\n                    cache_entry, backends = result\n                    try:\n                        self._package.initialize(\n                            fn, cache_entry, ignore_inlined_sources=False\n                        )\n                        self._package.install(backends)\n                    except RuntimeError as e:\n                        log.warning(\"Failed to load entry from dynamo cache: %s\", e)\n                        self._package.initialize(fn, None, ignore_inlined_sources=False)\n\n        fn = innermost_fn(fn)\n\n        # add context containing GraphModule to any GraphModule forward functions\n        if isinstance(fn, GraphModule):\n            # add context containing GraphModule to any GraphModule forward functions\n            code_context.get_context(fn.forward.__code__)[\"orig_graphmodule\"] = (\n                weakref.ref(fn)\n            )\n\n        # Optimize the forward method of torch.nn.Module object\n        if isinstance(fn, torch.nn.Module):\n            mod = fn\n            new_mod = OptimizedModule(mod, self)\n            # Save the function pointer to find the original callable while nesting\n            # of decorators.\n            new_mod._torchdynamo_orig_callable = mod.forward\n\n            # when compiling torch.nn.Module,\n            # provide public api OptimizedModule.get_compiler_config()\n            assert not hasattr(new_mod, \"get_compiler_config\")\n            new_mod.get_compiler_config = get_compiler_config\n\n            return new_mod\n\n        if inspect.isclass(fn):\n            # User has wrapped the class with compile/disable decorator. Apply\n            # disable to init/call method.\n            cls_obj = fn\n            cls_obj.__call__ = self(cls_obj.__call__)\n            if issubclass(cls_obj, torch.nn.Module):\n                # NN module variable tracker directly inlines the _call_impl.\n                cls_obj._call_impl = self(cls_obj._call_impl)\n            return cls_obj\n\n        assert callable(fn), (\n            f\"A callable function is expected, but {type(fn)} is provided.\"\n        )\n\n        try:\n            filename = inspect.getsourcefile(fn)\n        except TypeError:\n            filename = None\n        if config.wrap_top_frame or (\n            (filename is None or trace_rules.check(fn))\n            and (\n                getattr(fn, \"__name__\", \"\")\n                not in [\"_call_impl\", \"_wrapped_call_impl\", \"_lazy_forward\"]\n            )\n            and filename not in DONT_WRAP_FILES\n        ):\n            # call to a builtin without a frame for us to capture\n            fn = external_utils.wrap_inline(fn)\n\n        def do_nothing(*arg: Any, **kwargs: Any) -> None:\n            pass\n\n        callback: Callable[..., Any] = do_nothing\n        if hasattr(self, \"callback\"):\n            callback = self.callback  # type: ignore[assignment]\n\n        is_jit_tracing = torch._C._is_tracing\n        is_fx_tracing = torch.fx._symbolic_trace.is_fx_tracing\n\n        @functools.wraps(fn)\n        def compile_wrapper(*args: Any, **kwargs: Any) -> Any:\n            prior = set_eval_frame(None)\n            try:\n                if is_fx_tracing():\n                    if config.error_on_nested_fx_trace:\n                        raise RuntimeError(\n                            \"Detected that you are using FX to symbolically trace \"\n                            \"a dynamo-optimized function. This is not supported at the moment.\"\n                        )\n                    else:\n                        return fn(*args, **kwargs)\n\n                if is_jit_tracing():\n                    raise RuntimeError(\n                        \"Detected that you are using FX to torch.jit.trace \"\n                        \"a dynamo-optimized function. This is not supported at the moment.\"\n                    )\n\n                cleanups = [enter() for enter in self.enter_exit_hooks]\n                prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n                    _is_skip_guard_eval_unsafe_stance()\n                )\n                prior_error_on_graph_break = None\n                if self.error_on_graph_break is not None:\n                    prior_error_on_graph_break = _get_error_on_graph_break()\n                    _set_error_on_graph_break(self.error_on_graph_break)\n\n                # Ensure that if an assertion occurs after graph pushes\n                # something onto the DynamicLayerStack then we pop it off (the\n                # constructed graph code isn't guarded with try/finally).\n                #\n                # This used to be a context but putting a `with` here is a noticeable\n                # perf regression (#126293)\n                saved_dynamic_layer_stack_depth = (\n                    torch._C._functorch.get_dynamic_layer_stack_depth()\n                )\n\n                _maybe_set_eval_frame(_callback_from_stance(callback))\n\n                try:\n                    return fn(*args, **kwargs)\n                except Unsupported as e:\n                    if config.verbose:\n                        raise\n                    # strip internal tracebacks from causes\n                    cur_exn: BaseException = e\n                    while cur_exn.__cause__ is not None:\n                        cur_exn.__cause__.with_traceback(None)\n                        cur_exn = cur_exn.__cause__\n                    raise e.with_traceback(None) from e.__cause__  # User compiler error\n                except ShortenTraceback as e:\n                    # Failures in the backend likely don't have useful\n                    # data in the TorchDynamo frames, so we strip them out.\n                    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1\n                finally:\n                    # Restore the dynamic layer stack depth if necessary.\n                    set_eval_frame(None)\n                    if prior_error_on_graph_break is not None:\n                        _set_error_on_graph_break(prior_error_on_graph_break)\n                    torch._C._functorch.pop_dynamic_layer_stack_and_undo_to_depth(\n                        saved_dynamic_layer_stack_depth\n                    )\n\n                    set_skip_guard_eval_unsafe(prior_skip_guard_eval_unsafe)\n                    for cleanup in cleanups:\n                        cleanup()\n            finally:\n                _maybe_set_eval_frame(prior)\n\n        # hooks to properly handle inlining\n        compile_wrapper._torchdynamo_inline = (  # type: ignore[attr-defined]\n            external_utils.wrap_inline_with_set_fullgraph(fn, self.error_on_graph_break)\n        )\n\n        # Save the function pointer to find the original callable while nesting\n        # of decorators.\n        compile_wrapper._torchdynamo_orig_callable = fn  # type: ignore[attr-defined]\n\n        # when compiling user function instead of nn.Module\n        # provide public api _fn.get_compiler_config()\n        assert not hasattr(compile_wrapper, \"get_compiler_config\")\n        compile_wrapper.get_compiler_config = get_compiler_config  # type: ignore[attr-defined]\n\n        # If the function is called using torch._dynamo.optimize decorator, we\n        # should prevent any type of skipping.\n        if callback not in (None, False):\n            if not hasattr(fn, \"__code__\"):\n                raise RuntimeError(\n                    textwrap.dedent(\n                        \"\"\"\n\n                        torch._dynamo.optimize is called on a non function object.\n                        If this is a callable class, please wrap the relevant code into a function and optimize the\n                        wrapper function.\n\n                        >> class CallableClass:\n                        >>     def __init__(self) -> None:\n                        >>         super().__init__()\n                        >>         self.relu = torch.nn.ReLU()\n                        >>\n                        >>     def __call__(self, x):\n                        >>         return self.relu(torch.sin(x))\n                        >>\n                        >>     def print_hello(self):\n                        >>         print(\"Hello world\")\n                        >>\n                        >> mod = CallableClass()\n\n                        If you want to optimize the __call__ function and other code, wrap that up in a function\n\n                        >> def wrapper_fn(x):\n                        >>     y = mod(x)\n                        >>     return y.sum()\n\n                        and then optimize the wrapper_fn\n\n                        >> opt_wrapper_fn = torch._dynamo.optimize(wrapper_fn)\n                        \"\"\"\n                    )\n                )\n            always_optimize_code_objects[fn.__code__] = True\n\n        return compile_wrapper\n\n\nclass OptimizeContext(_TorchDynamoContext):\n    def __init__(\n        self,\n        callback: DynamoCallback,\n        backend_ctx_ctor: Callable[[], contextlib.AbstractContextManager[Any]],\n        first_ctx: bool = False,\n        *,\n        error_on_graph_break: bool = False,\n        export: bool = False,\n        dynamic: Optional[bool] = None,\n        compiler_config: Optional[Any] = None,\n        rebuild_ctx: Optional[\n            Callable[[], Union[OptimizeContext, _NullDecorator]]\n        ] = None,\n        package: Optional[CompilePackage] = None,\n    ) -> None:\n        def on_enter() -> None:\n            install_generation_tagging_init()\n\n        super().__init__(\n            callback=callback,\n            on_enter=on_enter,\n            backend_ctx_ctor=backend_ctx_ctor,\n            patch_fn=TorchPatcher.patch,\n            first_ctx=first_ctx,\n            error_on_graph_break=error_on_graph_break,\n            export=export,\n            dynamic=dynamic,\n            compiler_config=compiler_config,\n            package=package,\n        )\n\n        if config.compiled_autograd:\n            _dynamic = self._dynamic\n            if _dynamic is None:\n                _dynamic = not torch._dynamo.config.assume_static_by_default\n\n            def call_compiled_autograd() -> functools.partial[Optional[bool]]:\n                assert rebuild_ctx is not None\n                compiler_fn = rebuild_ctx()\n                ctx = torch._dynamo.compiled_autograd._enable(\n                    compiler_fn, dynamic=_dynamic, ignore_active_disable_ctx=False\n                )\n                ctx.__enter__()\n                return functools.partial(ctx.__exit__, None, None, None)\n\n            self.enter_exit_hooks.append(call_compiled_autograd)\n\n    def __reduce__(\n        self,\n    ) -> tuple[type[OptimizeContext], tuple[Any, ...], dict[str, Any]]:\n        return (\n            self.__class__,\n            (self.callback, self._backend_ctx_ctor, self.first_ctx),\n            {\n                \"export\": self.export,\n                \"dynamic\": self._dynamic,\n                \"compiler_config\": self.compiler_config,\n            },\n        )\n\n\nclass RunOnlyContext(_TorchDynamoContext):\n    def __init__(self) -> None:\n        # cudagraph trees relies on generation increment\n        def on_enter() -> None:\n            torch._dynamo.mutation_guard.GenerationTracker.generation += 1\n\n        super().__init__(callback=False, on_enter=on_enter)\n\n    def __reduce__(self) -> tuple[type[RunOnlyContext], tuple[Any, ...]]:\n        return (self.__class__, ())\n\n\nclass DisableContext(_TorchDynamoContext):\n    def __init__(self, msg: Optional[str] = None, wrapping: bool = True) -> None:\n        super().__init__(callback=None)\n        self.msg = msg\n        self.wrapping = wrapping\n\n    def __call__(self, fn: Callable[..., Any]) -> Callable[..., Any]:\n        # Earlier this code was in the base class _TorchDynamoContext. But we\n        # moved it here to have better code organization. For disable, we just\n        # want the callback to be None. We don't have to check trace_rules or\n        # create any wrapper.\n        fn = innermost_fn(fn)\n\n        if isinstance(fn, torch.nn.Module):\n            mod = fn\n            new_mod = OptimizedModule(mod, self)\n            new_mod._torchdynamo_orig_callable = mod.forward\n            return new_mod\n\n        if isinstance(fn, type):\n            # User has wrapped the class with compile/disable decorator. Apply\n            # disable to init/call method.\n            cls_obj = fn\n            # Disable on init is useful for reconstruction of bytecodes where we\n            # want to prevent Dynamo from tracing into the init function. Check\n            # test_reconstruction in test_model_output.py.\n            cls_obj.__init__ = self(cls_obj.__init__)  # type: ignore[misc]\n            cls_obj.__call__ = self(cls_obj.__call__)\n            if issubclass(cls_obj, torch.nn.Module):\n                # NN module variable tracker directly inlines the _call_impl. Disable it.\n                cls_obj._call_impl = self(cls_obj._call_impl)\n            return cls_obj\n\n        assert callable(fn), (\n            f\"A callable function is expected, but {type(fn)} is provided.\"\n        )\n\n        def _fn(*args: Any, **kwargs: Any) -> Any:\n            prior = set_eval_frame(None)\n            try:\n                _maybe_set_eval_frame(_callback_from_stance(self.callback))\n                try:\n                    return fn(*args, **kwargs)\n                finally:\n                    set_eval_frame(None)\n            finally:\n                _maybe_set_eval_frame(prior)\n\n        # Under some circumstances (e.g. precompile) we can end up calling @disable\n        # decorator in generated bytecode and trigger recompile. This is due to the\n        # fact that the old callback from torch.compile() is still active and under\n        # this circumstance we will trigger a failure with set_stance(\"fail_on_recompile\").\n        # Therefore we want to skip calling into any frame in this case.\n        if self.wrapping:\n            _fn = functools.wraps(fn)(_fn)\n\n        _fn._torchdynamo_disable = True  # type: ignore[attr-defined]\n        _fn._torchdynamo_disable_msg = self.msg  # type: ignore[attr-defined]\n\n        # Save the function pointer to find the original callable while nesting\n        # of decorators.\n        _fn._torchdynamo_orig_callable = fn  # type: ignore[attr-defined]\n\n        return _fn\n\n    def __reduce__(self) -> tuple[type[DisableContext], tuple[Any, ...]]:\n        return (self.__class__, ())\n\n\ndef _optimize_catch_errors(\n    compile_fn: convert_frame.ConvertFrameProtocol,\n    hooks: Hooks,\n    backend_ctx_ctor: Callable[\n        [], contextlib.AbstractContextManager[Any]\n    ] = null_context,\n    error_on_graph_break: bool = False,\n    export: bool = False,\n    dynamic: Optional[bool] = None,\n    compiler_config: Optional[Any] = None,\n    rebuild_ctx: Optional[Callable[[], Union[OptimizeContext, _NullDecorator]]] = None,\n    package: Optional[CompilePackage] = None,\n) -> OptimizeContext:\n    return OptimizeContext(\n        convert_frame.catch_errors_wrapper(compile_fn, hooks),\n        backend_ctx_ctor=backend_ctx_ctor,\n        first_ctx=True,\n        error_on_graph_break=error_on_graph_break,\n        export=export,\n        dynamic=dynamic,\n        compiler_config=compiler_config,\n        rebuild_ctx=rebuild_ctx,\n        package=package,\n    )\n\n\ndef get_compiler_fn(\n    compiler_fn: Union[str, Callable[..., Any], None],\n) -> WrapBackendDebug:\n    from .repro.after_dynamo import wrap_backend_debug\n\n    if compiler_fn is None:\n        # Special case None to avoid crashing in hasattr\n        compiler_str = None\n    elif hasattr(compiler_fn, \"compiler_name\"):\n        compiler_str = compiler_fn.compiler_name  # type: ignore[union-attr]\n        assert isinstance(compiler_str, str)\n    elif isinstance(compiler_fn, str):\n        compiler_str = compiler_fn\n    else:\n        compiler_str = None\n    compiler_fn = lookup_backend(compiler_fn)\n    return wrap_backend_debug(compiler_fn, compiler_str)\n\n\nclass _NullDecorator(contextlib.nullcontext):  # type: ignore[type-arg]\n    def __call__(self, fn: Callable[..., Any]) -> Callable[..., Any]:\n        assert callable(fn), (\n            f\"A callable function is expected, but {type(fn)} is provided.\"\n        )\n        return fn\n\n\ndef check_if_dynamo_supported() -> None:\n    if sys.version_info >= (3, 14):\n        raise RuntimeError(\"Python 3.14+ not yet supported for torch.compile\")\n    elif sysconfig.get_config_var(\"Py_GIL_DISABLED\") == 1 and sys.version_info < (\n        3,\n        13,\n        3,\n    ):\n        raise RuntimeError(\n            \"torch.compile is not supported on Python < 3.13.3 built with GIL disabled. \"\n            \"Please use Python 3.13.3+.\"\n        )\n\n\ndef is_dynamo_supported() -> bool:\n    try:\n        check_if_dynamo_supported()\n        return True\n    except Exception:\n        return False\n\n\ndef check_if_inductor_supported() -> None:\n    check_if_dynamo_supported()\n\n\ndef is_inductor_supported() -> bool:\n    try:\n        check_if_inductor_supported()\n        return True\n    except Exception:\n        return False\n\n\ndef check_for_incompatible_configs() -> None:\n    # Some of the configs should be mutually exclusive\n    assert not (config.suppress_errors and config.fail_on_recompile_limit_hit), (\n        \"Dynamo configs suppress_error and fail_on_recompile_limit_hit can not both be active at the same time.\"\n    )\n\n\ndef optimize(*args: Any, **kwargs: Any) -> Union[OptimizeContext, _NullDecorator]:\n    def rebuild_ctx() -> Union[OptimizeContext, _NullDecorator]:\n        ca_kwargs_override = config.compiled_autograd_kwargs_override\n        if ca_kwargs_override:\n            # NOTE: The process of translating other `torch.compile` kwargs to `torch._dynamo.optimize` kwargs\n            # is more complicated, we will add it in the future when needed.\n            assert set(ca_kwargs_override.keys()) == {\"fullgraph\"}, (\n                f\"Only `fullgraph` kwarg override is supported for now, but got {ca_kwargs_override.keys()}\"\n            )\n            kwargs[\"nopython\"] = ca_kwargs_override[\"fullgraph\"]\n        return optimize(*args, **kwargs)\n\n    return _optimize(rebuild_ctx, *args, **kwargs)\n\n\ndef _optimize(\n    rebuild_ctx: Callable[[], Union[OptimizeContext, _NullDecorator]],\n    backend: Union[str, Callable[..., Any]] = \"inductor\",\n    *,\n    nopython: bool = False,\n    guard_export_fn: Optional[Callable[[_guards.GuardsSet], None]] = None,\n    guard_fail_fn: Optional[Callable[[GuardFail], None]] = None,\n    guard_filter_fn: Optional[Callable[[list[GuardFilterEntry]], list[bool]]] = None,\n    disable: bool = False,\n    dynamic: Optional[bool] = None,\n    package: Optional[CompilePackage] = None,\n) -> Union[OptimizeContext, _NullDecorator]:\n    \"\"\"\n    The main entrypoint of TorchDynamo.  Do graph capture and call\n    backend() to optimize extracted graphs.\n\n    Args:\n        backend: One of the two things:\n            - Either, a function/callable taking a torch.fx.GraphModule and\n            example_inputs and returning a python callable that runs the\n            graph faster.\n            One can also provide additional context for the backend, like\n            torch.jit.fuser(\"fuser2\"), by setting the backend_ctx_ctor attribute.\n            See AOTAutogradMemoryEfficientFusionWithContext for the usage.\n            - Or, a string backend name in `torch._dynamo.list_backends()`\n        nopython: If True, graph breaks will be errors and there will\n            be a single whole-program graph.\n        disable: If True, turn this decorator into a no-op\n        dynamic: If True, upfront compile as dynamic a kernel as possible.  If False,\n            disable all dynamic shapes support (always specialize).  If None, automatically\n            detect when sizes vary and generate dynamic kernels upon recompile.\n\n    Example Usage::\n\n        @torch._dynamo.optimize()\n        def toy_example(a, b): ...\n    \"\"\"\n    check_if_dynamo_supported()\n    check_for_incompatible_configs()\n    # Note: The hooks object could be global instead of passed around, *however* that would make\n    # for a confusing API usage and plumbing story wherein we nest multiple .optimize calls.\n    # There is some prior art around this, w/r/t nesting backend calls are enforced to be the same\n    # compiler, however, this feels onerous for callback and hooks, and it feels better to give our users an\n    # easier to understand UX at the cost of a little more plumbing on our end.\n    hooks = Hooks(\n        guard_export_fn=guard_export_fn,\n        guard_fail_fn=guard_fail_fn,\n        guard_filter_fn=guard_filter_fn,\n    )\n    torch._C._log_api_usage_once(\"torch._dynamo.optimize\")\n    if (\n        disable\n        or os.environ.get(\"TORCHDYNAMO_DISABLE\", \"\") == \"1\"\n        or (not justknobs_check(\"pytorch/compiler:enable_dynamo\"))\n    ):\n        return _NullDecorator()\n\n    backend = get_compiler_fn(backend)\n\n    # Find if backend has any extra context manager\n    backend_ctx_ctor = getattr(backend, \"backend_ctx_ctor\", null_context)\n\n    # The backend function is stashed in the callable returned by\n    # _optimize_catch_errors in the field _torchdynamo_orig_backend. This can\n    # be used by eval_frame.c to insert a guard on the backend.\n\n    # With CachingPrecompile, instantiate an uninitialized CompilePackage\n    # which gets initialized by _optimize_catch_errors.__call__ once we have a function\n    if config.caching_precompile and package is None:\n        from .package import CompilePackage\n\n        package = CompilePackage(fn=None, dynamo=None, ignore_inlined_sources=False)\n\n    return _optimize_catch_errors(\n        convert_frame.convert_frame(\n            backend,\n            hooks,\n            package=package,\n        ),\n        hooks,\n        backend_ctx_ctor,\n        error_on_graph_break=nopython,\n        dynamic=dynamic,\n        compiler_config=(\n            backend.get_compiler_config()\n            if hasattr(backend, \"get_compiler_config\")\n            else None\n        ),\n        rebuild_ctx=rebuild_ctx,\n        package=package,\n    )\n\n\n# TODO(voz): Consider making \"explain\" output alongside a run / part of a run\n@patch(\"torch._dynamo.symbolic_convert.explain\", True)\ndef explain(f: Callable[..., Any], *extra_args: Any, **extra_kwargs: Any) -> Any:\n    from .backends.debugging import ExplainOutput\n\n    def inner(*args: Any, **kwargs: Any) -> ExplainOutput:\n        # TODO(voz): Do we want a decorator for this?\n        from . import reset  # type: ignore[attr-defined]\n\n        reset()\n\n        graphs: list[torch.fx.GraphModule] = []\n        break_reasons: list[Any] = []\n        op_count: int = 0\n        ops_per_graph: list[torch.fx.Node] = []\n        out_guards: list[_guards.Guard] = []\n\n        def dynamo_graph_accumulating_compiler(\n            gm: torch.fx.GraphModule, example_inputs: Any\n        ) -> Callable[..., Any]:\n            from .backends.debugging import _explain_graph_detail\n\n            nonlocal graphs\n            nonlocal op_count\n            nonlocal ops_per_graph\n            nonlocal break_reasons\n\n            gm, graphs, op_count, ops_per_graph, break_reasons = _explain_graph_detail(\n                gm, graphs, op_count, ops_per_graph, break_reasons\n            )\n\n            return gm.forward\n\n        def guard_export_print(guards: Iterable[_guards.Guard]) -> None:\n            nonlocal out_guards\n            out_guards.extend(guards)\n\n        opt_f = optimize(\n            dynamo_graph_accumulating_compiler,\n            nopython=False,\n            guard_export_fn=guard_export_print,\n        )(f)\n        # TODO(voz): We may have instances of `f` that mutate inputs, we should track sideeffects and reject.\n        opt_f(*args, **kwargs)\n\n        graph_count = len(graphs)\n        graph_break_count = graph_count - 1\n        compile_time = compile_times(repr=\"str\")\n\n        # TODO(voz): Do we want a decorator for this?\n        reset()\n\n        return ExplainOutput(\n            graphs,\n            graph_count,\n            graph_break_count,\n            break_reasons,\n            op_count,\n            ops_per_graph,\n            out_guards,\n            compile_time,\n        )\n\n    if extra_args or extra_kwargs:\n        warnings.warn(\n            \"explain(f, *args, **kwargs) is deprecated, use explain(f)(*args, **kwargs) instead.  \"\n            \"If you don't migrate, we may break your explain call in the future if your user defined kwargs \"\n            \"conflict with future kwargs added to explain(f).\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return inner(*extra_args, **extra_kwargs)\n    else:\n        return inner\n\n\nclass FlattenInputOutputSignature(torch.fx.Transformer):\n    def __init__(\n        self,\n        m: torch.fx.GraphModule,\n        flat_args: list[Any],\n        matched_input_elements_positions: list[int],\n        flat_results: Sequence[Any],\n        matched_output_elements_positions: list[int],\n        example_fake_inputs: list[torch.Tensor],\n        flat_args_dynamic_dims: list[set[int]],\n        fake_mode: Optional[fake_tensor.FakeTensorMode] = None,\n    ) -> None:\n        super().__init__(m)\n\n        assert len(flat_args_dynamic_dims) == len(flat_args)\n        matched_input_elements_to_fake = {\n            val: example_fake_inputs[ix]\n            for ix, val in enumerate(matched_input_elements_positions)\n        }\n\n        self.new_args = []\n        for i in range(0, len(flat_args)):\n            arg = super().placeholder(f\"arg{i}\", (), {})\n            if i in matched_input_elements_to_fake:\n                arg.node.meta[\"val\"] = matched_input_elements_to_fake[i]\n            else:\n                # Fill node.meta[\"val\"] with faketensor from the input,\n                # if it's not found in matched_input_elements_positions\n                if fake_mode is not None and isinstance(flat_args[i], torch.Tensor):\n                    # TODO(zhxchen17) Also preserve all the user constraints here.\n                    arg.node.meta[\"val\"] = fake_mode.from_tensor(\n                        flat_args[i],\n                        symbolic_context=StatelessSymbolicContext(\n                            dynamic_sizes=[\n                                (\n                                    DimDynamic.DYNAMIC\n                                    if d in flat_args_dynamic_dims[i]\n                                    else DimDynamic.STATIC\n                                )\n                                for d in range(len(flat_args[i].shape))\n                            ],\n                            constraint_sizes=[None] * len(flat_args[i].shape),\n                        ),\n                    )\n                elif isinstance(flat_args[i], _IntWrapper):\n                    arg.node.meta[\"val\"] = flat_args[i].val\n                else:\n                    arg.node.meta[\"val\"] = flat_args[i]\n\n            self.new_args.append(arg)\n        self.old_args_gen = (self.new_args[i] for i in matched_input_elements_positions)\n        self.matched_output_elements_positions = matched_output_elements_positions\n        self.flat_results = flat_results\n\n    def placeholder(\n        self, target: Target, args: tuple[Argument, ...], kwargs: dict[str, Any]\n    ) -> Any:\n        arg = next(self.old_args_gen)\n        if \"val\" in self.current_node.meta:\n            arg.node.meta[\"val\"] = self.current_node.meta[\"val\"]\n        if \"tensor_dict\" in self.current_node.meta:\n            arg.node.meta[\"tensor_dict\"] = self.current_node.meta[\"tensor_dict\"]\n        if \"example_value\" in self.current_node.meta:\n            # NB: intentionally do not use set_example_value\n            arg.node.meta[\"example_value\"] = self.current_node.meta[\"example_value\"]\n        if \"unbacked_bindings\" in self.current_node.meta:\n            arg.node.meta[\"unbacked_bindings\"] = self.current_node.meta[\n                \"unbacked_bindings\"\n            ]\n        return arg\n\n    def output(\n        self, target: Target, args: tuple[Argument, ...], kwargs: dict[str, Any]\n    ) -> Any:\n        dynamo_result_flat = args[0]\n        lookup = [*dynamo_result_flat, *self.new_args]  # type: ignore[misc]\n        new_results_flat = []\n        for i in range(len(self.flat_results)):\n            if self.matched_output_elements_positions[i] is not None:\n                new_results_flat.append(\n                    lookup[self.matched_output_elements_positions[i]]\n                )\n            else:\n                const_val = self.flat_results[i]\n                assert isinstance(const_val, tuple(common_constant_types))\n                new_results_flat.append(const_val)\n        return super().output(target, (new_results_flat,), {})\n\n    def run_node(self, n: Node) -> Any:\n        self.current_node = n\n        result_proxy = super().run_node(n)\n        if \"val\" in self.current_node.meta:\n            result_proxy.node.meta[\"val\"] = self.current_node.meta[\"val\"]\n        if \"example_value\" in self.current_node.meta:\n            # NB: intentionally do not use set_example_value\n            result_proxy.node.meta[\"example_value\"] = self.current_node.meta[\n                \"example_value\"\n            ]\n        if \"unbacked_bindings\" in self.current_node.meta:\n            result_proxy.node.meta[\"unbacked_bindings\"] = self.current_node.meta[\n                \"unbacked_bindings\"\n            ]\n        if self.current_node.op != \"output\":\n            result_proxy.node._rename(\n                getattr(self.current_node, \"name\", result_proxy.node.name)\n            )\n        return result_proxy\n\n    def transform(self) -> torch.fx.GraphModule:\n        result_gm = super().transform()\n        if \"dynamo_flat_name_to_original_fqn\" in self.module.meta:  # type: ignore[operator]\n            result_gm.meta[\"dynamo_flat_name_to_original_fqn\"] = self.module.meta[  # type: ignore[index]\n                \"dynamo_flat_name_to_original_fqn\"  # type: ignore[index]\n            ]\n        if \"dynamo_compile_id\" in self.module.meta:  # type: ignore[operator]\n            result_gm.meta[\"dynamo_compile_id\"] = self.module.meta[\"dynamo_compile_id\"]  # type: ignore[index]\n        return result_gm\n\n\nclass ExportResult(NamedTuple):\n    graph_module: torch.fx.GraphModule\n    guards: _guards.GuardsSet\n    # NB: Do not add new fields without overriding __iter__; people are\n    # destructuring so it is BC-breaking\n\n\n# NOTE: this function only supports graphs created by Dynamo's OutputGraph module\ndef check_signature_rewritable(graph: torch.fx.GraphModule) -> None:\n    input_errors = []\n    for node in graph.graph.find_nodes(op=\"placeholder\"):\n        # set in OutputGraph._call_user_compiler\n        assert hasattr(node, \"_dynamo_source\")\n        assert hasattr(graph, \"_source_to_user_stacks\")\n\n        # NOTE: We can safely ignore these type warnings if and only if\n        # the function is made from OutputGraph (checked in the assertions)\n        source = node._dynamo_source  # type: ignore[attr-defined]\n        user_stacks = graph._source_to_user_stacks.get(source)  # type: ignore[operator, union-attr]\n        if user_stacks is None:\n            continue\n        assert len(user_stacks) > 0\n        # In some cases we may not have a useful stack.  Look for a\n        # useful stack\n        stack = None\n        for s in user_stacks:\n            if len(s) == 0:\n                continue\n            stack = s\n            break\n        if stack is None:\n            msg = f\"{source.name()}, a closed over free variable\"\n        else:\n            tb = \"\".join(traceback.format_list(stack))\n            extra = \"\"\n            if len(user_stacks) > 1:\n                extra = f\"(elided {len(user_stacks) - 1} more accesses)\"\n            msg = f\"{source.name()}, accessed at:\\n{tb}{extra}\"\n        # TODO: option to print ALL of the stack traces at once\n        input_errors.append(msg)\n\n    if input_errors:\n        raise UserError(\n            UserErrorType.INVALID_INPUT,\n            \"Cannot export model which references tensors that are neither \"\n            \"buffers/parameters/constants nor are direct inputs.  For each tensor, if you'd \"\n            \"like this tensor to be an explicit input, add it as a dummy argument \"\n            \"to the top-level model definition you are exporting; if you would \"\n            \"like its value to be embedded as an exported constant, wrap its access \"\n            \"in a function marked with @assume_constant_result.\\n\\n\"\n            + \"\\n\\n\".join(input_errors),\n        )\n\n\ndef rewrite_signature(\n    f_sig: inspect.Signature,\n    graph: torch.fx.GraphModule,\n    fake_mode: Optional[fake_tensor.FakeTensorMode],\n    flat_args: list[Any],\n    in_spec: pytree.TreeSpec,\n    example_fake_inputs: list[Any],\n    graph_captured_input: Iterable[Any],\n    graph_captured_output: Optional[Iterable[Any]],\n    dynamo_traced_result: Any,\n    flat_args_dynamic_dims: list[set[int]],\n) -> torch.fx.GraphModule:\n    orig_args, orig_kwargs = pytree.tree_unflatten(flat_args, in_spec)\n\n    def check_user_input_output(\n        flat_values: list[Any], error_type: UserErrorType\n    ) -> None:\n        supported_types = [\n            torch.Tensor,\n            torch.SymInt,\n            torch.SymFloat,\n            torch.SymBool,\n            torch._C.ScriptObject,\n            _IntWrapper,\n        ] + list(common_constant_types)\n\n        def is_supported_type(val: Any) -> bool:\n            return isinstance(val, tuple(supported_types))\n\n        value_type = \"input\" if error_type == UserErrorType.INVALID_INPUT else \"output\"\n        # We only check that the outputs are not None. Inputs can be None.\n        for v in flat_values:\n            if not is_supported_type(v):\n                if error_type == UserErrorType.INVALID_INPUT and v is None:\n                    continue\n\n                raise UserError(\n                    error_type,\n                    f\"It looks like one of the {value_type}s with type `{type(v)}` \"\n                    \"is not supported or pytree-flattenable. \\n\"\n                    f\"Exported graphs {value_type}s can only contain the \"\n                    f\"following supported types: {supported_types}. \\n\"\n                    \"If you are using a custom class object, \"\n                    \"please register a pytree_flatten/unflatten function \"\n                    \"using `torch.utils._pytree.register_pytree_node` or \"\n                    \"`torch.export.register_dataclass`.\",\n                )\n\n    check_user_input_output(flat_args, UserErrorType.INVALID_INPUT)\n    flat_results_traced, out_spec_traced = pytree.tree_flatten(dynamo_traced_result)\n    check_user_input_output(flat_results_traced, UserErrorType.INVALID_OUTPUT)\n\n    def check_optional_input_and_error(f_sig: inspect.Signature) -> None:\n        # Check if function has optional input.\n        for name, param in f_sig.parameters.items():\n            if param.default is not inspect.Parameter.empty:\n                from torch._dynamo.exc import Unsupported\n\n                log.error(\n                    \"Parameter %s is optional with a default value of %s\",\n                    name,\n                    param.default,\n                )\n                raise Unsupported(\n                    \"Tracing through optional input is not supported yet\",\n                    case_name=\"optional_input\",\n                )\n\n    def produce_matching(\n        debug_type: str, sources: Iterable[Any], candidates: Iterable[Any]\n    ) -> list[Optional[int]]:\n        matched_elements_positions: list[Optional[int]] = []\n        dict_of_source_vals = {}\n        for i, val in enumerate(sources):\n            dict_of_source_vals[id(val)] = i\n\n        for i, val in enumerate(candidates):\n            if isinstance(val, tuple(common_constant_types)):\n                matched_elements_positions.append(None)\n            elif id(val) not in dict_of_source_vals:\n                if debug_type == \"inputs\":\n                    check_optional_input_and_error(f_sig)\n                raise AssertionError(\n                    f\"Unexpectedly found a {type(val)} in the {debug_type}.\\n\"\n                    'Please file an issue along with a paste of the logs from TORCH_LOGS=\"+export\"',\n                )\n            else:\n                matched_elements_positions.append(dict_of_source_vals[id(val)])\n\n        return matched_elements_positions\n\n    matched_input_elements_positions = produce_matching(\n        \"inputs\", flat_args, graph_captured_input\n    )\n\n    assert graph_captured_output is not None\n    matched_output_elements_positions = produce_matching(\n        \"outputs\", list(graph_captured_output) + flat_args, flat_results_traced\n    )\n\n    new_graph = FlattenInputOutputSignature(\n        graph,\n        flat_args,\n        matched_input_elements_positions,  # type: ignore[arg-type]\n        flat_results_traced,\n        matched_output_elements_positions,  # type: ignore[arg-type]\n        example_fake_inputs,\n        flat_args_dynamic_dims,\n        fake_mode,\n    ).transform()\n\n    # Make dynamo graph to have same input/output spec as user code\n    def argument_names(\n        f_sig: inspect.Signature, args: list[Any], kwargs: dict[str, Any]\n    ) -> list[str]:\n        def signature_to_fullargspec(sig: inspect.Signature) -> inspect.FullArgSpec:\n            # Get a list of Parameter objects from the Signature object\n            params = list(sig.parameters.values())\n            # Separate positional arguments, keyword-only arguments and varargs/varkw\n            args = [\n                p.name\n                for p in params\n                if p.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD\n            ]\n            kwonlyargs = [\n                p.name for p in params if p.kind == inspect.Parameter.KEYWORD_ONLY\n            ]\n            varargs = next(\n                (p.name for p in params if p.kind == inspect.Parameter.VAR_POSITIONAL),\n                None,\n            )\n            varkw = next(\n                (p.name for p in params if p.kind == inspect.Parameter.VAR_KEYWORD),\n                None,\n            )\n            # Get default values for positional arguments and keyword-only arguments\n            defaults = tuple(\n                p.default\n                for p in params\n                if p.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD\n                and p.default is not inspect.Parameter.empty\n            )\n            kwonlydefaults = {\n                p.name: p.default\n                for p in params\n                if p.kind == inspect.Parameter.KEYWORD_ONLY\n                and p.default is not inspect.Parameter.empty\n            }\n            # Get annotations for parameters and return value\n            annotations = {}\n            if sig.return_annotation:\n                annotations = {\"return\": sig.return_annotation}\n            for parameter in params:\n                annotations[parameter.name] = parameter.annotation\n            # Return a FullArgSpec object with the extracted attributes\n            return inspect.FullArgSpec(\n                args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations\n            )\n\n        fullargspec = signature_to_fullargspec(f_sig)\n\n        # 1. Map `args` 1-to-1 to positional arguments in original signature.\n        input_strs = fullargspec.args[: len(args)]\n\n        if len(args) > len(fullargspec.args):\n            # 2. If there are more arguments left in `args`, they map to varargs in original\n            # signature. Assign names as {varargs}_0, {varargs}_1, ...\n            assert fullargspec.varargs is not None, \"More arguments than expected\"\n            input_strs += [\n                f\"{fullargspec.varargs}_{i}\"\n                for i in range(0, len(args) - len(input_strs))\n            ]\n        elif len(args) < len(fullargspec.args):\n            # 3. If there are fewer arguments in `args` than `fullargspec.args`,\n            # it implies these are arguments either with default values, or provided in\n            # `kwargs`. The former can be safely ignored. Because Dynamo.export does not\n            # export them as part of the function signature. The latter will be handled\n            # in the next step.\n            for unprovided_arg in fullargspec.args[\n                len(args) : -len(fullargspec.defaults or [])\n            ]:\n                assert unprovided_arg in kwargs, f\"Missing argument {unprovided_arg}\"\n\n        # 4. Keyword arguments provided in `kwargs`.\n        input_strs += list(kwargs.keys())\n\n        # 5. Keyword-only arguments with default values if not provided are not exported\n        # as part of the function signature.\n        for kwonly_arg in fullargspec.kwonlyargs:\n            kwonlydefaults = fullargspec.kwonlydefaults or {}\n            assert kwonly_arg in kwargs or kwonly_arg in kwonlydefaults, (\n                f\"Missing keyword only argument {kwonly_arg}\"\n            )\n\n        return input_strs\n\n    new_graph.graph._codegen = _PyTreeCodeGen(\n        _PyTreeInfo(\n            argument_names(f_sig, orig_args, orig_kwargs),\n            in_spec,\n            out_spec_traced,\n        )\n    )\n    new_graph.recompile()\n    return new_graph\n\n\ndef export(\n    f: Callable[..., Any],\n    *extra_args: Any,\n    aten_graph: bool = False,\n    pre_dispatch: bool = False,\n    decomposition_table: Optional[\n        dict[torch._ops.OpOverload, Callable[..., Any]]\n    ] = None,\n    tracing_mode: str = \"symbolic\",\n    dynamic_shapes: Optional[Union[dict[str, Any], tuple[Any], list[Any]]] = None,\n    specialize_float: bool = True,\n    assume_static_by_default: bool = False,\n    same_signature: bool = True,\n    disable_constraint_solver: bool = False,\n    prefer_deferred_runtime_asserts_over_guards: bool = False,\n    allow_complex_guards_as_runtime_asserts: bool = False,\n    _log_export_usage: bool = True,\n    constraints: Optional[list[Constraint]] = None,\n    **extra_kwargs: Any,\n) -> Callable[..., ExportResult]:\n    \"\"\"\n    Export an input function f to a format that can be executed outside of PyTorch using the FX graph.\n\n    Args:\n        f (callable): A PyTorch function to be exported.\n\n        aten_graph (bool): If True, exports a graph with ATen operators.\n        If False, exports a graph with Python operators. Default is False.\n\n        pre_dispatch (bool): If True, exports a graph with ATen operators,\n        but before any logic in the PyTorch dispatcher has run.\n        This can be useful if you want to apply further transformations on a graph before running it\n        through autograd, autocast, or any other functionalities that are integrated into the dispatcher.\n        This flag is only valid if aten_graph=True is set.\n        Default is False.\n\n        decomposition_table (dict): A dictionary that maps operators to their decomposition functions.\n        Required if aten_graph or tracing_mode is specified. Default is None.\n\n        tracing_mode (str): If \"symbolic\", turn on dynamic shapes support. Default is \"symbolic\".\n\n        dynamic_shapes:\n         An optional argument where the type should either be:\n         1) a dict from argument names of ``f`` to their dynamic shape specifications,\n         2) a tuple that specifies dynamic shape specifications for each input in original order.\n         If you are specifying dynamism on keyword args, you will need to pass them in the order that\n         is defined in the original function signature.\n\n         The dynamic shape of a tensor argument can be specified as either\n         (1) a dict from dynamic dimension indices to :func:`Dim` types, where it is\n         not required to include static dimension indices in this dict, but when they are,\n         they should be mapped to None; or (2) a tuple / list of :func:`Dim` types or None,\n         where the :func:`Dim` types correspond to dynamic dimensions, and static dimensions\n         are denoted by None. Arguments that are dicts or tuples / lists of tensors are\n         recursively specified by using mappings or sequences of contained specifications.\n\n        same_signature (bool): If True, rewrite the returned graph's signature to be the same as f.\n\n        disable_constraint_solver (bool): Whether the dim constraint solver must be disabled.\n\n    Returns:\n        A function that given args and kwargs, returns a tuple of (graph, guards)\n        Graph: An FX graph representing the execution of the input PyTorch function with the provided arguments and options.\n        Guards: The guards we accumulated during tracing f above\n\n    Raises:\n        AssertionError: If decomposition_table is specified without setting aten_graph=True,\n        or if graph breaks during tracing in export.\n\n        AssertionError: If Dynamo input and output is not consistent with traced input/output.\n\n    Note - this headerdoc was authored by ChatGPT, with slight modifications by the author.\n    \"\"\"\n    if _log_export_usage:\n        log_export_usage(event=\"export.private_api\", flags={\"_dynamo\"})\n\n    # Deal with \"local variable referenced before assignment\"\n    _f = f\n    _specialize_float = specialize_float\n    _assume_static_by_default = assume_static_by_default\n    _constraints = constraints\n\n    def inner(*args: Any, **kwargs: Any) -> ExportResult:\n        if not _constraints:\n            combined_args = _combine_args(_f, args, kwargs)\n            constraints = _process_dynamic_shapes(combined_args, dynamic_shapes)\n        else:\n            constraints = _constraints\n\n        f = _f\n        specialize_float = _specialize_float\n        assume_static_by_default = _assume_static_by_default\n        check_if_dynamo_supported()\n        torch._C._log_api_usage_once(\"torch._dynamo.export\")\n        if decomposition_table is not None:\n            assert aten_graph, (\n                \"Specifying a decomposition_table table or tracing mode is illegal without setting aten_graph=True\"\n            )\n        if pre_dispatch:\n            assert aten_graph, \"pre_dispatch=True can only be used when aten_graph=True\"\n        f = innermost_fn(f)\n        call_to_inspect = f.forward if isinstance(f, torch.nn.Module) else f\n        original_signature = inspect.signature(call_to_inspect)  # type: ignore[arg-type]\n        graph = None\n        out_guards = None\n        graph_captured_input = None\n        graph_captured_result: Optional[tuple[torch.Tensor, ...]] = None\n        fake_mode = None\n        result_traced = None\n\n        def guard_export_print(guards: _guards.GuardsSet) -> None:\n            nonlocal out_guards\n            assert out_guards is None, (\n                \"whole graph export entails exactly one guard export\"\n            )\n            out_guards = guards\n\n        example_inputs: list[Any] = []\n\n        def dynamo_normalization_capturing_compiler(\n            gm: torch.fx.GraphModule, inner_example_inputs: list[Any]\n        ) -> Callable[..., Any]:\n            nonlocal graph\n            assert graph is None, (\n                \"Tried to emit a second graph during export. Tracing through 'f' must produce a single graph.\"\n            )\n            graph = gm\n\n            nonlocal fake_mode, example_inputs\n            # NB: do NOT pass inner_example_inputs here, we are detecting the\n            # Dynamo allocated fake mode, which should be DISTINCT from a\n            # potential outer ambient fake mode which the user provided.\n            # example_inputs is always the user specified inputs, so they\n            # would have the wrong fake mode attached to them\n            fake_mode = _guards.detect_fake_mode()\n            example_inputs = inner_example_inputs\n\n            def result_capturing_wrapper(*graph_inputs: Any) -> Any:\n                nonlocal graph_captured_result\n                nonlocal graph_captured_input\n\n                graph_captured_input = graph_inputs\n                assert graph is not None\n\n                named_parameters = dict(graph.named_parameters(remove_duplicate=False))\n                named_buffers = dict(graph.named_buffers(remove_duplicate=False))\n\n                ambient_fake_mode = (\n                    _guards.detect_fake_mode(graph_inputs)\n                    if _guards.detect_fake_mode(graph_inputs) is not None\n                    else fake_mode\n                )\n\n                # We reran fake tensor propagation, but we didn't do\n                # anything with the resulting unbacked SymInts.  Drop them\n                # from the pending list.\n                # NB: this is wrong if graph_captured_result has\n                # data-dependent output size!\n                ignore_fresh_unbacked = null_context()\n                assert ambient_fake_mode is not None\n                if shape_env := ambient_fake_mode.shape_env:\n                    ignore_fresh_unbacked = shape_env.ignore_fresh_unbacked_symbols()\n\n                with (\n                    ambient_fake_mode,\n                    enable_python_dispatcher(),\n                    ignore_fresh_unbacked,\n                ):\n                    params_and_buffers = {\n                        **named_parameters,\n                        **named_buffers,\n                    }\n                    fake_params_buffers = {}\n\n                    for name, value in params_and_buffers.items():\n                        fake_params_buffers[name] = ambient_fake_mode.from_tensor(\n                            value, static_shapes=True\n                        )\n\n                    from torch._export.non_strict_utils import (\n                        key_path_to_source,\n                        KeyPath,\n                    )\n\n                    def fakify_with_ambient(\n                        path: KeyPath, t: Union[torch.Tensor, _IntWrapper, Any]\n                    ) -> Any:\n                        if isinstance(t, torch.Tensor):\n                            return ambient_fake_mode.from_tensor(t, static_shapes=True)\n                        elif isinstance(t, _IntWrapper):\n                            if (\n                                t.dynamism is not None\n                                and isinstance(t.dynamism, _DimHint)\n                                and t.dynamism.type\n                                in (\n                                    _DimHintType.DYNAMIC,\n                                    _DimHintType.AUTO,\n                                )\n                            ):  # type: ignore[union-attr]\n                                source = key_path_to_source(path)\n                                symint = ambient_fake_mode.shape_env.create_unspecified_symint_and_symbol(  # type: ignore[union-attr]\n                                    t.val, source, DimDynamic.DYNAMIC\n                                )\n                                return symint\n                            else:\n                                return t.val\n                        else:\n                            return t\n\n                    fake_graph_inputs = pytree.tree_map_with_path(\n                        fakify_with_ambient, graph_inputs\n                    )\n                    graph_captured_result = torch.func.functional_call(\n                        graph, fake_params_buffers, fake_graph_inputs\n                    )\n\n                return graph_captured_result\n\n            return result_capturing_wrapper\n\n        # Note: This is needed by rewrite_signature. We need to put it before\n        # optimize_assert since user program may mutate the inputs.\n        flat_args, in_spec = pytree.tree_flatten((args, kwargs))\n\n        remove_from_cache(f)\n        constraint_violation_error = None\n        if tracing_mode != \"symbolic\":\n            assume_static_by_default = True\n        with (\n            config.patch(\n                specialize_int=True,\n                specialize_float=specialize_float,\n                assume_static_by_default=assume_static_by_default,\n                automatic_dynamic_shapes=False,\n                capture_dynamic_output_shape_ops=True,\n                capture_scalar_outputs=True,\n                prefer_deferred_runtime_asserts_over_guards=prefer_deferred_runtime_asserts_over_guards,\n                allow_complex_guards_as_runtime_asserts=allow_complex_guards_as_runtime_asserts,\n            ),\n            _compiling_state_context(),\n        ):\n            opt_f = optimize_assert(\n                dynamo_normalization_capturing_compiler,\n                hooks=Hooks(\n                    guard_export_fn=guard_export_print,\n                    guard_fail_fn=None,\n                ),\n                export=True,\n                export_constraints=constraints,\n            )(f)\n            # TODO(voz): We may have instances of `f` that mutate inputs, we should track sideeffects and reject.\n            try:\n                result_traced = opt_f(*args, **kwargs)\n            except ConstraintViolationError as e:\n                constraint_violation_error = e\n        remove_from_cache(f)\n\n        if (\n            not disable_constraint_solver\n            and (shape_env := getattr(fake_mode, \"shape_env\", None)) is not None\n            and (dim_constraints := shape_env.dim_constraints) is not None\n            and not isinstance(\n                call_to_inspect, (torch._ops.OpOverloadPacket, torch._ops.OpOverload)\n            )\n            and not trace_rules.check(call_to_inspect)\n        ):\n            dim_constraints.solve()\n            forced_specializations = dim_constraints.forced_specializations()\n            msg = dim_constraints.prettify_results(\n                original_signature,\n                dynamic_shapes,\n                constraint_violation_error,\n                forced_specializations,\n            )\n            if constraint_violation_error:\n                constraint_violation_error.args = (\n                    constraint_violation_error.args[0] + msg,\n                )\n            else:\n                if forced_specializations:\n                    constraint_violation_error = ConstraintViolationError(msg)\n                else:\n                    log.info(\n                        \"Summary of dimension constraints:%s\",\n                        msg,\n                    )\n\n            # Error if we have any constraints on static values\n            for k in shape_env.var_to_range.keys():\n                if isinstance(k, sympy.Integer):\n                    constraint_violation_error = ConstraintViolationError(\n                        f\"{''.join(traceback.format_list(shape_env.var_to_stack[k]))}\\n\"\n                        \"It appears that you're trying to set a constraint on a \"\n                        f\"value which we evaluated to have a static value of {k}. \"\n                        'Set TORCH_LOGS=\"+export\" for more information.'\n                    )\n        if constraint_violation_error:\n            raise constraint_violation_error\n\n        if graph is None:\n            assert same_signature, (\n                \"Failed to produce a graph during tracing as no tensor operations were found and same_signature is False.\"\n            )\n            # If the module does not contain any tensor computation, we would create a graph with inputs and outputs.\n            # To be consistent with the graph traced by dynano, `graph` will have only tensor inputs as placeholders\n            # and tensor outputs as output nodes. non-tensor inputs and outputs will be added when rewriting signature.\n            # We will also construct the `example_inputs`, `graph_captured_input`, and `graph_captured_result` corresponding\n            # to `graph`.\n            example_inputs = []\n            graph_captured_input = ()\n            graph_captured_result = ()\n            fake_mode = torch._subclasses.FakeTensorMode(\n                shape_env=ShapeEnv(), export=True\n            )\n            if out_guards is None:\n                out_guards = _guards.GuardsSet()\n            assert out_guards is not None  # suppress mypy error\n            parameter_names = list(original_signature.parameters.keys())\n            fx_graph = torch.fx.Graph()\n            for i, name in enumerate(parameter_names):\n                if torch.is_tensor(flat_args[i]):\n                    node = fx_graph.placeholder(name)\n                    node.meta[\"val\"] = fake_mode.from_tensor(\n                        flat_args[i], static_shapes=True\n                    )\n                    graph_captured_input = graph_captured_input + (flat_args[i],)\n                    example_inputs.append(flat_args[i])\n            fx_graph.output(graph_captured_result)\n            module = torch.nn.Module()\n            graph = torch.fx.GraphModule(module, fx_graph)\n            log.info(\n                \"Failed to capture a graph during tracing as no tensor operations were found.:\\n\\n%s\",\n                graph.print_readable(print_output=False, colored=True),\n            )\n        else:\n            assert out_guards is not None, \"Failed to produce guards during tracing\"\n            assert fake_mode is not None\n\n            log.info(\n                \"Dynamo captured graph:\\n\\n%s\",\n                graph.print_readable(print_output=False, colored=True),\n            )\n\n            # This check need to happened before aten_graph\n            # because placeholder's _source_node attribute is not preserved by make_fx\n            if same_signature:\n                check_signature_rewritable(graph)\n\n        # NB: This is mostly hitting the cache; Dynamo already converted these\n        example_fake_inputs = [\n            fake_mode.from_tensor(t) if isinstance(t, torch.Tensor) else t\n            for t in example_inputs\n        ]\n\n        if aten_graph:\n            # Running graph with interpreter is needed for propagating the stack_trace\n            def graph_with_interpreter(*args: Any) -> Any:\n                with torch.fx.traceback.preserve_node_meta():\n                    return torch.fx.Interpreter(graph).run(*args)  # type: ignore[arg-type]\n\n            with unset_fake_temporarily(), enable_python_dispatcher(), fake_mode:\n                try:\n                    graph = make_fx(\n                        graph_with_interpreter,\n                        decomposition_table=decomposition_table,\n                        tracing_mode=\"real\",\n                        _allow_non_fake_inputs=True,\n                        pre_dispatch=pre_dispatch,\n                        _allow_fake_constant=False,\n                    )(*example_fake_inputs)\n                except CondOpArgsMismatchError as e:\n                    # Wrap the internal error to the user-facing error\n                    raise UserError(  # noqa: B904\n                        UserErrorType.DYNAMIC_CONTROL_FLOW,\n                        str(e),\n                        case_name=\"cond_operands\",\n                    )\n\n            assert graph is not None\n            for node in graph.graph.find_nodes(op=\"get_attr\"):\n                if isinstance(getattr(graph, node.target), torch.Tensor):  # type: ignore[arg-type]\n                    node.meta[\"val\"] = fake_mode.from_tensor(\n                        getattr(graph, node.target),  # type: ignore[arg-type]\n                        static_shapes=True,\n                    )\n\n        if same_signature:\n            flat_args_dynamic_dims = [\n                {\n                    c.dim\n                    for c in (constraints or ())\n                    if (\n                        c.t_id == id(x)\n                        and not isinstance(c, _RelaxedConstraint)\n                        and c.constraint_range.vr.lower != c.constraint_range.vr.upper\n                    )\n                }\n                for x in flat_args\n            ]\n            graph = rewrite_signature(\n                original_signature,\n                graph,\n                fake_mode,\n                flat_args,\n                in_spec,\n                example_fake_inputs,\n                graph_captured_input,  # type: ignore[arg-type]\n                graph_captured_result,\n                result_traced,  # type: ignore[possibly-undefined]\n                flat_args_dynamic_dims,\n            )\n        return ExportResult(graph, out_guards)\n\n    if extra_args or extra_kwargs:\n        warnings.warn(\n            \"export(f, *args, **kwargs) is deprecated, use export(f)(*args, **kwargs) instead.  \"\n            \"If you don't migrate, we may break your export call in the future if your user defined kwargs \"\n            \"conflict with future kwargs added to export(f).\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        return inner(*extra_args, **extra_kwargs)  # type: ignore[return-value]\n    else:\n        return inner\n\n\ndef optimize_assert(*args: Any, **kwargs: Any) -> OptimizeContext:\n    if \"rebuild_ctx\" in kwargs and kwargs[\"rebuild_ctx\"] is not None:\n        # called from optimize\n        rebuild_ctx = kwargs[\"rebuild_ctx\"]\n        del kwargs[\"rebuild_ctx\"]\n    else:\n\n        def rebuild_ctx() -> OptimizeContext:\n            return optimize_assert(*args, **kwargs)\n\n    return _optimize_assert(rebuild_ctx, *args, **kwargs)\n\n\ndef _optimize_assert(\n    rebuild_ctx: Callable[[], OptimizeContext],\n    backend: Union[str, Callable[..., Any], None],\n    *,\n    hooks: Hooks = Hooks(None, None, None),\n    export: bool = False,\n    export_constraints: Optional[Any] = None,\n    dynamic: Optional[bool] = None,\n    package: Optional[CompilePackage] = None,\n) -> OptimizeContext:\n    \"\"\"\n    The same as `torch._dynamo.optimize(backend, nopython=True)`,\n    but ignores symbolic_convert.error_on_graph_break setting.\n\n    Used for export, since we must always error on graph breaks and ignore\n    symbolic_convert.error_on_graph_break. Can also be used for testing.\n    \"\"\"\n    backend = get_compiler_fn(backend)\n\n    # Find if backend has any extra context manager\n    backend_ctx_ctor = getattr(backend, \"backend_ctx_ctor\", null_context)\n\n    if config.caching_precompile and package is None:\n        # Create an uninitialized package that will be set/filled by\n        # _OptimizeContext.__call__\n        # We need to instantiate the object here because the same CompilePackage\n        # needs to be shared between convert_frame_assert\n        # and OptimizeContext.\n        from .package import CompilePackage\n\n        package = CompilePackage(fn=None, dynamo=None, ignore_inlined_sources=False)\n\n    return _optimize_catch_errors(\n        convert_frame.convert_frame_assert(\n            backend,\n            export=export,\n            export_constraints=export_constraints,\n            package=package,\n        ),\n        hooks,\n        backend_ctx_ctor,\n        export=export,\n        dynamic=dynamic,\n        rebuild_ctx=rebuild_ctx,\n        package=package,\n    )\n\n\nclass TorchPatcher:\n    @staticmethod\n    @functools.cache\n    def patch() -> None:\n        # A better way to disable the following would be decorate the source\n        # functions with @torch._disable_dynamo. However, this causes issues\n        # with torch.deploy internally.\n        from .decorators import disable\n\n        torch.jit.trace = disable(\n            torch.jit.trace, reason=\"tracing into TorchScript not fully supported\"\n        )\n        torch.jit.trace_module = disable(\n            torch.jit.trace_module,\n            reason=\"tracing into TorchScript not fully supported\",\n        )\n        torch.jit._get_trace_graph = disable(\n            torch.jit._get_trace_graph,\n            reason=\"tracing into TorchScript not fully supported\",\n        )\n        torch.fx._symbolic_trace.Tracer.trace = disable(\n            torch.fx._symbolic_trace.Tracer.trace,\n            reason=\"tracing into FX not fully supported\",\n        )\n        torch.distributions.Distribution.set_default_validate_args(False)\n\n        from torch.optim import (\n            adadelta,\n            adagrad,\n            adam,\n            adamax,\n            adamw,\n            asgd,\n            lbfgs,\n            nadam,\n            radam,\n            rmsprop,\n            rprop,\n            sgd,\n            sparse_adam,\n        )\n\n        optimizer_modules = {\n            adadelta,\n            adagrad,\n            adam,\n            adamax,\n            adamw,\n            asgd,\n            lbfgs,\n            nadam,\n            radam,\n            rmsprop,\n            rprop,\n            sgd,\n            sparse_adam,\n        }\n\n        for opt_mod in optimizer_modules:\n            opt_name = opt_mod.__name__.split(\".\")[-1]\n            fused_fn_name = f\"_fused_{opt_name}\"\n\n            if hasattr(opt_mod, fused_fn_name):\n                setattr(\n                    opt_mod,\n                    fused_fn_name,\n                    disable(\n                        getattr(opt_mod, fused_fn_name),\n                        reason=\"don't trace into fused optimizer\",\n                    ),\n                )\n\n        optimizer_classes = [\n            opt\n            for opt in torch.optim.__dict__.values()\n            if inspect.isclass(opt) and issubclass(opt, torch.optim.Optimizer)\n        ]\n\n        # Note: we don't support sparsity or tracing through backwards\n        excluded_optimizer_classes = {\n            torch.optim.SparseAdam,\n            torch.optim.LBFGS,\n        }\n\n        for opt in optimizer_classes:\n            if opt in excluded_optimizer_classes:\n                opt.step = disable(\n                    opt.step, reason=f\"optimizer {opt} step not supported\"\n                )\n\n            if hasattr(opt, \"_init_group\"):\n                opt._init_group = disable(\n                    opt._init_group, reason=f\"optimizer {opt} _init_group not supported\"\n                )\n\n    @staticmethod\n    def suppress_torch_distributed_warnings(\n        fn: Callable[..., Any],\n    ) -> Callable[..., Any]:\n        def inner_fn(*args: Any, **kwargs: Any) -> Any:\n            with torch._logging.hide_warnings(\n                torch._logging._internal.user_warning_filter\n            ):\n                return fn(*args, **kwargs)\n\n        return inner_fn\n\n\ndef skip_code(code: types.CodeType) -> None:\n    set_code_exec_strategy(\n        code, FrameExecStrategy(FrameAction.SKIP, FrameAction.DEFAULT)\n    )\n", 2287], "<frozen importlib._bootstrap>": ["\"\"\"Core implementation of import.\n\nThis module is NOT meant to be directly imported! It has been designed such\nthat it can be bootstrapped into Python as the implementation of import. As\nsuch it requires the injection of specific modules and attributes in order to\nwork. One should use importlib as the public-facing version of this module.\n\n\"\"\"\n#\n# IMPORTANT: Whenever making changes to this module, be sure to run a top-level\n# `make regen-importlib` followed by `make` in order to get the frozen version\n# of the module updated. Not doing so will result in the Makefile to fail for\n# all others who don't have a ./python around to freeze the module\n# in the early stages of compilation.\n#\n\n# See importlib._setup() for what is injected into the global namespace.\n\n# When editing this code be aware that code executed at import time CANNOT\n# reference any injected objects! This includes not only global code but also\n# anything specified at the class level.\n\ndef _object_name(obj):\n    try:\n        return obj.__qualname__\n    except AttributeError:\n        return type(obj).__qualname__\n\n# Bootstrap-related code ######################################################\n\n# Modules injected manually by _setup()\n_thread = None\n_warnings = None\n_weakref = None\n\n# Import done by _install_external_importers()\n_bootstrap_external = None\n\n\ndef _wrap(new, old):\n    \"\"\"Simple substitute for functools.update_wrapper.\"\"\"\n    for replace in ['__module__', '__name__', '__qualname__', '__doc__']:\n        if hasattr(old, replace):\n            setattr(new, replace, getattr(old, replace))\n    new.__dict__.update(old.__dict__)\n\n\ndef _new_module(name):\n    return type(sys)(name)\n\n\n# Module-level locking ########################################################\n\n# For a list that can have a weakref to it.\nclass _List(list):\n    pass\n\n\n# Copied from weakref.py with some simplifications and modifications unique to\n# bootstrapping importlib. Many methods were simply deleting for simplicity, so if they\n# are needed in the future they may work if simply copied back in.\nclass _WeakValueDictionary:\n\n    def __init__(self):\n        self_weakref = _weakref.ref(self)\n\n        # Inlined to avoid issues with inheriting from _weakref.ref before _weakref is\n        # set by _setup(). Since there's only one instance of this class, this is\n        # not expensive.\n        class KeyedRef(_weakref.ref):\n\n            __slots__ = \"key\",\n\n            def __new__(type, ob, key):\n                self = super().__new__(type, ob, type.remove)\n                self.key = key\n                return self\n\n            def __init__(self, ob, key):\n                super().__init__(ob, self.remove)\n\n            @staticmethod\n            def remove(wr):\n                nonlocal self_weakref\n\n                self = self_weakref()\n                if self is not None:\n                    if self._iterating:\n                        self._pending_removals.append(wr.key)\n                    else:\n                        _weakref._remove_dead_weakref(self.data, wr.key)\n\n        self._KeyedRef = KeyedRef\n        self.clear()\n\n    def clear(self):\n        self._pending_removals = []\n        self._iterating = set()\n        self.data = {}\n\n    def _commit_removals(self):\n        pop = self._pending_removals.pop\n        d = self.data\n        while True:\n            try:\n                key = pop()\n            except IndexError:\n                return\n            _weakref._remove_dead_weakref(d, key)\n\n    def get(self, key, default=None):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            wr = self.data[key]\n        except KeyError:\n            return default\n        else:\n            if (o := wr()) is None:\n                return default\n            else:\n                return o\n\n    def setdefault(self, key, default=None):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            o = None\n        if o is None:\n            if self._pending_removals:\n                self._commit_removals()\n            self.data[key] = self._KeyedRef(default, key)\n            return default\n        else:\n            return o\n\n\n# A dict mapping module names to weakrefs of _ModuleLock instances.\n# Dictionary protected by the global import lock.\n_module_locks = {}\n\n# A dict mapping thread IDs to weakref'ed lists of _ModuleLock instances.\n# This maps a thread to the module locks it is blocking on acquiring.  The\n# values are lists because a single thread could perform a re-entrant import\n# and be \"in the process\" of blocking on locks for more than one module.  A\n# thread can be \"in the process\" because a thread cannot actually block on\n# acquiring more than one lock but it can have set up bookkeeping that reflects\n# that it intends to block on acquiring more than one lock.\n#\n# The dictionary uses a WeakValueDictionary to avoid keeping unnecessary\n# lists around, regardless of GC runs. This way there's no memory leak if\n# the list is no longer needed (GH-106176).\n_blocking_on = None\n\n\nclass _BlockingOnManager:\n    \"\"\"A context manager responsible to updating ``_blocking_on``.\"\"\"\n    def __init__(self, thread_id, lock):\n        self.thread_id = thread_id\n        self.lock = lock\n\n    def __enter__(self):\n        \"\"\"Mark the running thread as waiting for self.lock. via _blocking_on.\"\"\"\n        # Interactions with _blocking_on are *not* protected by the global\n        # import lock here because each thread only touches the state that it\n        # owns (state keyed on its thread id).  The global import lock is\n        # re-entrant (i.e., a single thread may take it more than once) so it\n        # wouldn't help us be correct in the face of re-entrancy either.\n\n        self.blocked_on = _blocking_on.setdefault(self.thread_id, _List())\n        self.blocked_on.append(self.lock)\n\n    def __exit__(self, *args, **kwargs):\n        \"\"\"Remove self.lock from this thread's _blocking_on list.\"\"\"\n        self.blocked_on.remove(self.lock)\n\n\nclass _DeadlockError(RuntimeError):\n    pass\n\n\n\ndef _has_deadlocked(target_id, *, seen_ids, candidate_ids, blocking_on):\n    \"\"\"Check if 'target_id' is holding the same lock as another thread(s).\n\n    The search within 'blocking_on' starts with the threads listed in\n    'candidate_ids'.  'seen_ids' contains any threads that are considered\n    already traversed in the search.\n\n    Keyword arguments:\n    target_id     -- The thread id to try to reach.\n    seen_ids      -- A set of threads that have already been visited.\n    candidate_ids -- The thread ids from which to begin.\n    blocking_on   -- A dict representing the thread/blocking-on graph.  This may\n                     be the same object as the global '_blocking_on' but it is\n                     a parameter to reduce the impact that global mutable\n                     state has on the result of this function.\n    \"\"\"\n    if target_id in candidate_ids:\n        # If we have already reached the target_id, we're done - signal that it\n        # is reachable.\n        return True\n\n    # Otherwise, try to reach the target_id from each of the given candidate_ids.\n    for tid in candidate_ids:\n        if not (candidate_blocking_on := blocking_on.get(tid)):\n            # There are no edges out from this node, skip it.\n            continue\n        elif tid in seen_ids:\n            # bpo 38091: the chain of tid's we encounter here eventually leads\n            # to a fixed point or a cycle, but does not reach target_id.\n            # This means we would not actually deadlock.  This can happen if\n            # other threads are at the beginning of acquire() below.\n            return False\n        seen_ids.add(tid)\n\n        # Follow the edges out from this thread.\n        edges = [lock.owner for lock in candidate_blocking_on]\n        if _has_deadlocked(target_id, seen_ids=seen_ids, candidate_ids=edges,\n                blocking_on=blocking_on):\n            return True\n\n    return False\n\n\nclass _ModuleLock:\n    \"\"\"A recursive lock implementation which is able to detect deadlocks\n    (e.g. thread 1 trying to take locks A then B, and thread 2 trying to\n    take locks B then A).\n    \"\"\"\n\n    def __init__(self, name):\n        # Create an RLock for protecting the import process for the\n        # corresponding module.  Since it is an RLock, a single thread will be\n        # able to take it more than once.  This is necessary to support\n        # re-entrancy in the import system that arises from (at least) signal\n        # handlers and the garbage collector.  Consider the case of:\n        #\n        #  import foo\n        #  -> ...\n        #     -> importlib._bootstrap._ModuleLock.acquire\n        #        -> ...\n        #           -> <garbage collector>\n        #              -> __del__\n        #                 -> import foo\n        #                    -> ...\n        #                       -> importlib._bootstrap._ModuleLock.acquire\n        #                          -> _BlockingOnManager.__enter__\n        #\n        # If a different thread than the running one holds the lock then the\n        # thread will have to block on taking the lock, which is what we want\n        # for thread safety.\n        self.lock = _thread.RLock()\n        self.wakeup = _thread.allocate_lock()\n\n        # The name of the module for which this is a lock.\n        self.name = name\n\n        # Can end up being set to None if this lock is not owned by any thread\n        # or the thread identifier for the owning thread.\n        self.owner = None\n\n        # Represent the number of times the owning thread has acquired this lock\n        # via a list of True.  This supports RLock-like (\"re-entrant lock\")\n        # behavior, necessary in case a single thread is following a circular\n        # import dependency and needs to take the lock for a single module\n        # more than once.\n        #\n        # Counts are represented as a list of True because list.append(True)\n        # and list.pop() are both atomic and thread-safe in CPython and it's hard\n        # to find another primitive with the same properties.\n        self.count = []\n\n        # This is a count of the number of threads that are blocking on\n        # self.wakeup.acquire() awaiting to get their turn holding this module\n        # lock.  When the module lock is released, if this is greater than\n        # zero, it is decremented and `self.wakeup` is released one time.  The\n        # intent is that this will let one other thread make more progress on\n        # acquiring this module lock.  This repeats until all the threads have\n        # gotten a turn.\n        #\n        # This is incremented in self.acquire() when a thread notices it is\n        # going to have to wait for another thread to finish.\n        #\n        # See the comment above count for explanation of the representation.\n        self.waiters = []\n\n    def has_deadlock(self):\n        # To avoid deadlocks for concurrent or re-entrant circular imports,\n        # look at _blocking_on to see if any threads are blocking\n        # on getting the import lock for any module for which the import lock\n        # is held by this thread.\n        return _has_deadlocked(\n            # Try to find this thread.\n            target_id=_thread.get_ident(),\n            seen_ids=set(),\n            # Start from the thread that holds the import lock for this\n            # module.\n            candidate_ids=[self.owner],\n            # Use the global \"blocking on\" state.\n            blocking_on=_blocking_on,\n        )\n\n    def acquire(self):\n        \"\"\"\n        Acquire the module lock.  If a potential deadlock is detected,\n        a _DeadlockError is raised.\n        Otherwise, the lock is always acquired and True is returned.\n        \"\"\"\n        tid = _thread.get_ident()\n        with _BlockingOnManager(tid, self):\n            while True:\n                # Protect interaction with state on self with a per-module\n                # lock.  This makes it safe for more than one thread to try to\n                # acquire the lock for a single module at the same time.\n                with self.lock:\n                    if self.count == [] or self.owner == tid:\n                        # If the lock for this module is unowned then we can\n                        # take the lock immediately and succeed.  If the lock\n                        # for this module is owned by the running thread then\n                        # we can also allow the acquire to succeed.  This\n                        # supports circular imports (thread T imports module A\n                        # which imports module B which imports module A).\n                        self.owner = tid\n                        self.count.append(True)\n                        return True\n\n                    # At this point we know the lock is held (because count !=\n                    # 0) by another thread (because owner != tid).  We'll have\n                    # to get in line to take the module lock.\n\n                    # But first, check to see if this thread would create a\n                    # deadlock by acquiring this module lock.  If it would\n                    # then just stop with an error.\n                    #\n                    # It's not clear who is expected to handle this error.\n                    # There is one handler in _lock_unlock_module but many\n                    # times this method is called when entering the context\n                    # manager _ModuleLockManager instead - so _DeadlockError\n                    # will just propagate up to application code.\n                    #\n                    # This seems to be more than just a hypothetical -\n                    # https://stackoverflow.com/questions/59509154\n                    # https://github.com/encode/django-rest-framework/issues/7078\n                    if self.has_deadlock():\n                        raise _DeadlockError(f'deadlock detected by {self!r}')\n\n                    # Check to see if we're going to be able to acquire the\n                    # lock.  If we are going to have to wait then increment\n                    # the waiters so `self.release` will know to unblock us\n                    # later on.  We do this part non-blockingly so we don't\n                    # get stuck here before we increment waiters.  We have\n                    # this extra acquire call (in addition to the one below,\n                    # outside the self.lock context manager) to make sure\n                    # self.wakeup is held when the next acquire is called (so\n                    # we block).  This is probably needlessly complex and we\n                    # should just take self.wakeup in the return codepath\n                    # above.\n                    if self.wakeup.acquire(False):\n                        self.waiters.append(None)\n\n                # Now take the lock in a blocking fashion.  This won't\n                # complete until the thread holding this lock\n                # (self.owner) calls self.release.\n                self.wakeup.acquire()\n\n                # Taking the lock has served its purpose (making us wait), so we can\n                # give it up now.  We'll take it w/o blocking again on the\n                # next iteration around this 'while' loop.\n                self.wakeup.release()\n\n    def release(self):\n        tid = _thread.get_ident()\n        with self.lock:\n            if self.owner != tid:\n                raise RuntimeError('cannot release un-acquired lock')\n            assert len(self.count) > 0\n            self.count.pop()\n            if not len(self.count):\n                self.owner = None\n                if len(self.waiters) > 0:\n                    self.waiters.pop()\n                    self.wakeup.release()\n\n    def __repr__(self):\n        return f'_ModuleLock({self.name!r}) at {id(self)}'\n\n\nclass _DummyModuleLock:\n    \"\"\"A simple _ModuleLock equivalent for Python builds without\n    multi-threading support.\"\"\"\n\n    def __init__(self, name):\n        self.name = name\n        self.count = 0\n\n    def acquire(self):\n        self.count += 1\n        return True\n\n    def release(self):\n        if self.count == 0:\n            raise RuntimeError('cannot release un-acquired lock')\n        self.count -= 1\n\n    def __repr__(self):\n        return f'_DummyModuleLock({self.name!r}) at {id(self)}'\n\n\nclass _ModuleLockManager:\n\n    def __init__(self, name):\n        self._name = name\n        self._lock = None\n\n    def __enter__(self):\n        self._lock = _get_module_lock(self._name)\n        self._lock.acquire()\n\n    def __exit__(self, *args, **kwargs):\n        self._lock.release()\n\n\n# The following two functions are for consumption by Python/import.c.\n\ndef _get_module_lock(name):\n    \"\"\"Get or create the module lock for a given module name.\n\n    Acquire/release internally the global import lock to protect\n    _module_locks.\"\"\"\n\n    _imp.acquire_lock()\n    try:\n        try:\n            lock = _module_locks[name]()\n        except KeyError:\n            lock = None\n\n        if lock is None:\n            if _thread is None:\n                lock = _DummyModuleLock(name)\n            else:\n                lock = _ModuleLock(name)\n\n            def cb(ref, name=name):\n                _imp.acquire_lock()\n                try:\n                    # bpo-31070: Check if another thread created a new lock\n                    # after the previous lock was destroyed\n                    # but before the weakref callback was called.\n                    if _module_locks.get(name) is ref:\n                        del _module_locks[name]\n                finally:\n                    _imp.release_lock()\n\n            _module_locks[name] = _weakref.ref(lock, cb)\n    finally:\n        _imp.release_lock()\n\n    return lock\n\n\ndef _lock_unlock_module(name):\n    \"\"\"Acquires then releases the module lock for a given module name.\n\n    This is used to ensure a module is completely initialized, in the\n    event it is being imported by another thread.\n    \"\"\"\n    lock = _get_module_lock(name)\n    try:\n        lock.acquire()\n    except _DeadlockError:\n        # Concurrent circular import, we'll accept a partially initialized\n        # module object.\n        pass\n    else:\n        lock.release()\n\n# Frame stripping magic ###############################################\ndef _call_with_frames_removed(f, *args, **kwds):\n    \"\"\"remove_importlib_frames in import.c will always remove sequences\n    of importlib frames that end with a call to this function\n\n    Use it instead of a normal call in places where including the importlib\n    frames introduces unwanted noise into the traceback (e.g. when executing\n    module code)\n    \"\"\"\n    return f(*args, **kwds)\n\n\ndef _verbose_message(message, *args, verbosity=1):\n    \"\"\"Print the message to stderr if -v/PYTHONVERBOSE is turned on.\"\"\"\n    if sys.flags.verbose >= verbosity:\n        if not message.startswith(('#', 'import ')):\n            message = '# ' + message\n        print(message.format(*args), file=sys.stderr)\n\n\ndef _requires_builtin(fxn):\n    \"\"\"Decorator to verify the named module is built-in.\"\"\"\n    def _requires_builtin_wrapper(self, fullname):\n        if fullname not in sys.builtin_module_names:\n            raise ImportError(f'{fullname!r} is not a built-in module',\n                              name=fullname)\n        return fxn(self, fullname)\n    _wrap(_requires_builtin_wrapper, fxn)\n    return _requires_builtin_wrapper\n\n\ndef _requires_frozen(fxn):\n    \"\"\"Decorator to verify the named module is frozen.\"\"\"\n    def _requires_frozen_wrapper(self, fullname):\n        if not _imp.is_frozen(fullname):\n            raise ImportError(f'{fullname!r} is not a frozen module',\n                              name=fullname)\n        return fxn(self, fullname)\n    _wrap(_requires_frozen_wrapper, fxn)\n    return _requires_frozen_wrapper\n\n\n# Typically used by loader classes as a method replacement.\ndef _load_module_shim(self, fullname):\n    \"\"\"Load the specified module into sys.modules and return it.\n\n    This method is deprecated.  Use loader.exec_module() instead.\n\n    \"\"\"\n    msg = (\"the load_module() method is deprecated and slated for removal in \"\n           \"Python 3.15; use exec_module() instead\")\n    _warnings.warn(msg, DeprecationWarning)\n    spec = spec_from_loader(fullname, self)\n    if fullname in sys.modules:\n        module = sys.modules[fullname]\n        _exec(spec, module)\n        return sys.modules[fullname]\n    else:\n        return _load(spec)\n\n# Module specifications #######################################################\n\ndef _module_repr(module):\n    \"\"\"The implementation of ModuleType.__repr__().\"\"\"\n    loader = getattr(module, '__loader__', None)\n    if spec := getattr(module, \"__spec__\", None):\n        return _module_repr_from_spec(spec)\n    # Fall through to a catch-all which always succeeds.\n    try:\n        name = module.__name__\n    except AttributeError:\n        name = '?'\n    try:\n        filename = module.__file__\n    except AttributeError:\n        if loader is None:\n            return f'<module {name!r}>'\n        else:\n            return f'<module {name!r} ({loader!r})>'\n    else:\n        return f'<module {name!r} from {filename!r}>'\n\n\nclass ModuleSpec:\n    \"\"\"The specification for a module, used for loading.\n\n    A module's spec is the source for information about the module.  For\n    data associated with the module, including source, use the spec's\n    loader.\n\n    `name` is the absolute name of the module.  `loader` is the loader\n    to use when loading the module.  `parent` is the name of the\n    package the module is in.  The parent is derived from the name.\n\n    `is_package` determines if the module is considered a package or\n    not.  On modules this is reflected by the `__path__` attribute.\n\n    `origin` is the specific location used by the loader from which to\n    load the module, if that information is available.  When filename is\n    set, origin will match.\n\n    `has_location` indicates that a spec's \"origin\" reflects a location.\n    When this is True, `__file__` attribute of the module is set.\n\n    `cached` is the location of the cached bytecode file, if any.  It\n    corresponds to the `__cached__` attribute.\n\n    `submodule_search_locations` is the sequence of path entries to\n    search when importing submodules.  If set, is_package should be\n    True--and False otherwise.\n\n    Packages are simply modules that (may) have submodules.  If a spec\n    has a non-None value in `submodule_search_locations`, the import\n    system will consider modules loaded from the spec as packages.\n\n    Only finders (see importlib.abc.MetaPathFinder and\n    importlib.abc.PathEntryFinder) should modify ModuleSpec instances.\n\n    \"\"\"\n\n    def __init__(self, name, loader, *, origin=None, loader_state=None,\n                 is_package=None):\n        self.name = name\n        self.loader = loader\n        self.origin = origin\n        self.loader_state = loader_state\n        self.submodule_search_locations = [] if is_package else None\n        self._uninitialized_submodules = []\n\n        # file-location attributes\n        self._set_fileattr = False\n        self._cached = None\n\n    def __repr__(self):\n        args = [f'name={self.name!r}', f'loader={self.loader!r}']\n        if self.origin is not None:\n            args.append(f'origin={self.origin!r}')\n        if self.submodule_search_locations is not None:\n            args.append(f'submodule_search_locations={self.submodule_search_locations}')\n        return f'{self.__class__.__name__}({\", \".join(args)})'\n\n    def __eq__(self, other):\n        smsl = self.submodule_search_locations\n        try:\n            return (self.name == other.name and\n                    self.loader == other.loader and\n                    self.origin == other.origin and\n                    smsl == other.submodule_search_locations and\n                    self.cached == other.cached and\n                    self.has_location == other.has_location)\n        except AttributeError:\n            return NotImplemented\n\n    @property\n    def cached(self):\n        if self._cached is None:\n            if self.origin is not None and self._set_fileattr:\n                if _bootstrap_external is None:\n                    raise NotImplementedError\n                self._cached = _bootstrap_external._get_cached(self.origin)\n        return self._cached\n\n    @cached.setter\n    def cached(self, cached):\n        self._cached = cached\n\n    @property\n    def parent(self):\n        \"\"\"The name of the module's parent.\"\"\"\n        if self.submodule_search_locations is None:\n            return self.name.rpartition('.')[0]\n        else:\n            return self.name\n\n    @property\n    def has_location(self):\n        return self._set_fileattr\n\n    @has_location.setter\n    def has_location(self, value):\n        self._set_fileattr = bool(value)\n\n\ndef spec_from_loader(name, loader, *, origin=None, is_package=None):\n    \"\"\"Return a module spec based on various loader methods.\"\"\"\n    if origin is None:\n        origin = getattr(loader, '_ORIGIN', None)\n\n    if not origin and hasattr(loader, 'get_filename'):\n        if _bootstrap_external is None:\n            raise NotImplementedError\n        spec_from_file_location = _bootstrap_external.spec_from_file_location\n\n        if is_package is None:\n            return spec_from_file_location(name, loader=loader)\n        search = [] if is_package else None\n        return spec_from_file_location(name, loader=loader,\n                                       submodule_search_locations=search)\n\n    if is_package is None:\n        if hasattr(loader, 'is_package'):\n            try:\n                is_package = loader.is_package(name)\n            except ImportError:\n                is_package = None  # aka, undefined\n        else:\n            # the default\n            is_package = False\n\n    return ModuleSpec(name, loader, origin=origin, is_package=is_package)\n\n\ndef _spec_from_module(module, loader=None, origin=None):\n    # This function is meant for use in _setup().\n    try:\n        spec = module.__spec__\n    except AttributeError:\n        pass\n    else:\n        if spec is not None:\n            return spec\n\n    name = module.__name__\n    if loader is None:\n        try:\n            loader = module.__loader__\n        except AttributeError:\n            # loader will stay None.\n            pass\n    try:\n        location = module.__file__\n    except AttributeError:\n        location = None\n    if origin is None:\n        if loader is not None:\n            origin = getattr(loader, '_ORIGIN', None)\n        if not origin and location is not None:\n            origin = location\n    try:\n        cached = module.__cached__\n    except AttributeError:\n        cached = None\n    try:\n        submodule_search_locations = list(module.__path__)\n    except AttributeError:\n        submodule_search_locations = None\n\n    spec = ModuleSpec(name, loader, origin=origin)\n    spec._set_fileattr = False if location is None else (origin == location)\n    spec.cached = cached\n    spec.submodule_search_locations = submodule_search_locations\n    return spec\n\n\ndef _init_module_attrs(spec, module, *, override=False):\n    # The passed-in module may be not support attribute assignment,\n    # in which case we simply don't set the attributes.\n    # __name__\n    if (override or getattr(module, '__name__', None) is None):\n        try:\n            module.__name__ = spec.name\n        except AttributeError:\n            pass\n    # __loader__\n    if override or getattr(module, '__loader__', None) is None:\n        loader = spec.loader\n        if loader is None:\n            # A backward compatibility hack.\n            if spec.submodule_search_locations is not None:\n                if _bootstrap_external is None:\n                    raise NotImplementedError\n                NamespaceLoader = _bootstrap_external.NamespaceLoader\n\n                loader = NamespaceLoader.__new__(NamespaceLoader)\n                loader._path = spec.submodule_search_locations\n                spec.loader = loader\n                # While the docs say that module.__file__ is not set for\n                # built-in modules, and the code below will avoid setting it if\n                # spec.has_location is false, this is incorrect for namespace\n                # packages.  Namespace packages have no location, but their\n                # __spec__.origin is None, and thus their module.__file__\n                # should also be None for consistency.  While a bit of a hack,\n                # this is the best place to ensure this consistency.\n                #\n                # See # https://docs.python.org/3/library/importlib.html#importlib.abc.Loader.load_module\n                # and bpo-32305\n                module.__file__ = None\n        try:\n            module.__loader__ = loader\n        except AttributeError:\n            pass\n    # __package__\n    if override or getattr(module, '__package__', None) is None:\n        try:\n            module.__package__ = spec.parent\n        except AttributeError:\n            pass\n    # __spec__\n    try:\n        module.__spec__ = spec\n    except AttributeError:\n        pass\n    # __path__\n    if override or getattr(module, '__path__', None) is None:\n        if spec.submodule_search_locations is not None:\n            # XXX We should extend __path__ if it's already a list.\n            try:\n                module.__path__ = spec.submodule_search_locations\n            except AttributeError:\n                pass\n    # __file__/__cached__\n    if spec.has_location:\n        if override or getattr(module, '__file__', None) is None:\n            try:\n                module.__file__ = spec.origin\n            except AttributeError:\n                pass\n\n        if override or getattr(module, '__cached__', None) is None:\n            if spec.cached is not None:\n                try:\n                    module.__cached__ = spec.cached\n                except AttributeError:\n                    pass\n    return module\n\n\ndef module_from_spec(spec):\n    \"\"\"Create a module based on the provided spec.\"\"\"\n    # Typically loaders will not implement create_module().\n    module = None\n    if hasattr(spec.loader, 'create_module'):\n        # If create_module() returns `None` then it means default\n        # module creation should be used.\n        module = spec.loader.create_module(spec)\n    elif hasattr(spec.loader, 'exec_module'):\n        raise ImportError('loaders that define exec_module() '\n                          'must also define create_module()')\n    if module is None:\n        module = _new_module(spec.name)\n    _init_module_attrs(spec, module)\n    return module\n\n\ndef _module_repr_from_spec(spec):\n    \"\"\"Return the repr to use for the module.\"\"\"\n    name = '?' if spec.name is None else spec.name\n    if spec.origin is None:\n        loader = spec.loader\n        if loader is None:\n            return f'<module {name!r}>'\n        elif (\n            _bootstrap_external is not None\n            and isinstance(loader, _bootstrap_external.NamespaceLoader)\n        ):\n            return f'<module {name!r} (namespace) from {list(loader._path)}>'\n        else:\n            return f'<module {name!r} ({loader!r})>'\n    else:\n        if spec.has_location:\n            return f'<module {name!r} from {spec.origin!r}>'\n        else:\n            return f'<module {spec.name!r} ({spec.origin})>'\n\n\n# Used by importlib.reload() and _load_module_shim().\ndef _exec(spec, module):\n    \"\"\"Execute the spec's specified module in an existing module's namespace.\"\"\"\n    name = spec.name\n    with _ModuleLockManager(name):\n        if sys.modules.get(name) is not module:\n            msg = f'module {name!r} not in sys.modules'\n            raise ImportError(msg, name=name)\n        try:\n            if spec.loader is None:\n                if spec.submodule_search_locations is None:\n                    raise ImportError('missing loader', name=spec.name)\n                # Namespace package.\n                _init_module_attrs(spec, module, override=True)\n            else:\n                _init_module_attrs(spec, module, override=True)\n                if not hasattr(spec.loader, 'exec_module'):\n                    msg = (f\"{_object_name(spec.loader)}.exec_module() not found; \"\n                           \"falling back to load_module()\")\n                    _warnings.warn(msg, ImportWarning)\n                    spec.loader.load_module(name)\n                else:\n                    spec.loader.exec_module(module)\n        finally:\n            # Update the order of insertion into sys.modules for module\n            # clean-up at shutdown.\n            module = sys.modules.pop(spec.name)\n            sys.modules[spec.name] = module\n    return module\n\n\ndef _load_backward_compatible(spec):\n    # It is assumed that all callers have been warned about using load_module()\n    # appropriately before calling this function.\n    try:\n        spec.loader.load_module(spec.name)\n    except:\n        if spec.name in sys.modules:\n            module = sys.modules.pop(spec.name)\n            sys.modules[spec.name] = module\n        raise\n    # The module must be in sys.modules at this point!\n    # Move it to the end of sys.modules.\n    module = sys.modules.pop(spec.name)\n    sys.modules[spec.name] = module\n    if getattr(module, '__loader__', None) is None:\n        try:\n            module.__loader__ = spec.loader\n        except AttributeError:\n            pass\n    if getattr(module, '__package__', None) is None:\n        try:\n            # Since module.__path__ may not line up with\n            # spec.submodule_search_paths, we can't necessarily rely\n            # on spec.parent here.\n            module.__package__ = module.__name__\n            if not hasattr(module, '__path__'):\n                module.__package__ = spec.name.rpartition('.')[0]\n        except AttributeError:\n            pass\n    if getattr(module, '__spec__', None) is None:\n        try:\n            module.__spec__ = spec\n        except AttributeError:\n            pass\n    return module\n\ndef _load_unlocked(spec):\n    # A helper for direct use by the import system.\n    if spec.loader is not None:\n        # Not a namespace package.\n        if not hasattr(spec.loader, 'exec_module'):\n            msg = (f\"{_object_name(spec.loader)}.exec_module() not found; \"\n                    \"falling back to load_module()\")\n            _warnings.warn(msg, ImportWarning)\n            return _load_backward_compatible(spec)\n\n    module = module_from_spec(spec)\n\n    # This must be done before putting the module in sys.modules\n    # (otherwise an optimization shortcut in import.c becomes\n    # wrong).\n    spec._initializing = True\n    try:\n        sys.modules[spec.name] = module\n        try:\n            if spec.loader is None:\n                if spec.submodule_search_locations is None:\n                    raise ImportError('missing loader', name=spec.name)\n                # A namespace package so do nothing.\n            else:\n                spec.loader.exec_module(module)\n        except:\n            try:\n                del sys.modules[spec.name]\n            except KeyError:\n                pass\n            raise\n        # Move the module to the end of sys.modules.\n        # We don't ensure that the import-related module attributes get\n        # set in the sys.modules replacement case.  Such modules are on\n        # their own.\n        module = sys.modules.pop(spec.name)\n        sys.modules[spec.name] = module\n        _verbose_message('import {!r} # {!r}', spec.name, spec.loader)\n    finally:\n        spec._initializing = False\n\n    return module\n\n# A method used during testing of _load_unlocked() and by\n# _load_module_shim().\ndef _load(spec):\n    \"\"\"Return a new module object, loaded by the spec's loader.\n\n    The module is not added to its parent.\n\n    If a module is already in sys.modules, that existing module gets\n    clobbered.\n\n    \"\"\"\n    with _ModuleLockManager(spec.name):\n        return _load_unlocked(spec)\n\n\n# Loaders #####################################################################\n\nclass BuiltinImporter:\n\n    \"\"\"Meta path import for built-in modules.\n\n    All methods are either class or static methods to avoid the need to\n    instantiate the class.\n\n    \"\"\"\n\n    _ORIGIN = \"built-in\"\n\n    @classmethod\n    def find_spec(cls, fullname, path=None, target=None):\n        if _imp.is_builtin(fullname):\n            return spec_from_loader(fullname, cls, origin=cls._ORIGIN)\n        else:\n            return None\n\n    @staticmethod\n    def create_module(spec):\n        \"\"\"Create a built-in module\"\"\"\n        if spec.name not in sys.builtin_module_names:\n            raise ImportError(f'{spec.name!r} is not a built-in module',\n                              name=spec.name)\n        return _call_with_frames_removed(_imp.create_builtin, spec)\n\n    @staticmethod\n    def exec_module(module):\n        \"\"\"Exec a built-in module\"\"\"\n        _call_with_frames_removed(_imp.exec_builtin, module)\n\n    @classmethod\n    @_requires_builtin\n    def get_code(cls, fullname):\n        \"\"\"Return None as built-in modules do not have code objects.\"\"\"\n        return None\n\n    @classmethod\n    @_requires_builtin\n    def get_source(cls, fullname):\n        \"\"\"Return None as built-in modules do not have source code.\"\"\"\n        return None\n\n    @classmethod\n    @_requires_builtin\n    def is_package(cls, fullname):\n        \"\"\"Return False as built-in modules are never packages.\"\"\"\n        return False\n\n    load_module = classmethod(_load_module_shim)\n\n\nclass FrozenImporter:\n\n    \"\"\"Meta path import for frozen modules.\n\n    All methods are either class or static methods to avoid the need to\n    instantiate the class.\n\n    \"\"\"\n\n    _ORIGIN = \"frozen\"\n\n    @classmethod\n    def _fix_up_module(cls, module):\n        spec = module.__spec__\n        state = spec.loader_state\n        if state is None:\n            # The module is missing FrozenImporter-specific values.\n\n            # Fix up the spec attrs.\n            origname = vars(module).pop('__origname__', None)\n            assert origname, 'see PyImport_ImportFrozenModuleObject()'\n            ispkg = hasattr(module, '__path__')\n            assert _imp.is_frozen_package(module.__name__) == ispkg, ispkg\n            filename, pkgdir = cls._resolve_filename(origname, spec.name, ispkg)\n            spec.loader_state = type(sys.implementation)(\n                filename=filename,\n                origname=origname,\n            )\n            __path__ = spec.submodule_search_locations\n            if ispkg:\n                assert __path__ == [], __path__\n                if pkgdir:\n                    spec.submodule_search_locations.insert(0, pkgdir)\n            else:\n                assert __path__ is None, __path__\n\n            # Fix up the module attrs (the bare minimum).\n            assert not hasattr(module, '__file__'), module.__file__\n            if filename:\n                try:\n                    module.__file__ = filename\n                except AttributeError:\n                    pass\n            if ispkg:\n                if module.__path__ != __path__:\n                    assert module.__path__ == [], module.__path__\n                    module.__path__.extend(__path__)\n        else:\n            # These checks ensure that _fix_up_module() is only called\n            # in the right places.\n            __path__ = spec.submodule_search_locations\n            ispkg = __path__ is not None\n            # Check the loader state.\n            assert sorted(vars(state)) == ['filename', 'origname'], state\n            if state.origname:\n                # The only frozen modules with \"origname\" set are stdlib modules.\n                (__file__, pkgdir,\n                 ) = cls._resolve_filename(state.origname, spec.name, ispkg)\n                assert state.filename == __file__, (state.filename, __file__)\n                if pkgdir:\n                    assert __path__ == [pkgdir], (__path__, pkgdir)\n                else:\n                    assert __path__ == ([] if ispkg else None), __path__\n            else:\n                __file__ = None\n                assert state.filename is None, state.filename\n                assert __path__ == ([] if ispkg else None), __path__\n            # Check the file attrs.\n            if __file__:\n                assert hasattr(module, '__file__')\n                assert module.__file__ == __file__, (module.__file__, __file__)\n            else:\n                assert not hasattr(module, '__file__'), module.__file__\n            if ispkg:\n                assert hasattr(module, '__path__')\n                assert module.__path__ == __path__, (module.__path__, __path__)\n            else:\n                assert not hasattr(module, '__path__'), module.__path__\n        assert not spec.has_location\n\n    @classmethod\n    def _resolve_filename(cls, fullname, alias=None, ispkg=False):\n        if not fullname or not getattr(sys, '_stdlib_dir', None):\n            return None, None\n        try:\n            sep = cls._SEP\n        except AttributeError:\n            sep = cls._SEP = '\\\\' if sys.platform == 'win32' else '/'\n\n        if fullname != alias:\n            if fullname.startswith('<'):\n                fullname = fullname[1:]\n                if not ispkg:\n                    fullname = f'{fullname}.__init__'\n            else:\n                ispkg = False\n        relfile = fullname.replace('.', sep)\n        if ispkg:\n            pkgdir = f'{sys._stdlib_dir}{sep}{relfile}'\n            filename = f'{pkgdir}{sep}__init__.py'\n        else:\n            pkgdir = None\n            filename = f'{sys._stdlib_dir}{sep}{relfile}.py'\n        return filename, pkgdir\n\n    @classmethod\n    def find_spec(cls, fullname, path=None, target=None):\n        info = _call_with_frames_removed(_imp.find_frozen, fullname)\n        if info is None:\n            return None\n        # We get the marshaled data in exec_module() (the loader\n        # part of the importer), instead of here (the finder part).\n        # The loader is the usual place to get the data that will\n        # be loaded into the module.  (For example, see _LoaderBasics\n        # in _bootstra_external.py.)  Most importantly, this importer\n        # is simpler if we wait to get the data.\n        # However, getting as much data in the finder as possible\n        # to later load the module is okay, and sometimes important.\n        # (That's why ModuleSpec.loader_state exists.)  This is\n        # especially true if it avoids throwing away expensive data\n        # the loader would otherwise duplicate later and can be done\n        # efficiently.  In this case it isn't worth it.\n        _, ispkg, origname = info\n        spec = spec_from_loader(fullname, cls,\n                                origin=cls._ORIGIN,\n                                is_package=ispkg)\n        filename, pkgdir = cls._resolve_filename(origname, fullname, ispkg)\n        spec.loader_state = type(sys.implementation)(\n            filename=filename,\n            origname=origname,\n        )\n        if pkgdir:\n            spec.submodule_search_locations.insert(0, pkgdir)\n        return spec\n\n    @staticmethod\n    def create_module(spec):\n        \"\"\"Set __file__, if able.\"\"\"\n        module = _new_module(spec.name)\n        try:\n            filename = spec.loader_state.filename\n        except AttributeError:\n            pass\n        else:\n            if filename:\n                module.__file__ = filename\n        return module\n\n    @staticmethod\n    def exec_module(module):\n        spec = module.__spec__\n        name = spec.name\n        code = _call_with_frames_removed(_imp.get_frozen_object, name)\n        exec(code, module.__dict__)\n\n    @classmethod\n    def load_module(cls, fullname):\n        \"\"\"Load a frozen module.\n\n        This method is deprecated.  Use exec_module() instead.\n\n        \"\"\"\n        # Warning about deprecation implemented in _load_module_shim().\n        module = _load_module_shim(cls, fullname)\n        info = _imp.find_frozen(fullname)\n        assert info is not None\n        _, ispkg, origname = info\n        module.__origname__ = origname\n        vars(module).pop('__file__', None)\n        if ispkg:\n            module.__path__ = []\n        cls._fix_up_module(module)\n        return module\n\n    @classmethod\n    @_requires_frozen\n    def get_code(cls, fullname):\n        \"\"\"Return the code object for the frozen module.\"\"\"\n        return _imp.get_frozen_object(fullname)\n\n    @classmethod\n    @_requires_frozen\n    def get_source(cls, fullname):\n        \"\"\"Return None as frozen modules do not have source code.\"\"\"\n        return None\n\n    @classmethod\n    @_requires_frozen\n    def is_package(cls, fullname):\n        \"\"\"Return True if the frozen module is a package.\"\"\"\n        return _imp.is_frozen_package(fullname)\n\n\n# Import itself ###############################################################\n\nclass _ImportLockContext:\n\n    \"\"\"Context manager for the import lock.\"\"\"\n\n    def __enter__(self):\n        \"\"\"Acquire the import lock.\"\"\"\n        _imp.acquire_lock()\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        \"\"\"Release the import lock regardless of any raised exceptions.\"\"\"\n        _imp.release_lock()\n\n\ndef _resolve_name(name, package, level):\n    \"\"\"Resolve a relative module name to an absolute one.\"\"\"\n    bits = package.rsplit('.', level - 1)\n    if len(bits) < level:\n        raise ImportError('attempted relative import beyond top-level package')\n    base = bits[0]\n    return f'{base}.{name}' if name else base\n\n\ndef _find_spec(name, path, target=None):\n    \"\"\"Find a module's spec.\"\"\"\n    meta_path = sys.meta_path\n    if meta_path is None:\n        # PyImport_Cleanup() is running or has been called.\n        raise ImportError(\"sys.meta_path is None, Python is likely \"\n                          \"shutting down\")\n\n    if not meta_path:\n        _warnings.warn('sys.meta_path is empty', ImportWarning)\n\n    # We check sys.modules here for the reload case.  While a passed-in\n    # target will usually indicate a reload there is no guarantee, whereas\n    # sys.modules provides one.\n    is_reload = name in sys.modules\n    for finder in meta_path:\n        with _ImportLockContext():\n            try:\n                find_spec = finder.find_spec\n            except AttributeError:\n                continue\n            else:\n                spec = find_spec(name, path, target)\n        if spec is not None:\n            # The parent import may have already imported this module.\n            if not is_reload and name in sys.modules:\n                module = sys.modules[name]\n                try:\n                    __spec__ = module.__spec__\n                except AttributeError:\n                    # We use the found spec since that is the one that\n                    # we would have used if the parent module hadn't\n                    # beaten us to the punch.\n                    return spec\n                else:\n                    if __spec__ is None:\n                        return spec\n                    else:\n                        return __spec__\n            else:\n                return spec\n    else:\n        return None\n\n\ndef _sanity_check(name, package, level):\n    \"\"\"Verify arguments are \"sane\".\"\"\"\n    if not isinstance(name, str):\n        raise TypeError(f'module name must be str, not {type(name)}')\n    if level < 0:\n        raise ValueError('level must be >= 0')\n    if level > 0:\n        if not isinstance(package, str):\n            raise TypeError('__package__ not set to a string')\n        elif not package:\n            raise ImportError('attempted relative import with no known parent '\n                              'package')\n    if not name and level == 0:\n        raise ValueError('Empty module name')\n\n\n_ERR_MSG_PREFIX = 'No module named '\n_ERR_MSG = _ERR_MSG_PREFIX + '{!r}'\n\ndef _find_and_load_unlocked(name, import_):\n    path = None\n    parent = name.rpartition('.')[0]\n    parent_spec = None\n    if parent:\n        if parent not in sys.modules:\n            _call_with_frames_removed(import_, parent)\n        # Crazy side-effects!\n        if name in sys.modules:\n            return sys.modules[name]\n        parent_module = sys.modules[parent]\n        try:\n            path = parent_module.__path__\n        except AttributeError:\n            msg = f'{_ERR_MSG_PREFIX}{name!r}; {parent!r} is not a package'\n            raise ModuleNotFoundError(msg, name=name) from None\n        parent_spec = parent_module.__spec__\n        child = name.rpartition('.')[2]\n    spec = _find_spec(name, path)\n    if spec is None:\n        raise ModuleNotFoundError(f'{_ERR_MSG_PREFIX}{name!r}', name=name)\n    else:\n        if parent_spec:\n            # Temporarily add child we are currently importing to parent's\n            # _uninitialized_submodules for circular import tracking.\n            parent_spec._uninitialized_submodules.append(child)\n        try:\n            module = _load_unlocked(spec)\n        finally:\n            if parent_spec:\n                parent_spec._uninitialized_submodules.pop()\n    if parent:\n        # Set the module as an attribute on its parent.\n        parent_module = sys.modules[parent]\n        try:\n            setattr(parent_module, child, module)\n        except AttributeError:\n            msg = f\"Cannot set an attribute on {parent!r} for child module {child!r}\"\n            _warnings.warn(msg, ImportWarning)\n    return module\n\n\n_NEEDS_LOADING = object()\n\n\ndef _find_and_load(name, import_):\n    \"\"\"Find and load the module.\"\"\"\n\n    # Optimization: we avoid unneeded module locking if the module\n    # already exists in sys.modules and is fully initialized.\n    module = sys.modules.get(name, _NEEDS_LOADING)\n    if (module is _NEEDS_LOADING or\n        getattr(getattr(module, \"__spec__\", None), \"_initializing\", False)):\n        with _ModuleLockManager(name):\n            module = sys.modules.get(name, _NEEDS_LOADING)\n            if module is _NEEDS_LOADING:\n                return _find_and_load_unlocked(name, import_)\n\n        # Optimization: only call _bootstrap._lock_unlock_module() if\n        # module.__spec__._initializing is True.\n        # NOTE: because of this, initializing must be set *before*\n        # putting the new module in sys.modules.\n        _lock_unlock_module(name)\n\n    if module is None:\n        message = f'import of {name} halted; None in sys.modules'\n        raise ModuleNotFoundError(message, name=name)\n\n    return module\n\n\ndef _gcd_import(name, package=None, level=0):\n    \"\"\"Import and return the module based on its name, the package the call is\n    being made from, and the level adjustment.\n\n    This function represents the greatest common denominator of functionality\n    between import_module and __import__. This includes setting __package__ if\n    the loader did not.\n\n    \"\"\"\n    _sanity_check(name, package, level)\n    if level > 0:\n        name = _resolve_name(name, package, level)\n    return _find_and_load(name, _gcd_import)\n\n\ndef _handle_fromlist(module, fromlist, import_, *, recursive=False):\n    \"\"\"Figure out what __import__ should return.\n\n    The import_ parameter is a callable which takes the name of module to\n    import. It is required to decouple the function from assuming importlib's\n    import implementation is desired.\n\n    \"\"\"\n    # The hell that is fromlist ...\n    # If a package was imported, try to import stuff from fromlist.\n    for x in fromlist:\n        if not isinstance(x, str):\n            if recursive:\n                where = module.__name__ + '.__all__'\n            else:\n                where = \"``from list''\"\n            raise TypeError(f\"Item in {where} must be str, \"\n                            f\"not {type(x).__name__}\")\n        elif x == '*':\n            if not recursive and hasattr(module, '__all__'):\n                _handle_fromlist(module, module.__all__, import_,\n                                 recursive=True)\n        elif not hasattr(module, x):\n            from_name = f'{module.__name__}.{x}'\n            try:\n                _call_with_frames_removed(import_, from_name)\n            except ModuleNotFoundError as exc:\n                # Backwards-compatibility dictates we ignore failed\n                # imports triggered by fromlist for modules that don't\n                # exist.\n                if (exc.name == from_name and\n                    sys.modules.get(from_name, _NEEDS_LOADING) is not None):\n                    continue\n                raise\n    return module\n\n\ndef _calc___package__(globals):\n    \"\"\"Calculate what __package__ should be.\n\n    __package__ is not guaranteed to be defined or could be set to None\n    to represent that its proper value is unknown.\n\n    \"\"\"\n    package = globals.get('__package__')\n    spec = globals.get('__spec__')\n    if package is not None:\n        if spec is not None and package != spec.parent:\n            _warnings.warn(\"__package__ != __spec__.parent \"\n                           f\"({package!r} != {spec.parent!r})\",\n                           DeprecationWarning, stacklevel=3)\n        return package\n    elif spec is not None:\n        return spec.parent\n    else:\n        _warnings.warn(\"can't resolve package from __spec__ or __package__, \"\n                       \"falling back on __name__ and __path__\",\n                       ImportWarning, stacklevel=3)\n        package = globals['__name__']\n        if '__path__' not in globals:\n            package = package.rpartition('.')[0]\n    return package\n\n\ndef __import__(name, globals=None, locals=None, fromlist=(), level=0):\n    \"\"\"Import a module.\n\n    The 'globals' argument is used to infer where the import is occurring from\n    to handle relative imports. The 'locals' argument is ignored. The\n    'fromlist' argument specifies what should exist as attributes on the module\n    being imported (e.g. ``from module import <fromlist>``).  The 'level'\n    argument represents the package location to import from in a relative\n    import (e.g. ``from ..pkg import mod`` would have a 'level' of 2).\n\n    \"\"\"\n    if level == 0:\n        module = _gcd_import(name)\n    else:\n        globals_ = globals if globals is not None else {}\n        package = _calc___package__(globals_)\n        module = _gcd_import(name, package, level)\n    if not fromlist:\n        # Return up to the first dot in 'name'. This is complicated by the fact\n        # that 'name' may be relative.\n        if level == 0:\n            return _gcd_import(name.partition('.')[0])\n        elif not name:\n            return module\n        else:\n            # Figure out where to slice the module's name up to the first dot\n            # in 'name'.\n            cut_off = len(name) - len(name.partition('.')[0])\n            # Slice end needs to be positive to alleviate need to special-case\n            # when ``'.' not in name``.\n            return sys.modules[module.__name__[:len(module.__name__)-cut_off]]\n    elif hasattr(module, '__path__'):\n        return _handle_fromlist(module, fromlist, _gcd_import)\n    else:\n        return module\n\n\ndef _builtin_from_name(name):\n    spec = BuiltinImporter.find_spec(name)\n    if spec is None:\n        raise ImportError('no built-in module named ' + name)\n    return _load_unlocked(spec)\n\n\ndef _setup(sys_module, _imp_module):\n    \"\"\"Setup importlib by importing needed built-in modules and injecting them\n    into the global namespace.\n\n    As sys is needed for sys.modules access and _imp is needed to load built-in\n    modules, those two modules must be explicitly passed in.\n\n    \"\"\"\n    global _imp, sys, _blocking_on\n    _imp = _imp_module\n    sys = sys_module\n\n    # Set up the spec for existing builtin/frozen modules.\n    module_type = type(sys)\n    for name, module in sys.modules.items():\n        if isinstance(module, module_type):\n            if name in sys.builtin_module_names:\n                loader = BuiltinImporter\n            elif _imp.is_frozen(name):\n                loader = FrozenImporter\n            else:\n                continue\n            spec = _spec_from_module(module, loader)\n            _init_module_attrs(spec, module)\n            if loader is FrozenImporter:\n                loader._fix_up_module(module)\n\n    # Directly load built-in modules needed during bootstrap.\n    self_module = sys.modules[__name__]\n    for builtin_name in ('_thread', '_warnings', '_weakref'):\n        if builtin_name not in sys.modules:\n            builtin_module = _builtin_from_name(builtin_name)\n        else:\n            builtin_module = sys.modules[builtin_name]\n        setattr(self_module, builtin_name, builtin_module)\n\n    # Instantiation requires _weakref to have been set.\n    _blocking_on = _WeakValueDictionary()\n\n\ndef _install(sys_module, _imp_module):\n    \"\"\"Install importers for builtin and frozen modules\"\"\"\n    _setup(sys_module, _imp_module)\n\n    sys.meta_path.append(BuiltinImporter)\n    sys.meta_path.append(FrozenImporter)\n\n\ndef _install_external_importers():\n    \"\"\"Install importers that require external filesystem access\"\"\"\n    global _bootstrap_external\n    import _frozen_importlib_external\n    _bootstrap_external = _frozen_importlib_external\n    _frozen_importlib_external._install(sys.modules[__name__])\n", 1551], "/root/miniconda3/envs/gs-lightning/lib/python3.12/site-packages/_distutils_hack/__init__.py": ["# don't import any costly modules\nimport os\nimport sys\n\nreport_url = (\n    \"https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\"\n)\n\n\ndef warn_distutils_present():\n    if 'distutils' not in sys.modules:\n        return\n    import warnings\n\n    warnings.warn(\n        \"Distutils was imported before Setuptools, but importing Setuptools \"\n        \"also replaces the `distutils` module in `sys.modules`. This may lead \"\n        \"to undesirable behaviors or errors. To avoid these issues, avoid \"\n        \"using distutils directly, ensure that setuptools is installed in the \"\n        \"traditional way (e.g. not an editable install), and/or make sure \"\n        \"that setuptools is always imported before distutils.\"\n    )\n\n\ndef clear_distutils():\n    if 'distutils' not in sys.modules:\n        return\n    import warnings\n\n    warnings.warn(\n        \"Setuptools is replacing distutils. Support for replacing \"\n        \"an already imported distutils is deprecated. In the future, \"\n        \"this condition will fail. \"\n        f\"Register concerns at {report_url}\"\n    )\n    mods = [\n        name\n        for name in sys.modules\n        if name == \"distutils\" or name.startswith(\"distutils.\")\n    ]\n    for name in mods:\n        del sys.modules[name]\n\n\ndef enabled():\n    \"\"\"\n    Allow selection of distutils by environment variable.\n    \"\"\"\n    which = os.environ.get('SETUPTOOLS_USE_DISTUTILS', 'local')\n    if which == 'stdlib':\n        import warnings\n\n        warnings.warn(\n            \"Reliance on distutils from stdlib is deprecated. Users \"\n            \"must rely on setuptools to provide the distutils module. \"\n            \"Avoid importing distutils or import setuptools first, \"\n            \"and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. \"\n            f\"Register concerns at {report_url}\"\n        )\n    return which == 'local'\n\n\ndef ensure_local_distutils():\n    import importlib\n\n    clear_distutils()\n\n    # With the DistutilsMetaFinder in place,\n    # perform an import to cause distutils to be\n    # loaded from setuptools._distutils. Ref #2906.\n    with shim():\n        importlib.import_module('distutils')\n\n    # check that submodules load as expected\n    core = importlib.import_module('distutils.core')\n    assert '_distutils' in core.__file__, core.__file__\n    assert 'setuptools._distutils.log' not in sys.modules\n\n\ndef do_override():\n    \"\"\"\n    Ensure that the local copy of distutils is preferred over stdlib.\n\n    See https://github.com/pypa/setuptools/issues/417#issuecomment-392298401\n    for more motivation.\n    \"\"\"\n    if enabled():\n        warn_distutils_present()\n        ensure_local_distutils()\n\n\nclass _TrivialRe:\n    def __init__(self, *patterns) -> None:\n        self._patterns = patterns\n\n    def match(self, string):\n        return all(pat in string for pat in self._patterns)\n\n\nclass DistutilsMetaFinder:\n    def find_spec(self, fullname, path, target=None):\n        # optimization: only consider top level modules and those\n        # found in the CPython test suite.\n        if path is not None and not fullname.startswith('test.'):\n            return None\n\n        method_name = 'spec_for_{fullname}'.format(**locals())\n        method = getattr(self, method_name, lambda: None)\n        return method()\n\n    def spec_for_distutils(self):\n        if self.is_cpython():\n            return None\n\n        import importlib\n        import importlib.abc\n        import importlib.util\n\n        try:\n            mod = importlib.import_module('setuptools._distutils')\n        except Exception:\n            # There are a couple of cases where setuptools._distutils\n            # may not be present:\n            # - An older Setuptools without a local distutils is\n            #   taking precedence. Ref #2957.\n            # - Path manipulation during sitecustomize removes\n            #   setuptools from the path but only after the hook\n            #   has been loaded. Ref #2980.\n            # In either case, fall back to stdlib behavior.\n            return None\n\n        class DistutilsLoader(importlib.abc.Loader):\n            def create_module(self, spec):\n                mod.__name__ = 'distutils'\n                return mod\n\n            def exec_module(self, module):\n                pass\n\n        return importlib.util.spec_from_loader(\n            'distutils', DistutilsLoader(), origin=mod.__file__\n        )\n\n    @staticmethod\n    def is_cpython():\n        \"\"\"\n        Suppress supplying distutils for CPython (build and tests).\n        Ref #2965 and #3007.\n        \"\"\"\n        return os.path.isfile('pybuilddir.txt')\n\n    def spec_for_pip(self):\n        \"\"\"\n        Ensure stdlib distutils when running under pip.\n        See pypa/pip#8761 for rationale.\n        \"\"\"\n        if sys.version_info >= (3, 12) or self.pip_imported_during_build():\n            return\n        clear_distutils()\n        self.spec_for_distutils = lambda: None\n\n    @classmethod\n    def pip_imported_during_build(cls):\n        \"\"\"\n        Detect if pip is being imported in a build script. Ref #2355.\n        \"\"\"\n        import traceback\n\n        return any(\n            cls.frame_file_is_setup(frame) for frame, line in traceback.walk_stack(None)\n        )\n\n    @staticmethod\n    def frame_file_is_setup(frame):\n        \"\"\"\n        Return True if the indicated frame suggests a setup.py file.\n        \"\"\"\n        # some frames may not have __file__ (#2940)\n        return frame.f_globals.get('__file__', '').endswith('setup.py')\n\n    def spec_for_sensitive_tests(self):\n        \"\"\"\n        Ensure stdlib distutils when running select tests under CPython.\n\n        python/cpython#91169\n        \"\"\"\n        clear_distutils()\n        self.spec_for_distutils = lambda: None\n\n    sensitive_tests = (\n        [\n            'test.test_distutils',\n            'test.test_peg_generator',\n            'test.test_importlib',\n        ]\n        if sys.version_info < (3, 10)\n        else [\n            'test.test_distutils',\n        ]\n    )\n\n\nfor name in DistutilsMetaFinder.sensitive_tests:\n    setattr(\n        DistutilsMetaFinder,\n        f'spec_for_{name}',\n        DistutilsMetaFinder.spec_for_sensitive_tests,\n    )\n\n\nDISTUTILS_FINDER = DistutilsMetaFinder()\n\n\ndef add_shim():\n    DISTUTILS_FINDER in sys.meta_path or insert_shim()\n\n\nclass shim:\n    def __enter__(self) -> None:\n        insert_shim()\n\n    def __exit__(self, exc: object, value: object, tb: object) -> None:\n        _remove_shim()\n\n\ndef insert_shim():\n    sys.meta_path.insert(0, DISTUTILS_FINDER)\n\n\ndef _remove_shim():\n    try:\n        sys.meta_path.remove(DISTUTILS_FINDER)\n    except ValueError:\n        pass\n\n\nif sys.version_info < (3, 12):\n    # DistutilsMetaFinder can only be disabled in Python < 3.12 (PEP 632)\n    remove_shim = _remove_shim\n", 239], "<frozen importlib._bootstrap_external>": ["\"\"\"Core implementation of path-based import.\n\nThis module is NOT meant to be directly imported! It has been designed such\nthat it can be bootstrapped into Python as the implementation of import. As\nsuch it requires the injection of specific modules and attributes in order to\nwork. One should use importlib as the public-facing version of this module.\n\n\"\"\"\n# IMPORTANT: Whenever making changes to this module, be sure to run a top-level\n# `make regen-importlib` followed by `make` in order to get the frozen version\n# of the module updated. Not doing so will result in the Makefile to fail for\n# all others who don't have a ./python around to freeze the module in the early\n# stages of compilation.\n#\n\n# See importlib._setup() for what is injected into the global namespace.\n\n# When editing this code be aware that code executed at import time CANNOT\n# reference any injected objects! This includes not only global code but also\n# anything specified at the class level.\n\n# Module injected manually by _set_bootstrap_module()\n_bootstrap = None\n\n# Import builtin modules\nimport _imp\nimport _io\nimport sys\nimport _warnings\nimport marshal\n\n\n_MS_WINDOWS = (sys.platform == 'win32')\nif _MS_WINDOWS:\n    import nt as _os\n    import winreg\nelse:\n    import posix as _os\n\n\nif _MS_WINDOWS:\n    path_separators = ['\\\\', '/']\nelse:\n    path_separators = ['/']\n# Assumption made in _path_join()\nassert all(len(sep) == 1 for sep in path_separators)\npath_sep = path_separators[0]\npath_sep_tuple = tuple(path_separators)\npath_separators = ''.join(path_separators)\n_pathseps_with_colon = {f':{s}' for s in path_separators}\n\n\n# Bootstrap-related code ######################################################\n_CASE_INSENSITIVE_PLATFORMS_STR_KEY = 'win',\n_CASE_INSENSITIVE_PLATFORMS_BYTES_KEY = 'cygwin', 'darwin'\n_CASE_INSENSITIVE_PLATFORMS =  (_CASE_INSENSITIVE_PLATFORMS_BYTES_KEY\n                                + _CASE_INSENSITIVE_PLATFORMS_STR_KEY)\n\n\ndef _make_relax_case():\n    if sys.platform.startswith(_CASE_INSENSITIVE_PLATFORMS):\n        if sys.platform.startswith(_CASE_INSENSITIVE_PLATFORMS_STR_KEY):\n            key = 'PYTHONCASEOK'\n        else:\n            key = b'PYTHONCASEOK'\n\n        def _relax_case():\n            \"\"\"True if filenames must be checked case-insensitively and ignore environment flags are not set.\"\"\"\n            return not sys.flags.ignore_environment and key in _os.environ\n    else:\n        def _relax_case():\n            \"\"\"True if filenames must be checked case-insensitively.\"\"\"\n            return False\n    return _relax_case\n\n_relax_case = _make_relax_case()\n\n\ndef _pack_uint32(x):\n    \"\"\"Convert a 32-bit integer to little-endian.\"\"\"\n    return (int(x) & 0xFFFFFFFF).to_bytes(4, 'little')\n\n\ndef _unpack_uint32(data):\n    \"\"\"Convert 4 bytes in little-endian to an integer.\"\"\"\n    assert len(data) == 4\n    return int.from_bytes(data, 'little')\n\ndef _unpack_uint16(data):\n    \"\"\"Convert 2 bytes in little-endian to an integer.\"\"\"\n    assert len(data) == 2\n    return int.from_bytes(data, 'little')\n\n\nif _MS_WINDOWS:\n    def _path_join(*path_parts):\n        \"\"\"Replacement for os.path.join().\"\"\"\n        if not path_parts:\n            return \"\"\n        if len(path_parts) == 1:\n            return path_parts[0]\n        root = \"\"\n        path = []\n        for new_root, tail in map(_os._path_splitroot, path_parts):\n            if new_root.startswith(path_sep_tuple) or new_root.endswith(path_sep_tuple):\n                root = new_root.rstrip(path_separators) or root\n                path = [path_sep + tail]\n            elif new_root.endswith(':'):\n                if root.casefold() != new_root.casefold():\n                    # Drive relative paths have to be resolved by the OS, so we reset the\n                    # tail but do not add a path_sep prefix.\n                    root = new_root\n                    path = [tail]\n                else:\n                    path.append(tail)\n            else:\n                root = new_root or root\n                path.append(tail)\n        path = [p.rstrip(path_separators) for p in path if p]\n        if len(path) == 1 and not path[0]:\n            # Avoid losing the root's trailing separator when joining with nothing\n            return root + path_sep\n        return root + path_sep.join(path)\n\nelse:\n    def _path_join(*path_parts):\n        \"\"\"Replacement for os.path.join().\"\"\"\n        return path_sep.join([part.rstrip(path_separators)\n                              for part in path_parts if part])\n\n\ndef _path_split(path):\n    \"\"\"Replacement for os.path.split().\"\"\"\n    i = max(path.rfind(p) for p in path_separators)\n    if i < 0:\n        return '', path\n    return path[:i], path[i + 1:]\n\n\ndef _path_stat(path):\n    \"\"\"Stat the path.\n\n    Made a separate function to make it easier to override in experiments\n    (e.g. cache stat results).\n\n    \"\"\"\n    return _os.stat(path)\n\n\ndef _path_is_mode_type(path, mode):\n    \"\"\"Test whether the path is the specified mode type.\"\"\"\n    try:\n        stat_info = _path_stat(path)\n    except OSError:\n        return False\n    return (stat_info.st_mode & 0o170000) == mode\n\n\ndef _path_isfile(path):\n    \"\"\"Replacement for os.path.isfile.\"\"\"\n    return _path_is_mode_type(path, 0o100000)\n\n\ndef _path_isdir(path):\n    \"\"\"Replacement for os.path.isdir.\"\"\"\n    if not path:\n        path = _os.getcwd()\n    return _path_is_mode_type(path, 0o040000)\n\n\nif _MS_WINDOWS:\n    def _path_isabs(path):\n        \"\"\"Replacement for os.path.isabs.\"\"\"\n        if not path:\n            return False\n        root = _os._path_splitroot(path)[0].replace('/', '\\\\')\n        return len(root) > 1 and (root.startswith('\\\\\\\\') or root.endswith('\\\\'))\n\nelse:\n    def _path_isabs(path):\n        \"\"\"Replacement for os.path.isabs.\"\"\"\n        return path.startswith(path_separators)\n\n\ndef _path_abspath(path):\n    \"\"\"Replacement for os.path.abspath.\"\"\"\n    if not _path_isabs(path):\n        for sep in path_separators:\n            path = path.removeprefix(f\".{sep}\")\n        return _path_join(_os.getcwd(), path)\n    else:\n        return path\n\n\ndef _write_atomic(path, data, mode=0o666):\n    \"\"\"Best-effort function to write data to a path atomically.\n    Be prepared to handle a FileExistsError if concurrent writing of the\n    temporary file is attempted.\"\"\"\n    # id() is used to generate a pseudo-random filename.\n    path_tmp = f'{path}.{id(path)}'\n    fd = _os.open(path_tmp,\n                  _os.O_EXCL | _os.O_CREAT | _os.O_WRONLY, mode & 0o666)\n    try:\n        # We first write data to a temporary file, and then use os.replace() to\n        # perform an atomic rename.\n        with _io.FileIO(fd, 'wb') as file:\n            bytes_written = file.write(data)\n        if bytes_written != len(data):\n            # Raise an OSError so the 'except' below cleans up the partially\n            # written file.\n            raise OSError(\"os.write() didn't write the full pyc file\")\n        _os.replace(path_tmp, path)\n    except OSError:\n        try:\n            _os.unlink(path_tmp)\n        except OSError:\n            pass\n        raise\n\n\n_code_type = type(_write_atomic.__code__)\n\n\n# Finder/loader utility code ###############################################\n\n# Magic word to reject .pyc files generated by other Python versions.\n# It should change for each incompatible change to the bytecode.\n#\n# The value of CR and LF is incorporated so if you ever read or write\n# a .pyc file in text mode the magic number will be wrong; also, the\n# Apple MPW compiler swaps their values, botching string constants.\n#\n# There were a variety of old schemes for setting the magic number.\n# The current working scheme is to increment the previous value by\n# 10.\n#\n# Starting with the adoption of PEP 3147 in Python 3.2, every bump in magic\n# number also includes a new \"magic tag\", i.e. a human readable string used\n# to represent the magic number in __pycache__ directories.  When you change\n# the magic number, you must also set a new unique magic tag.  Generally this\n# can be named after the Python major version of the magic number bump, but\n# it can really be anything, as long as it's different than anything else\n# that's come before.  The tags are included in the following table, starting\n# with Python 3.2a0.\n#\n# Known values:\n#  Python 1.5:   20121\n#  Python 1.5.1: 20121\n#     Python 1.5.2: 20121\n#     Python 1.6:   50428\n#     Python 2.0:   50823\n#     Python 2.0.1: 50823\n#     Python 2.1:   60202\n#     Python 2.1.1: 60202\n#     Python 2.1.2: 60202\n#     Python 2.2:   60717\n#     Python 2.3a0: 62011\n#     Python 2.3a0: 62021\n#     Python 2.3a0: 62011 (!)\n#     Python 2.4a0: 62041\n#     Python 2.4a3: 62051\n#     Python 2.4b1: 62061\n#     Python 2.5a0: 62071\n#     Python 2.5a0: 62081 (ast-branch)\n#     Python 2.5a0: 62091 (with)\n#     Python 2.5a0: 62092 (changed WITH_CLEANUP opcode)\n#     Python 2.5b3: 62101 (fix wrong code: for x, in ...)\n#     Python 2.5b3: 62111 (fix wrong code: x += yield)\n#     Python 2.5c1: 62121 (fix wrong lnotab with for loops and\n#                          storing constants that should have been removed)\n#     Python 2.5c2: 62131 (fix wrong code: for x, in ... in listcomp/genexp)\n#     Python 2.6a0: 62151 (peephole optimizations and STORE_MAP opcode)\n#     Python 2.6a1: 62161 (WITH_CLEANUP optimization)\n#     Python 2.7a0: 62171 (optimize list comprehensions/change LIST_APPEND)\n#     Python 2.7a0: 62181 (optimize conditional branches:\n#                          introduce POP_JUMP_IF_FALSE and POP_JUMP_IF_TRUE)\n#     Python 2.7a0  62191 (introduce SETUP_WITH)\n#     Python 2.7a0  62201 (introduce BUILD_SET)\n#     Python 2.7a0  62211 (introduce MAP_ADD and SET_ADD)\n#     Python 3000:   3000\n#                    3010 (removed UNARY_CONVERT)\n#                    3020 (added BUILD_SET)\n#                    3030 (added keyword-only parameters)\n#                    3040 (added signature annotations)\n#                    3050 (print becomes a function)\n#                    3060 (PEP 3115 metaclass syntax)\n#                    3061 (string literals become unicode)\n#                    3071 (PEP 3109 raise changes)\n#                    3081 (PEP 3137 make __file__ and __name__ unicode)\n#                    3091 (kill str8 interning)\n#                    3101 (merge from 2.6a0, see 62151)\n#                    3103 (__file__ points to source file)\n#     Python 3.0a4: 3111 (WITH_CLEANUP optimization).\n#     Python 3.0b1: 3131 (lexical exception stacking, including POP_EXCEPT\n                          #3021)\n#     Python 3.1a1: 3141 (optimize list, set and dict comprehensions:\n#                         change LIST_APPEND and SET_ADD, add MAP_ADD #2183)\n#     Python 3.1a1: 3151 (optimize conditional branches:\n#                         introduce POP_JUMP_IF_FALSE and POP_JUMP_IF_TRUE\n                          #4715)\n#     Python 3.2a1: 3160 (add SETUP_WITH #6101)\n#                   tag: cpython-32\n#     Python 3.2a2: 3170 (add DUP_TOP_TWO, remove DUP_TOPX and ROT_FOUR #9225)\n#                   tag: cpython-32\n#     Python 3.2a3  3180 (add DELETE_DEREF #4617)\n#     Python 3.3a1  3190 (__class__ super closure changed)\n#     Python 3.3a1  3200 (PEP 3155 __qualname__ added #13448)\n#     Python 3.3a1  3210 (added size modulo 2**32 to the pyc header #13645)\n#     Python 3.3a2  3220 (changed PEP 380 implementation #14230)\n#     Python 3.3a4  3230 (revert changes to implicit __class__ closure #14857)\n#     Python 3.4a1  3250 (evaluate positional default arguments before\n#                        keyword-only defaults #16967)\n#     Python 3.4a1  3260 (add LOAD_CLASSDEREF; allow locals of class to override\n#                        free vars #17853)\n#     Python 3.4a1  3270 (various tweaks to the __class__ closure #12370)\n#     Python 3.4a1  3280 (remove implicit class argument)\n#     Python 3.4a4  3290 (changes to __qualname__ computation #19301)\n#     Python 3.4a4  3300 (more changes to __qualname__ computation #19301)\n#     Python 3.4rc2 3310 (alter __qualname__ computation #20625)\n#     Python 3.5a1  3320 (PEP 465: Matrix multiplication operator #21176)\n#     Python 3.5b1  3330 (PEP 448: Additional Unpacking Generalizations #2292)\n#     Python 3.5b2  3340 (fix dictionary display evaluation order #11205)\n#     Python 3.5b3  3350 (add GET_YIELD_FROM_ITER opcode #24400)\n#     Python 3.5.2  3351 (fix BUILD_MAP_UNPACK_WITH_CALL opcode #27286)\n#     Python 3.6a0  3360 (add FORMAT_VALUE opcode #25483)\n#     Python 3.6a1  3361 (lineno delta of code.co_lnotab becomes signed #26107)\n#     Python 3.6a2  3370 (16 bit wordcode #26647)\n#     Python 3.6a2  3371 (add BUILD_CONST_KEY_MAP opcode #27140)\n#     Python 3.6a2  3372 (MAKE_FUNCTION simplification, remove MAKE_CLOSURE\n#                         #27095)\n#     Python 3.6b1  3373 (add BUILD_STRING opcode #27078)\n#     Python 3.6b1  3375 (add SETUP_ANNOTATIONS and STORE_ANNOTATION opcodes\n#                         #27985)\n#     Python 3.6b1  3376 (simplify CALL_FUNCTIONs & BUILD_MAP_UNPACK_WITH_CALL\n                          #27213)\n#     Python 3.6b1  3377 (set __class__ cell from type.__new__ #23722)\n#     Python 3.6b2  3378 (add BUILD_TUPLE_UNPACK_WITH_CALL #28257)\n#     Python 3.6rc1 3379 (more thorough __class__ validation #23722)\n#     Python 3.7a1  3390 (add LOAD_METHOD and CALL_METHOD opcodes #26110)\n#     Python 3.7a2  3391 (update GET_AITER #31709)\n#     Python 3.7a4  3392 (PEP 552: Deterministic pycs #31650)\n#     Python 3.7b1  3393 (remove STORE_ANNOTATION opcode #32550)\n#     Python 3.7b5  3394 (restored docstring as the first stmt in the body;\n#                         this might affected the first line number #32911)\n#     Python 3.8a1  3400 (move frame block handling to compiler #17611)\n#     Python 3.8a1  3401 (add END_ASYNC_FOR #33041)\n#     Python 3.8a1  3410 (PEP570 Python Positional-Only Parameters #36540)\n#     Python 3.8b2  3411 (Reverse evaluation order of key: value in dict\n#                         comprehensions #35224)\n#     Python 3.8b2  3412 (Swap the position of positional args and positional\n#                         only args in ast.arguments #37593)\n#     Python 3.8b4  3413 (Fix \"break\" and \"continue\" in \"finally\" #37830)\n#     Python 3.9a0  3420 (add LOAD_ASSERTION_ERROR #34880)\n#     Python 3.9a0  3421 (simplified bytecode for with blocks #32949)\n#     Python 3.9a0  3422 (remove BEGIN_FINALLY, END_FINALLY, CALL_FINALLY, POP_FINALLY bytecodes #33387)\n#     Python 3.9a2  3423 (add IS_OP, CONTAINS_OP and JUMP_IF_NOT_EXC_MATCH bytecodes #39156)\n#     Python 3.9a2  3424 (simplify bytecodes for *value unpacking)\n#     Python 3.9a2  3425 (simplify bytecodes for **value unpacking)\n#     Python 3.10a1 3430 (Make 'annotations' future by default)\n#     Python 3.10a1 3431 (New line number table format -- PEP 626)\n#     Python 3.10a2 3432 (Function annotation for MAKE_FUNCTION is changed from dict to tuple bpo-42202)\n#     Python 3.10a2 3433 (RERAISE restores f_lasti if oparg != 0)\n#     Python 3.10a6 3434 (PEP 634: Structural Pattern Matching)\n#     Python 3.10a7 3435 Use instruction offsets (as opposed to byte offsets).\n#     Python 3.10b1 3436 (Add GEN_START bytecode #43683)\n#     Python 3.10b1 3437 (Undo making 'annotations' future by default - We like to dance among core devs!)\n#     Python 3.10b1 3438 Safer line number table handling.\n#     Python 3.10b1 3439 (Add ROT_N)\n#     Python 3.11a1 3450 Use exception table for unwinding (\"zero cost\" exception handling)\n#     Python 3.11a1 3451 (Add CALL_METHOD_KW)\n#     Python 3.11a1 3452 (drop nlocals from marshaled code objects)\n#     Python 3.11a1 3453 (add co_fastlocalnames and co_fastlocalkinds)\n#     Python 3.11a1 3454 (compute cell offsets relative to locals bpo-43693)\n#     Python 3.11a1 3455 (add MAKE_CELL bpo-43693)\n#     Python 3.11a1 3456 (interleave cell args bpo-43693)\n#     Python 3.11a1 3457 (Change localsplus to a bytes object bpo-43693)\n#     Python 3.11a1 3458 (imported objects now don't use LOAD_METHOD/CALL_METHOD)\n#     Python 3.11a1 3459 (PEP 657: add end line numbers and column offsets for instructions)\n#     Python 3.11a1 3460 (Add co_qualname field to PyCodeObject bpo-44530)\n#     Python 3.11a1 3461 (JUMP_ABSOLUTE must jump backwards)\n#     Python 3.11a2 3462 (bpo-44511: remove COPY_DICT_WITHOUT_KEYS, change\n#                         MATCH_CLASS and MATCH_KEYS, and add COPY)\n#     Python 3.11a3 3463 (bpo-45711: JUMP_IF_NOT_EXC_MATCH no longer pops the\n#                         active exception)\n#     Python 3.11a3 3464 (bpo-45636: Merge numeric BINARY_*/INPLACE_* into\n#                         BINARY_OP)\n#     Python 3.11a3 3465 (Add COPY_FREE_VARS opcode)\n#     Python 3.11a4 3466 (bpo-45292: PEP-654 except*)\n#     Python 3.11a4 3467 (Change CALL_xxx opcodes)\n#     Python 3.11a4 3468 (Add SEND opcode)\n#     Python 3.11a4 3469 (bpo-45711: remove type, traceback from exc_info)\n#     Python 3.11a4 3470 (bpo-46221: PREP_RERAISE_STAR no longer pushes lasti)\n#     Python 3.11a4 3471 (bpo-46202: remove pop POP_EXCEPT_AND_RERAISE)\n#     Python 3.11a4 3472 (bpo-46009: replace GEN_START with POP_TOP)\n#     Python 3.11a4 3473 (Add POP_JUMP_IF_NOT_NONE/POP_JUMP_IF_NONE opcodes)\n#     Python 3.11a4 3474 (Add RESUME opcode)\n#     Python 3.11a5 3475 (Add RETURN_GENERATOR opcode)\n#     Python 3.11a5 3476 (Add ASYNC_GEN_WRAP opcode)\n#     Python 3.11a5 3477 (Replace DUP_TOP/DUP_TOP_TWO with COPY and\n#                         ROT_TWO/ROT_THREE/ROT_FOUR/ROT_N with SWAP)\n#     Python 3.11a5 3478 (New CALL opcodes)\n#     Python 3.11a5 3479 (Add PUSH_NULL opcode)\n#     Python 3.11a5 3480 (New CALL opcodes, second iteration)\n#     Python 3.11a5 3481 (Use inline cache for BINARY_OP)\n#     Python 3.11a5 3482 (Use inline caching for UNPACK_SEQUENCE and LOAD_GLOBAL)\n#     Python 3.11a5 3483 (Use inline caching for COMPARE_OP and BINARY_SUBSCR)\n#     Python 3.11a5 3484 (Use inline caching for LOAD_ATTR, LOAD_METHOD, and\n#                         STORE_ATTR)\n#     Python 3.11a5 3485 (Add an oparg to GET_AWAITABLE)\n#     Python 3.11a6 3486 (Use inline caching for PRECALL and CALL)\n#     Python 3.11a6 3487 (Remove the adaptive \"oparg counter\" mechanism)\n#     Python 3.11a6 3488 (LOAD_GLOBAL can push additional NULL)\n#     Python 3.11a6 3489 (Add JUMP_BACKWARD, remove JUMP_ABSOLUTE)\n#     Python 3.11a6 3490 (remove JUMP_IF_NOT_EXC_MATCH, add CHECK_EXC_MATCH)\n#     Python 3.11a6 3491 (remove JUMP_IF_NOT_EG_MATCH, add CHECK_EG_MATCH,\n#                         add JUMP_BACKWARD_NO_INTERRUPT, make JUMP_NO_INTERRUPT virtual)\n#     Python 3.11a7 3492 (make POP_JUMP_IF_NONE/NOT_NONE/TRUE/FALSE relative)\n#     Python 3.11a7 3493 (Make JUMP_IF_TRUE_OR_POP/JUMP_IF_FALSE_OR_POP relative)\n#     Python 3.11a7 3494 (New location info table)\n#     Python 3.11b4 3495 (Set line number of module's RESUME instr to 0 per PEP 626)\n#     Python 3.12a1 3500 (Remove PRECALL opcode)\n#     Python 3.12a1 3501 (YIELD_VALUE oparg == stack_depth)\n#     Python 3.12a1 3502 (LOAD_FAST_CHECK, no NULL-check in LOAD_FAST)\n#     Python 3.12a1 3503 (Shrink LOAD_METHOD cache)\n#     Python 3.12a1 3504 (Merge LOAD_METHOD back into LOAD_ATTR)\n#     Python 3.12a1 3505 (Specialization/Cache for FOR_ITER)\n#     Python 3.12a1 3506 (Add BINARY_SLICE and STORE_SLICE instructions)\n#     Python 3.12a1 3507 (Set lineno of module's RESUME to 0)\n#     Python 3.12a1 3508 (Add CLEANUP_THROW)\n#     Python 3.12a1 3509 (Conditional jumps only jump forward)\n#     Python 3.12a2 3510 (FOR_ITER leaves iterator on the stack)\n#     Python 3.12a2 3511 (Add STOPITERATION_ERROR instruction)\n#     Python 3.12a2 3512 (Remove all unused consts from code objects)\n#     Python 3.12a4 3513 (Add CALL_INTRINSIC_1 instruction, removed STOPITERATION_ERROR, PRINT_EXPR, IMPORT_STAR)\n#     Python 3.12a4 3514 (Remove ASYNC_GEN_WRAP, LIST_TO_TUPLE, and UNARY_POSITIVE)\n#     Python 3.12a5 3515 (Embed jump mask in COMPARE_OP oparg)\n#     Python 3.12a5 3516 (Add COMPARE_AND_BRANCH instruction)\n#     Python 3.12a5 3517 (Change YIELD_VALUE oparg to exception block depth)\n#     Python 3.12a6 3518 (Add RETURN_CONST instruction)\n#     Python 3.12a6 3519 (Modify SEND instruction)\n#     Python 3.12a6 3520 (Remove PREP_RERAISE_STAR, add CALL_INTRINSIC_2)\n#     Python 3.12a7 3521 (Shrink the LOAD_GLOBAL caches)\n#     Python 3.12a7 3522 (Removed JUMP_IF_FALSE_OR_POP/JUMP_IF_TRUE_OR_POP)\n#     Python 3.12a7 3523 (Convert COMPARE_AND_BRANCH back to COMPARE_OP)\n#     Python 3.12a7 3524 (Shrink the BINARY_SUBSCR caches)\n#     Python 3.12b1 3525 (Shrink the CALL caches)\n#     Python 3.12b1 3526 (Add instrumentation support)\n#     Python 3.12b1 3527 (Add LOAD_SUPER_ATTR)\n#     Python 3.12b1 3528 (Add LOAD_SUPER_ATTR_METHOD specialization)\n#     Python 3.12b1 3529 (Inline list/dict/set comprehensions)\n#     Python 3.12b1 3530 (Shrink the LOAD_SUPER_ATTR caches)\n#     Python 3.12b1 3531 (Add PEP 695 changes)\n\n#     Python 3.13 will start with 3550\n\n#     Please don't copy-paste the same pre-release tag for new entries above!!!\n#     You should always use the *upcoming* tag. For example, if 3.12a6 came out\n#     a week ago, I should put \"Python 3.12a7\" next to my new magic number.\n\n# MAGIC must change whenever the bytecode emitted by the compiler may no\n# longer be understood by older implementations of the eval loop (usually\n# due to the addition of new opcodes).\n#\n# Starting with Python 3.11, Python 3.n starts with magic number 2900+50n.\n#\n# Whenever MAGIC_NUMBER is changed, the ranges in the magic_values array\n# in PC/launcher.c must also be updated.\n\nMAGIC_NUMBER = (3531).to_bytes(2, 'little') + b'\\r\\n'\n\n_RAW_MAGIC_NUMBER = int.from_bytes(MAGIC_NUMBER, 'little')  # For import.c\n\n_PYCACHE = '__pycache__'\n_OPT = 'opt-'\n\nSOURCE_SUFFIXES = ['.py']\nif _MS_WINDOWS:\n    SOURCE_SUFFIXES.append('.pyw')\n\nEXTENSION_SUFFIXES = _imp.extension_suffixes()\n\nBYTECODE_SUFFIXES = ['.pyc']\n# Deprecated.\nDEBUG_BYTECODE_SUFFIXES = OPTIMIZED_BYTECODE_SUFFIXES = BYTECODE_SUFFIXES\n\ndef cache_from_source(path, debug_override=None, *, optimization=None):\n    \"\"\"Given the path to a .py file, return the path to its .pyc file.\n\n    The .py file does not need to exist; this simply returns the path to the\n    .pyc file calculated as if the .py file were imported.\n\n    The 'optimization' parameter controls the presumed optimization level of\n    the bytecode file. If 'optimization' is not None, the string representation\n    of the argument is taken and verified to be alphanumeric (else ValueError\n    is raised).\n\n    The debug_override parameter is deprecated. If debug_override is not None,\n    a True value is the same as setting 'optimization' to the empty string\n    while a False value is equivalent to setting 'optimization' to '1'.\n\n    If sys.implementation.cache_tag is None then NotImplementedError is raised.\n\n    \"\"\"\n    if debug_override is not None:\n        _warnings.warn('the debug_override parameter is deprecated; use '\n                       \"'optimization' instead\", DeprecationWarning)\n        if optimization is not None:\n            message = 'debug_override or optimization must be set to None'\n            raise TypeError(message)\n        optimization = '' if debug_override else 1\n    path = _os.fspath(path)\n    head, tail = _path_split(path)\n    base, sep, rest = tail.rpartition('.')\n    tag = sys.implementation.cache_tag\n    if tag is None:\n        raise NotImplementedError('sys.implementation.cache_tag is None')\n    almost_filename = ''.join([(base if base else rest), sep, tag])\n    if optimization is None:\n        if sys.flags.optimize == 0:\n            optimization = ''\n        else:\n            optimization = sys.flags.optimize\n    optimization = str(optimization)\n    if optimization != '':\n        if not optimization.isalnum():\n            raise ValueError(f'{optimization!r} is not alphanumeric')\n        almost_filename = f'{almost_filename}.{_OPT}{optimization}'\n    filename = almost_filename + BYTECODE_SUFFIXES[0]\n    if sys.pycache_prefix is not None:\n        # We need an absolute path to the py file to avoid the possibility of\n        # collisions within sys.pycache_prefix, if someone has two different\n        # `foo/bar.py` on their system and they import both of them using the\n        # same sys.pycache_prefix. Let's say sys.pycache_prefix is\n        # `C:\\Bytecode`; the idea here is that if we get `Foo\\Bar`, we first\n        # make it absolute (`C:\\Somewhere\\Foo\\Bar`), then make it root-relative\n        # (`Somewhere\\Foo\\Bar`), so we end up placing the bytecode file in an\n        # unambiguous `C:\\Bytecode\\Somewhere\\Foo\\Bar\\`.\n        head = _path_abspath(head)\n\n        # Strip initial drive from a Windows path. We know we have an absolute\n        # path here, so the second part of the check rules out a POSIX path that\n        # happens to contain a colon at the second character.\n        if head[1] == ':' and head[0] not in path_separators:\n            head = head[2:]\n\n        # Strip initial path separator from `head` to complete the conversion\n        # back to a root-relative path before joining.\n        return _path_join(\n            sys.pycache_prefix,\n            head.lstrip(path_separators),\n            filename,\n        )\n    return _path_join(head, _PYCACHE, filename)\n\n\ndef source_from_cache(path):\n    \"\"\"Given the path to a .pyc. file, return the path to its .py file.\n\n    The .pyc file does not need to exist; this simply returns the path to\n    the .py file calculated to correspond to the .pyc file.  If path does\n    not conform to PEP 3147/488 format, ValueError will be raised. If\n    sys.implementation.cache_tag is None then NotImplementedError is raised.\n\n    \"\"\"\n    if sys.implementation.cache_tag is None:\n        raise NotImplementedError('sys.implementation.cache_tag is None')\n    path = _os.fspath(path)\n    head, pycache_filename = _path_split(path)\n    found_in_pycache_prefix = False\n    if sys.pycache_prefix is not None:\n        stripped_path = sys.pycache_prefix.rstrip(path_separators)\n        if head.startswith(stripped_path + path_sep):\n            head = head[len(stripped_path):]\n            found_in_pycache_prefix = True\n    if not found_in_pycache_prefix:\n        head, pycache = _path_split(head)\n        if pycache != _PYCACHE:\n            raise ValueError(f'{_PYCACHE} not bottom-level directory in '\n                             f'{path!r}')\n    dot_count = pycache_filename.count('.')\n    if dot_count not in {2, 3}:\n        raise ValueError(f'expected only 2 or 3 dots in {pycache_filename!r}')\n    elif dot_count == 3:\n        optimization = pycache_filename.rsplit('.', 2)[-2]\n        if not optimization.startswith(_OPT):\n            raise ValueError(\"optimization portion of filename does not start \"\n                             f\"with {_OPT!r}\")\n        opt_level = optimization[len(_OPT):]\n        if not opt_level.isalnum():\n            raise ValueError(f\"optimization level {optimization!r} is not an \"\n                             \"alphanumeric value\")\n    base_filename = pycache_filename.partition('.')[0]\n    return _path_join(head, base_filename + SOURCE_SUFFIXES[0])\n\n\ndef _get_sourcefile(bytecode_path):\n    \"\"\"Convert a bytecode file path to a source path (if possible).\n\n    This function exists purely for backwards-compatibility for\n    PyImport_ExecCodeModuleWithFilenames() in the C API.\n\n    \"\"\"\n    if len(bytecode_path) == 0:\n        return None\n    rest, _, extension = bytecode_path.rpartition('.')\n    if not rest or extension.lower()[-3:-1] != 'py':\n        return bytecode_path\n    try:\n        source_path = source_from_cache(bytecode_path)\n    except (NotImplementedError, ValueError):\n        source_path = bytecode_path[:-1]\n    return source_path if _path_isfile(source_path) else bytecode_path\n\n\ndef _get_cached(filename):\n    if filename.endswith(tuple(SOURCE_SUFFIXES)):\n        try:\n            return cache_from_source(filename)\n        except NotImplementedError:\n            pass\n    elif filename.endswith(tuple(BYTECODE_SUFFIXES)):\n        return filename\n    else:\n        return None\n\n\ndef _calc_mode(path):\n    \"\"\"Calculate the mode permissions for a bytecode file.\"\"\"\n    try:\n        mode = _path_stat(path).st_mode\n    except OSError:\n        mode = 0o666\n    # We always ensure write access so we can update cached files\n    # later even when the source files are read-only on Windows (#6074)\n    mode |= 0o200\n    return mode\n\n\ndef _check_name(method):\n    \"\"\"Decorator to verify that the module being requested matches the one the\n    loader can handle.\n\n    The first argument (self) must define _name which the second argument is\n    compared against. If the comparison fails then ImportError is raised.\n\n    \"\"\"\n    def _check_name_wrapper(self, name=None, *args, **kwargs):\n        if name is None:\n            name = self.name\n        elif self.name != name:\n            raise ImportError('loader for %s cannot handle %s' %\n                                (self.name, name), name=name)\n        return method(self, name, *args, **kwargs)\n\n    # FIXME: @_check_name is used to define class methods before the\n    # _bootstrap module is set by _set_bootstrap_module().\n    if _bootstrap is not None:\n        _wrap = _bootstrap._wrap\n    else:\n        def _wrap(new, old):\n            for replace in ['__module__', '__name__', '__qualname__', '__doc__']:\n                if hasattr(old, replace):\n                    setattr(new, replace, getattr(old, replace))\n            new.__dict__.update(old.__dict__)\n\n    _wrap(_check_name_wrapper, method)\n    return _check_name_wrapper\n\n\ndef _classify_pyc(data, name, exc_details):\n    \"\"\"Perform basic validity checking of a pyc header and return the flags field,\n    which determines how the pyc should be further validated against the source.\n\n    *data* is the contents of the pyc file. (Only the first 16 bytes are\n    required, though.)\n\n    *name* is the name of the module being imported. It is used for logging.\n\n    *exc_details* is a dictionary passed to ImportError if it raised for\n    improved debugging.\n\n    ImportError is raised when the magic number is incorrect or when the flags\n    field is invalid. EOFError is raised when the data is found to be truncated.\n\n    \"\"\"\n    magic = data[:4]\n    if magic != MAGIC_NUMBER:\n        message = f'bad magic number in {name!r}: {magic!r}'\n        _bootstrap._verbose_message('{}', message)\n        raise ImportError(message, **exc_details)\n    if len(data) < 16:\n        message = f'reached EOF while reading pyc header of {name!r}'\n        _bootstrap._verbose_message('{}', message)\n        raise EOFError(message)\n    flags = _unpack_uint32(data[4:8])\n    # Only the first two flags are defined.\n    if flags & ~0b11:\n        message = f'invalid flags {flags!r} in {name!r}'\n        raise ImportError(message, **exc_details)\n    return flags\n\n\ndef _validate_timestamp_pyc(data, source_mtime, source_size, name,\n                            exc_details):\n    \"\"\"Validate a pyc against the source last-modified time.\n\n    *data* is the contents of the pyc file. (Only the first 16 bytes are\n    required.)\n\n    *source_mtime* is the last modified timestamp of the source file.\n\n    *source_size* is None or the size of the source file in bytes.\n\n    *name* is the name of the module being imported. It is used for logging.\n\n    *exc_details* is a dictionary passed to ImportError if it raised for\n    improved debugging.\n\n    An ImportError is raised if the bytecode is stale.\n\n    \"\"\"\n    if _unpack_uint32(data[8:12]) != (source_mtime & 0xFFFFFFFF):\n        message = f'bytecode is stale for {name!r}'\n        _bootstrap._verbose_message('{}', message)\n        raise ImportError(message, **exc_details)\n    if (source_size is not None and\n        _unpack_uint32(data[12:16]) != (source_size & 0xFFFFFFFF)):\n        raise ImportError(f'bytecode is stale for {name!r}', **exc_details)\n\n\ndef _validate_hash_pyc(data, source_hash, name, exc_details):\n    \"\"\"Validate a hash-based pyc by checking the real source hash against the one in\n    the pyc header.\n\n    *data* is the contents of the pyc file. (Only the first 16 bytes are\n    required.)\n\n    *source_hash* is the importlib.util.source_hash() of the source file.\n\n    *name* is the name of the module being imported. It is used for logging.\n\n    *exc_details* is a dictionary passed to ImportError if it raised for\n    improved debugging.\n\n    An ImportError is raised if the bytecode is stale.\n\n    \"\"\"\n    if data[8:16] != source_hash:\n        raise ImportError(\n            f'hash in bytecode doesn\\'t match hash of source {name!r}',\n            **exc_details,\n        )\n\n\ndef _compile_bytecode(data, name=None, bytecode_path=None, source_path=None):\n    \"\"\"Compile bytecode as found in a pyc.\"\"\"\n    code = marshal.loads(data)\n    if isinstance(code, _code_type):\n        _bootstrap._verbose_message('code object from {!r}', bytecode_path)\n        if source_path is not None:\n            _imp._fix_co_filename(code, source_path)\n        return code\n    else:\n        raise ImportError(f'Non-code object in {bytecode_path!r}',\n                          name=name, path=bytecode_path)\n\n\ndef _code_to_timestamp_pyc(code, mtime=0, source_size=0):\n    \"Produce the data for a timestamp-based pyc.\"\n    data = bytearray(MAGIC_NUMBER)\n    data.extend(_pack_uint32(0))\n    data.extend(_pack_uint32(mtime))\n    data.extend(_pack_uint32(source_size))\n    data.extend(marshal.dumps(code))\n    return data\n\n\ndef _code_to_hash_pyc(code, source_hash, checked=True):\n    \"Produce the data for a hash-based pyc.\"\n    data = bytearray(MAGIC_NUMBER)\n    flags = 0b1 | checked << 1\n    data.extend(_pack_uint32(flags))\n    assert len(source_hash) == 8\n    data.extend(source_hash)\n    data.extend(marshal.dumps(code))\n    return data\n\n\ndef decode_source(source_bytes):\n    \"\"\"Decode bytes representing source code and return the string.\n\n    Universal newline support is used in the decoding.\n    \"\"\"\n    import tokenize  # To avoid bootstrap issues.\n    source_bytes_readline = _io.BytesIO(source_bytes).readline\n    encoding = tokenize.detect_encoding(source_bytes_readline)\n    newline_decoder = _io.IncrementalNewlineDecoder(None, True)\n    return newline_decoder.decode(source_bytes.decode(encoding[0]))\n\n\n# Module specifications #######################################################\n\n_POPULATE = object()\n\n\ndef spec_from_file_location(name, location=None, *, loader=None,\n                            submodule_search_locations=_POPULATE):\n    \"\"\"Return a module spec based on a file location.\n\n    To indicate that the module is a package, set\n    submodule_search_locations to a list of directory paths.  An\n    empty list is sufficient, though its not otherwise useful to the\n    import system.\n\n    The loader must take a spec as its only __init__() arg.\n\n    \"\"\"\n    if location is None:\n        # The caller may simply want a partially populated location-\n        # oriented spec.  So we set the location to a bogus value and\n        # fill in as much as we can.\n        location = '<unknown>'\n        if hasattr(loader, 'get_filename'):\n            # ExecutionLoader\n            try:\n                location = loader.get_filename(name)\n            except ImportError:\n                pass\n    else:\n        location = _os.fspath(location)\n        try:\n            location = _path_abspath(location)\n        except OSError:\n            pass\n\n    # If the location is on the filesystem, but doesn't actually exist,\n    # we could return None here, indicating that the location is not\n    # valid.  However, we don't have a good way of testing since an\n    # indirect location (e.g. a zip file or URL) will look like a\n    # non-existent file relative to the filesystem.\n\n    spec = _bootstrap.ModuleSpec(name, loader, origin=location)\n    spec._set_fileattr = True\n\n    # Pick a loader if one wasn't provided.\n    if loader is None:\n        for loader_class, suffixes in _get_supported_file_loaders():\n            if location.endswith(tuple(suffixes)):\n                loader = loader_class(name, location)\n                spec.loader = loader\n                break\n        else:\n            return None\n\n    # Set submodule_search_paths appropriately.\n    if submodule_search_locations is _POPULATE:\n        # Check the loader.\n        if hasattr(loader, 'is_package'):\n            try:\n                is_package = loader.is_package(name)\n            except ImportError:\n                pass\n            else:\n                if is_package:\n                    spec.submodule_search_locations = []\n    else:\n        spec.submodule_search_locations = submodule_search_locations\n    if spec.submodule_search_locations == []:\n        if location:\n            dirname = _path_split(location)[0]\n            spec.submodule_search_locations.append(dirname)\n\n    return spec\n\n\ndef _bless_my_loader(module_globals):\n    \"\"\"Helper function for _warnings.c\n\n    See GH#97850 for details.\n    \"\"\"\n    # 2022-10-06(warsaw): For now, this helper is only used in _warnings.c and\n    # that use case only has the module globals.  This function could be\n    # extended to accept either that or a module object.  However, in the\n    # latter case, it would be better to raise certain exceptions when looking\n    # at a module, which should have either a __loader__ or __spec__.loader.\n    # For backward compatibility, it is possible that we'll get an empty\n    # dictionary for the module globals, and that cannot raise an exception.\n    if not isinstance(module_globals, dict):\n        return None\n\n    missing = object()\n    loader = module_globals.get('__loader__', None)\n    spec = module_globals.get('__spec__', missing)\n\n    if loader is None:\n        if spec is missing:\n            # If working with a module:\n            # raise AttributeError('Module globals is missing a __spec__')\n            return None\n        elif spec is None:\n            raise ValueError('Module globals is missing a __spec__.loader')\n\n    spec_loader = getattr(spec, 'loader', missing)\n\n    if spec_loader in (missing, None):\n        if loader is None:\n            exc = AttributeError if spec_loader is missing else ValueError\n            raise exc('Module globals is missing a __spec__.loader')\n        _warnings.warn(\n            'Module globals is missing a __spec__.loader',\n            DeprecationWarning)\n        spec_loader = loader\n\n    assert spec_loader is not None\n    if loader is not None and loader != spec_loader:\n        _warnings.warn(\n            'Module globals; __loader__ != __spec__.loader',\n            DeprecationWarning)\n        return loader\n\n    return spec_loader\n\n\n# Loaders #####################################################################\n\nclass WindowsRegistryFinder:\n\n    \"\"\"Meta path finder for modules declared in the Windows registry.\"\"\"\n\n    REGISTRY_KEY = (\n        'Software\\\\Python\\\\PythonCore\\\\{sys_version}'\n        '\\\\Modules\\\\{fullname}')\n    REGISTRY_KEY_DEBUG = (\n        'Software\\\\Python\\\\PythonCore\\\\{sys_version}'\n        '\\\\Modules\\\\{fullname}\\\\Debug')\n    DEBUG_BUILD = (_MS_WINDOWS and '_d.pyd' in EXTENSION_SUFFIXES)\n\n    @staticmethod\n    def _open_registry(key):\n        try:\n            return winreg.OpenKey(winreg.HKEY_CURRENT_USER, key)\n        except OSError:\n            return winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, key)\n\n    @classmethod\n    def _search_registry(cls, fullname):\n        if cls.DEBUG_BUILD:\n            registry_key = cls.REGISTRY_KEY_DEBUG\n        else:\n            registry_key = cls.REGISTRY_KEY\n        key = registry_key.format(fullname=fullname,\n                                  sys_version='%d.%d' % sys.version_info[:2])\n        try:\n            with cls._open_registry(key) as hkey:\n                filepath = winreg.QueryValue(hkey, '')\n        except OSError:\n            return None\n        return filepath\n\n    @classmethod\n    def find_spec(cls, fullname, path=None, target=None):\n        filepath = cls._search_registry(fullname)\n        if filepath is None:\n            return None\n        try:\n            _path_stat(filepath)\n        except OSError:\n            return None\n        for loader, suffixes in _get_supported_file_loaders():\n            if filepath.endswith(tuple(suffixes)):\n                spec = _bootstrap.spec_from_loader(fullname,\n                                                   loader(fullname, filepath),\n                                                   origin=filepath)\n                return spec\n\n\nclass _LoaderBasics:\n\n    \"\"\"Base class of common code needed by both SourceLoader and\n    SourcelessFileLoader.\"\"\"\n\n    def is_package(self, fullname):\n        \"\"\"Concrete implementation of InspectLoader.is_package by checking if\n        the path returned by get_filename has a filename of '__init__.py'.\"\"\"\n        filename = _path_split(self.get_filename(fullname))[1]\n        filename_base = filename.rsplit('.', 1)[0]\n        tail_name = fullname.rpartition('.')[2]\n        return filename_base == '__init__' and tail_name != '__init__'\n\n    def create_module(self, spec):\n        \"\"\"Use default semantics for module creation.\"\"\"\n\n    def exec_module(self, module):\n        \"\"\"Execute the module.\"\"\"\n        code = self.get_code(module.__name__)\n        if code is None:\n            raise ImportError(f'cannot load module {module.__name__!r} when '\n                              'get_code() returns None')\n        _bootstrap._call_with_frames_removed(exec, code, module.__dict__)\n\n    def load_module(self, fullname):\n        \"\"\"This method is deprecated.\"\"\"\n        # Warning implemented in _load_module_shim().\n        return _bootstrap._load_module_shim(self, fullname)\n\n\nclass SourceLoader(_LoaderBasics):\n\n    def path_mtime(self, path):\n        \"\"\"Optional method that returns the modification time (an int) for the\n        specified path (a str).\n\n        Raises OSError when the path cannot be handled.\n        \"\"\"\n        raise OSError\n\n    def path_stats(self, path):\n        \"\"\"Optional method returning a metadata dict for the specified\n        path (a str).\n\n        Possible keys:\n        - 'mtime' (mandatory) is the numeric timestamp of last source\n          code modification;\n        - 'size' (optional) is the size in bytes of the source code.\n\n        Implementing this method allows the loader to read bytecode files.\n        Raises OSError when the path cannot be handled.\n        \"\"\"\n        return {'mtime': self.path_mtime(path)}\n\n    def _cache_bytecode(self, source_path, cache_path, data):\n        \"\"\"Optional method which writes data (bytes) to a file path (a str).\n\n        Implementing this method allows for the writing of bytecode files.\n\n        The source path is needed in order to correctly transfer permissions\n        \"\"\"\n        # For backwards compatibility, we delegate to set_data()\n        return self.set_data(cache_path, data)\n\n    def set_data(self, path, data):\n        \"\"\"Optional method which writes data (bytes) to a file path (a str).\n\n        Implementing this method allows for the writing of bytecode files.\n        \"\"\"\n\n\n    def get_source(self, fullname):\n        \"\"\"Concrete implementation of InspectLoader.get_source.\"\"\"\n        path = self.get_filename(fullname)\n        try:\n            source_bytes = self.get_data(path)\n        except OSError as exc:\n            raise ImportError('source not available through get_data()',\n                              name=fullname) from exc\n        return decode_source(source_bytes)\n\n    def source_to_code(self, data, path, *, _optimize=-1):\n        \"\"\"Return the code object compiled from source.\n\n        The 'data' argument can be any object type that compile() supports.\n        \"\"\"\n        return _bootstrap._call_with_frames_removed(compile, data, path, 'exec',\n                                        dont_inherit=True, optimize=_optimize)\n\n    def get_code(self, fullname):\n        \"\"\"Concrete implementation of InspectLoader.get_code.\n\n        Reading of bytecode requires path_stats to be implemented. To write\n        bytecode, set_data must also be implemented.\n\n        \"\"\"\n        source_path = self.get_filename(fullname)\n        source_mtime = None\n        source_bytes = None\n        source_hash = None\n        hash_based = False\n        check_source = True\n        try:\n            bytecode_path = cache_from_source(source_path)\n        except NotImplementedError:\n            bytecode_path = None\n        else:\n            try:\n                st = self.path_stats(source_path)\n            except OSError:\n                pass\n            else:\n                source_mtime = int(st['mtime'])\n                try:\n                    data = self.get_data(bytecode_path)\n                except OSError:\n                    pass\n                else:\n                    exc_details = {\n                        'name': fullname,\n                        'path': bytecode_path,\n                    }\n                    try:\n                        flags = _classify_pyc(data, fullname, exc_details)\n                        bytes_data = memoryview(data)[16:]\n                        hash_based = flags & 0b1 != 0\n                        if hash_based:\n                            check_source = flags & 0b10 != 0\n                            if (_imp.check_hash_based_pycs != 'never' and\n                                (check_source or\n                                 _imp.check_hash_based_pycs == 'always')):\n                                source_bytes = self.get_data(source_path)\n                                source_hash = _imp.source_hash(\n                                    _RAW_MAGIC_NUMBER,\n                                    source_bytes,\n                                )\n                                _validate_hash_pyc(data, source_hash, fullname,\n                                                   exc_details)\n                        else:\n                            _validate_timestamp_pyc(\n                                data,\n                                source_mtime,\n                                st['size'],\n                                fullname,\n                                exc_details,\n                            )\n                    except (ImportError, EOFError):\n                        pass\n                    else:\n                        _bootstrap._verbose_message('{} matches {}', bytecode_path,\n                                                    source_path)\n                        return _compile_bytecode(bytes_data, name=fullname,\n                                                 bytecode_path=bytecode_path,\n                                                 source_path=source_path)\n        if source_bytes is None:\n            source_bytes = self.get_data(source_path)\n        code_object = self.source_to_code(source_bytes, source_path)\n        _bootstrap._verbose_message('code object from {}', source_path)\n        if (not sys.dont_write_bytecode and bytecode_path is not None and\n                source_mtime is not None):\n            if hash_based:\n                if source_hash is None:\n                    source_hash = _imp.source_hash(_RAW_MAGIC_NUMBER,\n                                                   source_bytes)\n                data = _code_to_hash_pyc(code_object, source_hash, check_source)\n            else:\n                data = _code_to_timestamp_pyc(code_object, source_mtime,\n                                              len(source_bytes))\n            try:\n                self._cache_bytecode(source_path, bytecode_path, data)\n            except NotImplementedError:\n                pass\n        return code_object\n\n\nclass FileLoader:\n\n    \"\"\"Base file loader class which implements the loader protocol methods that\n    require file system usage.\"\"\"\n\n    def __init__(self, fullname, path):\n        \"\"\"Cache the module name and the path to the file found by the\n        finder.\"\"\"\n        self.name = fullname\n        self.path = path\n\n    def __eq__(self, other):\n        return (self.__class__ == other.__class__ and\n                self.__dict__ == other.__dict__)\n\n    def __hash__(self):\n        return hash(self.name) ^ hash(self.path)\n\n    @_check_name\n    def load_module(self, fullname):\n        \"\"\"Load a module from a file.\n\n        This method is deprecated.  Use exec_module() instead.\n\n        \"\"\"\n        # The only reason for this method is for the name check.\n        # Issue #14857: Avoid the zero-argument form of super so the implementation\n        # of that form can be updated without breaking the frozen module.\n        return super(FileLoader, self).load_module(fullname)\n\n    @_check_name\n    def get_filename(self, fullname):\n        \"\"\"Return the path to the source file as found by the finder.\"\"\"\n        return self.path\n\n    def get_data(self, path):\n        \"\"\"Return the data from path as raw bytes.\"\"\"\n        if isinstance(self, (SourceLoader, ExtensionFileLoader)):\n            with _io.open_code(str(path)) as file:\n                return file.read()\n        else:\n            with _io.FileIO(path, 'r') as file:\n                return file.read()\n\n    @_check_name\n    def get_resource_reader(self, module):\n        from importlib.readers import FileReader\n        return FileReader(self)\n\n\nclass SourceFileLoader(FileLoader, SourceLoader):\n\n    \"\"\"Concrete implementation of SourceLoader using the file system.\"\"\"\n\n    def path_stats(self, path):\n        \"\"\"Return the metadata for the path.\"\"\"\n        st = _path_stat(path)\n        return {'mtime': st.st_mtime, 'size': st.st_size}\n\n    def _cache_bytecode(self, source_path, bytecode_path, data):\n        # Adapt between the two APIs\n        mode = _calc_mode(source_path)\n        return self.set_data(bytecode_path, data, _mode=mode)\n\n    def set_data(self, path, data, *, _mode=0o666):\n        \"\"\"Write bytes data to a file.\"\"\"\n        parent, filename = _path_split(path)\n        path_parts = []\n        # Figure out what directories are missing.\n        while parent and not _path_isdir(parent):\n            parent, part = _path_split(parent)\n            path_parts.append(part)\n        # Create needed directories.\n        for part in reversed(path_parts):\n            parent = _path_join(parent, part)\n            try:\n                _os.mkdir(parent)\n            except FileExistsError:\n                # Probably another Python process already created the dir.\n                continue\n            except OSError as exc:\n                # Could be a permission error, read-only filesystem: just forget\n                # about writing the data.\n                _bootstrap._verbose_message('could not create {!r}: {!r}',\n                                            parent, exc)\n                return\n        try:\n            _write_atomic(path, data, _mode)\n            _bootstrap._verbose_message('created {!r}', path)\n        except OSError as exc:\n            # Same as above: just don't write the bytecode.\n            _bootstrap._verbose_message('could not create {!r}: {!r}', path,\n                                        exc)\n\n\nclass SourcelessFileLoader(FileLoader, _LoaderBasics):\n\n    \"\"\"Loader which handles sourceless file imports.\"\"\"\n\n    def get_code(self, fullname):\n        path = self.get_filename(fullname)\n        data = self.get_data(path)\n        # Call _classify_pyc to do basic validation of the pyc but ignore the\n        # result. There's no source to check against.\n        exc_details = {\n            'name': fullname,\n            'path': path,\n        }\n        _classify_pyc(data, fullname, exc_details)\n        return _compile_bytecode(\n            memoryview(data)[16:],\n            name=fullname,\n            bytecode_path=path,\n        )\n\n    def get_source(self, fullname):\n        \"\"\"Return None as there is no source code.\"\"\"\n        return None\n\n\nclass ExtensionFileLoader(FileLoader, _LoaderBasics):\n\n    \"\"\"Loader for extension modules.\n\n    The constructor is designed to work with FileFinder.\n\n    \"\"\"\n\n    def __init__(self, name, path):\n        self.name = name\n        self.path = path\n\n    def __eq__(self, other):\n        return (self.__class__ == other.__class__ and\n                self.__dict__ == other.__dict__)\n\n    def __hash__(self):\n        return hash(self.name) ^ hash(self.path)\n\n    def create_module(self, spec):\n        \"\"\"Create an uninitialized extension module\"\"\"\n        module = _bootstrap._call_with_frames_removed(\n            _imp.create_dynamic, spec)\n        _bootstrap._verbose_message('extension module {!r} loaded from {!r}',\n                         spec.name, self.path)\n        return module\n\n    def exec_module(self, module):\n        \"\"\"Initialize an extension module\"\"\"\n        _bootstrap._call_with_frames_removed(_imp.exec_dynamic, module)\n        _bootstrap._verbose_message('extension module {!r} executed from {!r}',\n                         self.name, self.path)\n\n    def is_package(self, fullname):\n        \"\"\"Return True if the extension module is a package.\"\"\"\n        file_name = _path_split(self.path)[1]\n        return any(file_name == '__init__' + suffix\n                   for suffix in EXTENSION_SUFFIXES)\n\n    def get_code(self, fullname):\n        \"\"\"Return None as an extension module cannot create a code object.\"\"\"\n        return None\n\n    def get_source(self, fullname):\n        \"\"\"Return None as extension modules have no source code.\"\"\"\n        return None\n\n    @_check_name\n    def get_filename(self, fullname):\n        \"\"\"Return the path to the source file as found by the finder.\"\"\"\n        return self.path\n\n\nclass _NamespacePath:\n    \"\"\"Represents a namespace package's path.  It uses the module name\n    to find its parent module, and from there it looks up the parent's\n    __path__.  When this changes, the module's own path is recomputed,\n    using path_finder.  For top-level modules, the parent module's path\n    is sys.path.\"\"\"\n\n    # When invalidate_caches() is called, this epoch is incremented\n    # https://bugs.python.org/issue45703\n    _epoch = 0\n\n    def __init__(self, name, path, path_finder):\n        self._name = name\n        self._path = path\n        self._last_parent_path = tuple(self._get_parent_path())\n        self._last_epoch = self._epoch\n        self._path_finder = path_finder\n\n    def _find_parent_path_names(self):\n        \"\"\"Returns a tuple of (parent-module-name, parent-path-attr-name)\"\"\"\n        parent, dot, me = self._name.rpartition('.')\n        if dot == '':\n            # This is a top-level module. sys.path contains the parent path.\n            return 'sys', 'path'\n        # Not a top-level module. parent-module.__path__ contains the\n        #  parent path.\n        return parent, '__path__'\n\n    def _get_parent_path(self):\n        parent_module_name, path_attr_name = self._find_parent_path_names()\n        return getattr(sys.modules[parent_module_name], path_attr_name)\n\n    def _recalculate(self):\n        # If the parent's path has changed, recalculate _path\n        parent_path = tuple(self._get_parent_path()) # Make a copy\n        if parent_path != self._last_parent_path or self._epoch != self._last_epoch:\n            spec = self._path_finder(self._name, parent_path)\n            # Note that no changes are made if a loader is returned, but we\n            #  do remember the new parent path\n            if spec is not None and spec.loader is None:\n                if spec.submodule_search_locations:\n                    self._path = spec.submodule_search_locations\n            self._last_parent_path = parent_path     # Save the copy\n            self._last_epoch = self._epoch\n        return self._path\n\n    def __iter__(self):\n        return iter(self._recalculate())\n\n    def __getitem__(self, index):\n        return self._recalculate()[index]\n\n    def __setitem__(self, index, path):\n        self._path[index] = path\n\n    def __len__(self):\n        return len(self._recalculate())\n\n    def __repr__(self):\n        return f'_NamespacePath({self._path!r})'\n\n    def __contains__(self, item):\n        return item in self._recalculate()\n\n    def append(self, item):\n        self._path.append(item)\n\n\n# This class is actually exposed publicly in a namespace package's __loader__\n# attribute, so it should be available through a non-private name.\n# https://github.com/python/cpython/issues/92054\nclass NamespaceLoader:\n    def __init__(self, name, path, path_finder):\n        self._path = _NamespacePath(name, path, path_finder)\n\n    def is_package(self, fullname):\n        return True\n\n    def get_source(self, fullname):\n        return ''\n\n    def get_code(self, fullname):\n        return compile('', '<string>', 'exec', dont_inherit=True)\n\n    def create_module(self, spec):\n        \"\"\"Use default semantics for module creation.\"\"\"\n\n    def exec_module(self, module):\n        pass\n\n    def load_module(self, fullname):\n        \"\"\"Load a namespace module.\n\n        This method is deprecated.  Use exec_module() instead.\n\n        \"\"\"\n        # The import system never calls this method.\n        _bootstrap._verbose_message('namespace module loaded with path {!r}',\n                                    self._path)\n        # Warning implemented in _load_module_shim().\n        return _bootstrap._load_module_shim(self, fullname)\n\n    def get_resource_reader(self, module):\n        from importlib.readers import NamespaceReader\n        return NamespaceReader(self._path)\n\n\n# We use this exclusively in module_from_spec() for backward-compatibility.\n_NamespaceLoader = NamespaceLoader\n\n\n# Finders #####################################################################\n\nclass PathFinder:\n\n    \"\"\"Meta path finder for sys.path and package __path__ attributes.\"\"\"\n\n    @staticmethod\n    def invalidate_caches():\n        \"\"\"Call the invalidate_caches() method on all path entry finders\n        stored in sys.path_importer_caches (where implemented).\"\"\"\n        for name, finder in list(sys.path_importer_cache.items()):\n            # Drop entry if finder name is a relative path. The current\n            # working directory may have changed.\n            if finder is None or not _path_isabs(name):\n                del sys.path_importer_cache[name]\n            elif hasattr(finder, 'invalidate_caches'):\n                finder.invalidate_caches()\n        # Also invalidate the caches of _NamespacePaths\n        # https://bugs.python.org/issue45703\n        _NamespacePath._epoch += 1\n\n        from importlib.metadata import MetadataPathFinder\n        MetadataPathFinder.invalidate_caches()\n\n    @staticmethod\n    def _path_hooks(path):\n        \"\"\"Search sys.path_hooks for a finder for 'path'.\"\"\"\n        if sys.path_hooks is not None and not sys.path_hooks:\n            _warnings.warn('sys.path_hooks is empty', ImportWarning)\n        for hook in sys.path_hooks:\n            try:\n                return hook(path)\n            except ImportError:\n                continue\n        else:\n            return None\n\n    @classmethod\n    def _path_importer_cache(cls, path):\n        \"\"\"Get the finder for the path entry from sys.path_importer_cache.\n\n        If the path entry is not in the cache, find the appropriate finder\n        and cache it. If no finder is available, store None.\n\n        \"\"\"\n        if path == '':\n            try:\n                path = _os.getcwd()\n            except FileNotFoundError:\n                # Don't cache the failure as the cwd can easily change to\n                # a valid directory later on.\n                return None\n        try:\n            finder = sys.path_importer_cache[path]\n        except KeyError:\n            finder = cls._path_hooks(path)\n            sys.path_importer_cache[path] = finder\n        return finder\n\n    @classmethod\n    def _get_spec(cls, fullname, path, target=None):\n        \"\"\"Find the loader or namespace_path for this module/package name.\"\"\"\n        # If this ends up being a namespace package, namespace_path is\n        #  the list of paths that will become its __path__\n        namespace_path = []\n        for entry in path:\n            if not isinstance(entry, str):\n                continue\n            finder = cls._path_importer_cache(entry)\n            if finder is not None:\n                spec = finder.find_spec(fullname, target)\n                if spec is None:\n                    continue\n                if spec.loader is not None:\n                    return spec\n                portions = spec.submodule_search_locations\n                if portions is None:\n                    raise ImportError('spec missing loader')\n                # This is possibly part of a namespace package.\n                #  Remember these path entries (if any) for when we\n                #  create a namespace package, and continue iterating\n                #  on path.\n                namespace_path.extend(portions)\n        else:\n            spec = _bootstrap.ModuleSpec(fullname, None)\n            spec.submodule_search_locations = namespace_path\n            return spec\n\n    @classmethod\n    def find_spec(cls, fullname, path=None, target=None):\n        \"\"\"Try to find a spec for 'fullname' on sys.path or 'path'.\n\n        The search is based on sys.path_hooks and sys.path_importer_cache.\n        \"\"\"\n        if path is None:\n            path = sys.path\n        spec = cls._get_spec(fullname, path, target)\n        if spec is None:\n            return None\n        elif spec.loader is None:\n            namespace_path = spec.submodule_search_locations\n            if namespace_path:\n                # We found at least one namespace path.  Return a spec which\n                # can create the namespace package.\n                spec.origin = None\n                spec.submodule_search_locations = _NamespacePath(fullname, namespace_path, cls._get_spec)\n                return spec\n            else:\n                return None\n        else:\n            return spec\n\n    @staticmethod\n    def find_distributions(*args, **kwargs):\n        \"\"\"\n        Find distributions.\n\n        Return an iterable of all Distribution instances capable of\n        loading the metadata for packages matching ``context.name``\n        (or all names if ``None`` indicated) along the paths in the list\n        of directories ``context.path``.\n        \"\"\"\n        from importlib.metadata import MetadataPathFinder\n        return MetadataPathFinder.find_distributions(*args, **kwargs)\n\n\nclass FileFinder:\n\n    \"\"\"File-based finder.\n\n    Interactions with the file system are cached for performance, being\n    refreshed when the directory the finder is handling has been modified.\n\n    \"\"\"\n\n    def __init__(self, path, *loader_details):\n        \"\"\"Initialize with the path to search on and a variable number of\n        2-tuples containing the loader and the file suffixes the loader\n        recognizes.\"\"\"\n        loaders = []\n        for loader, suffixes in loader_details:\n            loaders.extend((suffix, loader) for suffix in suffixes)\n        self._loaders = loaders\n        # Base (directory) path\n        if not path or path == '.':\n            self.path = _os.getcwd()\n        else:\n            self.path = _path_abspath(path)\n        self._path_mtime = -1\n        self._path_cache = set()\n        self._relaxed_path_cache = set()\n\n    def invalidate_caches(self):\n        \"\"\"Invalidate the directory mtime.\"\"\"\n        self._path_mtime = -1\n\n    def _get_spec(self, loader_class, fullname, path, smsl, target):\n        loader = loader_class(fullname, path)\n        return spec_from_file_location(fullname, path, loader=loader,\n                                       submodule_search_locations=smsl)\n\n    def find_spec(self, fullname, target=None):\n        \"\"\"Try to find a spec for the specified module.\n\n        Returns the matching spec, or None if not found.\n        \"\"\"\n        is_namespace = False\n        tail_module = fullname.rpartition('.')[2]\n        try:\n            mtime = _path_stat(self.path or _os.getcwd()).st_mtime\n        except OSError:\n            mtime = -1\n        if mtime != self._path_mtime:\n            self._fill_cache()\n            self._path_mtime = mtime\n        # tail_module keeps the original casing, for __file__ and friends\n        if _relax_case():\n            cache = self._relaxed_path_cache\n            cache_module = tail_module.lower()\n        else:\n            cache = self._path_cache\n            cache_module = tail_module\n        # Check if the module is the name of a directory (and thus a package).\n        if cache_module in cache:\n            base_path = _path_join(self.path, tail_module)\n            for suffix, loader_class in self._loaders:\n                init_filename = '__init__' + suffix\n                full_path = _path_join(base_path, init_filename)\n                if _path_isfile(full_path):\n                    return self._get_spec(loader_class, fullname, full_path, [base_path], target)\n            else:\n                # If a namespace package, return the path if we don't\n                #  find a module in the next section.\n                is_namespace = _path_isdir(base_path)\n        # Check for a file w/ a proper suffix exists.\n        for suffix, loader_class in self._loaders:\n            try:\n                full_path = _path_join(self.path, tail_module + suffix)\n            except ValueError:\n                return None\n            _bootstrap._verbose_message('trying {}', full_path, verbosity=2)\n            if cache_module + suffix in cache:\n                if _path_isfile(full_path):\n                    return self._get_spec(loader_class, fullname, full_path,\n                                          None, target)\n        if is_namespace:\n            _bootstrap._verbose_message('possible namespace for {}', base_path)\n            spec = _bootstrap.ModuleSpec(fullname, None)\n            spec.submodule_search_locations = [base_path]\n            return spec\n        return None\n\n    def _fill_cache(self):\n        \"\"\"Fill the cache of potential modules and packages for this directory.\"\"\"\n        path = self.path\n        try:\n            contents = _os.listdir(path or _os.getcwd())\n        except (FileNotFoundError, PermissionError, NotADirectoryError):\n            # Directory has either been removed, turned into a file, or made\n            # unreadable.\n            contents = []\n        # We store two cached versions, to handle runtime changes of the\n        # PYTHONCASEOK environment variable.\n        if not sys.platform.startswith('win'):\n            self._path_cache = set(contents)\n        else:\n            # Windows users can import modules with case-insensitive file\n            # suffixes (for legacy reasons). Make the suffix lowercase here\n            # so it's done once instead of for every import. This is safe as\n            # the specified suffixes to check against are always specified in a\n            # case-sensitive manner.\n            lower_suffix_contents = set()\n            for item in contents:\n                name, dot, suffix = item.partition('.')\n                if dot:\n                    new_name = f'{name}.{suffix.lower()}'\n                else:\n                    new_name = name\n                lower_suffix_contents.add(new_name)\n            self._path_cache = lower_suffix_contents\n        if sys.platform.startswith(_CASE_INSENSITIVE_PLATFORMS):\n            self._relaxed_path_cache = {fn.lower() for fn in contents}\n\n    @classmethod\n    def path_hook(cls, *loader_details):\n        \"\"\"A class method which returns a closure to use on sys.path_hook\n        which will return an instance using the specified loaders and the path\n        called on the closure.\n\n        If the path called on the closure is not a directory, ImportError is\n        raised.\n\n        \"\"\"\n        def path_hook_for_FileFinder(path):\n            \"\"\"Path hook for importlib.machinery.FileFinder.\"\"\"\n            if not _path_isdir(path):\n                raise ImportError('only directories are supported', path=path)\n            return cls(path, *loader_details)\n\n        return path_hook_for_FileFinder\n\n    def __repr__(self):\n        return f'FileFinder({self.path!r})'\n\n\n# Import setup ###############################################################\n\ndef _fix_up_module(ns, name, pathname, cpathname=None):\n    # This function is used by PyImport_ExecCodeModuleObject().\n    loader = ns.get('__loader__')\n    spec = ns.get('__spec__')\n    if not loader:\n        if spec:\n            loader = spec.loader\n        elif pathname == cpathname:\n            loader = SourcelessFileLoader(name, pathname)\n        else:\n            loader = SourceFileLoader(name, pathname)\n    if not spec:\n        spec = spec_from_file_location(name, pathname, loader=loader)\n        if cpathname:\n            spec.cached = _path_abspath(cpathname)\n    try:\n        ns['__spec__'] = spec\n        ns['__loader__'] = loader\n        ns['__file__'] = pathname\n        ns['__cached__'] = cpathname\n    except Exception:\n        # Not important enough to report.\n        pass\n\n\ndef _get_supported_file_loaders():\n    \"\"\"Returns a list of file-based module loaders.\n\n    Each item is a tuple (loader, suffixes).\n    \"\"\"\n    extensions = ExtensionFileLoader, _imp.extension_suffixes()\n    source = SourceFileLoader, SOURCE_SUFFIXES\n    bytecode = SourcelessFileLoader, BYTECODE_SUFFIXES\n    return [extensions, source, bytecode]\n\n\ndef _set_bootstrap_module(_bootstrap_module):\n    global _bootstrap\n    _bootstrap = _bootstrap_module\n\n\ndef _install(_bootstrap_module):\n    \"\"\"Install the path-based import components.\"\"\"\n    _set_bootstrap_module(_bootstrap_module)\n    supported_loaders = _get_supported_file_loaders()\n    sys.path_hooks.extend([FileFinder.path_hook(*supported_loaders)])\n    sys.meta_path.append(PathFinder)\n", 1749], "/root/miniconda3/envs/gs-lightning/lib/python3.12/site-packages/__editable___torch_2_9_0a0_git7cc5d03_finder.py": ["from __future__ import annotations\nimport sys\nfrom importlib.machinery import ModuleSpec, PathFinder\nfrom importlib.machinery import all_suffixes as module_suffixes\nfrom importlib.util import spec_from_file_location\nfrom itertools import chain\nfrom pathlib import Path\n\nMAPPING: dict[str, str] = {'functorch': '/data/wangyingqi/code/pytorch/functorch', 'torch': '/data/wangyingqi/code/pytorch/torch', 'torchgen': '/data/wangyingqi/code/pytorch/torchgen'}\nNAMESPACES: dict[str, list[str]] = {}\nPATH_PLACEHOLDER = '__editable__.torch-2.9.0a0+git7cc5d03.finder' + \".__path_hook__\"\n\n\nclass _EditableFinder:  # MetaPathFinder\n    @classmethod\n    def find_spec(cls, fullname: str, path=None, target=None) -> ModuleSpec | None:  # type: ignore\n        # Top-level packages and modules (we know these exist in the FS)\n        if fullname in MAPPING:\n            pkg_path = MAPPING[fullname]\n            return cls._find_spec(fullname, Path(pkg_path))\n\n        # Handle immediate children modules (required for namespaces to work)\n        # To avoid problems with case sensitivity in the file system we delegate\n        # to the importlib.machinery implementation.\n        parent, _, child = fullname.rpartition(\".\")\n        if parent and parent in MAPPING:\n            return PathFinder.find_spec(fullname, path=[MAPPING[parent]])\n\n        # Other levels of nesting should be handled automatically by importlib\n        # using the parent path.\n        return None\n\n    @classmethod\n    def _find_spec(cls, fullname: str, candidate_path: Path) -> ModuleSpec | None:\n        init = candidate_path / \"__init__.py\"\n        candidates = (candidate_path.with_suffix(x) for x in module_suffixes())\n        for candidate in chain([init], candidates):\n            if candidate.exists():\n                return spec_from_file_location(fullname, candidate)\n        return None\n\n\nclass _EditableNamespaceFinder:  # PathEntryFinder\n    @classmethod\n    def _path_hook(cls, path) -> type[_EditableNamespaceFinder]:\n        if path == PATH_PLACEHOLDER:\n            return cls\n        raise ImportError\n\n    @classmethod\n    def _paths(cls, fullname: str) -> list[str]:\n        paths = NAMESPACES[fullname]\n        if not paths and fullname in MAPPING:\n            paths = [MAPPING[fullname]]\n        # Always add placeholder, for 2 reasons:\n        # 1. __path__ cannot be empty for the spec to be considered namespace.\n        # 2. In the case of nested namespaces, we need to force\n        #    import machinery to query _EditableNamespaceFinder again.\n        return [*paths, PATH_PLACEHOLDER]\n\n    @classmethod\n    def find_spec(cls, fullname: str, target=None) -> ModuleSpec | None:  # type: ignore\n        if fullname in NAMESPACES:\n            spec = ModuleSpec(fullname, None, is_package=True)\n            spec.submodule_search_locations = cls._paths(fullname)\n            return spec\n        return None\n\n    @classmethod\n    def find_module(cls, _fullname) -> None:\n        return None\n\n\ndef install():\n    if not any(finder == _EditableFinder for finder in sys.meta_path):\n        sys.meta_path.append(_EditableFinder)\n\n    if not NAMESPACES:\n        return\n\n    if not any(hook == _EditableNamespaceFinder._path_hook for hook in sys.path_hooks):\n        # PathEntryFinder is needed to create NamespaceSpec without private APIS\n        sys.path_hooks.append(_EditableNamespaceFinder._path_hook)\n    if PATH_PLACEHOLDER not in sys.path:\n        sys.path.append(PATH_PLACEHOLDER)  # Used just to trigger the path hook\n", 85], "/data/wangyingqi/code/pytorch/torch/_dynamo/utils.py": ["# mypy: allow-untyped-defs\n\n\"\"\"\nUtility functions and classes used throughout the TorchDynamo system.\n\nThis module contains a collection of helper utilities used by various parts of Dynamo for:\n- Performance metrics collection and reporting\n- Compilation timing and debugging\n- Graph manipulation and tensor operations\n- Runtime guards and checks\n- Common data structure operations\n- Testing and development tools\n\nThis is an internal module that provides shared functionality used across the Dynamo codebase.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport atexit\nimport collections\nimport contextlib\nimport copy\nimport dataclasses\nimport datetime\nimport dis\nimport enum\nimport functools\nimport gc\nimport importlib\nimport inspect\nimport itertools\nimport json\nimport linecache\nimport logging\nimport math\nimport operator\nimport os\nimport re\nimport sys\nimport textwrap\nimport threading\nimport time\nimport traceback\nimport types\nimport typing\nimport uuid\nimport warnings\nimport weakref\nfrom collections import Counter, OrderedDict\nfrom contextlib import AbstractContextManager, contextmanager\nfrom dataclasses import is_dataclass\nfrom functools import lru_cache\nfrom types import CodeType, MethodWrapperType\nfrom typing import (\n    Any,\n    Callable,\n    cast,\n    ClassVar,\n    Generic,\n    Optional,\n    overload,\n    TypeVar,\n    Union,\n)\nfrom typing_extensions import Literal, TypeAlias, TypeGuard, TypeIs\n\nimport torch\nimport torch._functorch.config\nimport torch.fx.experimental.symbolic_shapes\nimport torch.utils._pytree as pytree\nfrom torch import fx\nfrom torch._C import (\n    _instruction_counter,\n    _len_torch_function_stack,\n    _pop_torch_function_stack,\n    _push_on_torch_function_stack,\n)\nfrom torch._dispatch.python import enable_python_dispatcher\nfrom torch._dynamo.metrics_context import MetricsContext, RuntimeMetricsContext\nfrom torch._guards import CompileId, Source, TracingContext\nfrom torch._subclasses.meta_utils import is_sparse_compressed\nfrom torch._utils_internal import (\n    justknobs_check,\n    log_chromium_event_internal,\n    log_compilation_event,\n    record_chromium_event_internal,\n    signpost_event,\n)\nfrom torch.fx._utils import _format_graph_code, lazy_format_graph_code\nfrom torch.monitor import _WaitCounter\nfrom torch.nn.modules.lazy import LazyModuleMixin\nfrom torch.utils._triton import has_triton, has_triton_package\nfrom torch.utils.hooks import RemovableHandle\n\nfrom .graph_utils import _get_flat_args\n\n\nif typing.TYPE_CHECKING:\n    from collections.abc import (\n        Generator,\n        ItemsView,\n        Iterable,\n        Iterator,\n        KeysView,\n        ValuesView,\n    )\n\n\ntry:\n    import numpy as np\nexcept ModuleNotFoundError:\n    np = None  # type: ignore[assignment]\n\ntry:\n    import torch._logging\n    import torch._numpy as tnp\n    from torch._guards import detect_fake_mode  # noqa: F401\n    from torch._logging import LazyString\n\n    from . import config\n\n    # NOTE: Make sure `NP_SUPPORTED_MODULES` and `NP_TO_TNP_MODULE` are in sync.\n    if np:\n        NP_SUPPORTED_MODULES: tuple[types.ModuleType, ...] = (\n            np,\n            np.fft,\n            np.linalg,\n            np.random,\n        )\n\n        NP_TO_TNP_MODULE = {\n            np: tnp,\n            np.fft: tnp.fft,\n            np.linalg: tnp.linalg,\n            np.random: tnp.random,\n        }\n    else:\n        NP_SUPPORTED_MODULES = ()\n\n        NP_TO_TNP_MODULE = {}\n    from torch._subclasses.fake_tensor import FakeTensor, is_fake, maybe_get_fake_mode\nexcept ImportError:\n    pass\n\n\nT = TypeVar(\"T\")\n\nunpatched_nn_module_getattr = torch.nn.Module.__getattr__\nunpatched_nn_module_call = torch.nn.Module.__call__\nunpatched_nn_module_call_impl = torch.nn.Module._call_impl\n\ncounters: collections.defaultdict[str, Counter[str]] = collections.defaultdict(\n    collections.Counter\n)\noptimus_scuba_log: dict[str, Any] = {}\ntroubleshooting_url = (\n    \"https://pytorch.org/docs/main/torch.compiler_troubleshooting.html\"\n)\nnnmodule_doc_url = \"https://pytorch.org/docs/main/torch.compiler_nn_module.html\"\nnnmodule_doc_url_msg = f\"See {nnmodule_doc_url} for more information and limitations.\"\nlog = logging.getLogger(__name__)\n\n# profiling compilation time by function\ncompilation_time_metrics: dict[str, list[float]] = {}\n\n# This supports calculate_time_spent(), which reports cumulative times\n# across the process for any \"phase\" populated by dynamo_timed. Reset if\n# reset_frame_count() is called.\ncumulative_time_spent_ns: dict[str, float] = collections.defaultdict(float)\n\ntimer_counter = itertools.count()\n\n\n# Abstraction on top of counters.\nclass ReInplaceTrigger(enum.Enum):\n    AUTO_FUNC_V1 = 1\n    AUTO_FUNC_V2 = 2\n    TRITON_OPS = 3\n\n\nclass ReinplaceCounters:\n    _values: collections.defaultdict[str, int] = collections.defaultdict(int)\n\n    # Track sizes of known not re-inplaced tensors (exclude dynamic shapes).\n    @classmethod\n    def add_missed_bytes(cls, trigger: ReInplaceTrigger, bytes: int):\n        if bytes != 0:\n            cls._values[f\"missed_bytes_{trigger.name}\"] += bytes\n\n    # Track number of not re-inplaced tensors.\n    @classmethod\n    def add_missed_opportunities(cls, trigger: ReInplaceTrigger, count: int):\n        if count != 0:\n            cls._values[f\"missed_tensors_{trigger}\"] += count\n\n    @classmethod\n    def clear(cls):\n        cls._values.clear()\n\n    @classmethod\n    def get_total_missed(cls):\n        sum = 0\n        for trigger in ReInplaceTrigger:\n            sum += cls._values.get(f\"missed_tensors_{trigger}\", 0)\n        return sum\n\n    @classmethod\n    def get_total_missed_bytes(cls):\n        sum = 0\n        for trigger in ReInplaceTrigger:\n            sum += cls._values.get(f\"missed_bytes_{trigger.name}\", 0)\n        return sum\n\n    @classmethod\n    def log(cls):\n        # if not empty log.\n        if cls._values:\n            signpost_event(\"inductor\", \"reinplace_counters\", cls._values)\n\n\ndef tabulate(\n    rows: Union[list[tuple[str, object]], list[list[object]]],\n    headers: Union[tuple[str, ...], list[str]],\n) -> str:\n    try:\n        import tabulate\n\n        return tabulate.tabulate(rows, headers=headers)\n    except ImportError:\n        return \"\\n\".join(\n            \", \".join(map(str, row)) for row in itertools.chain([headers], rows)\n        )\n\n\ncurr_frame = 0\n\n\n# Note: Called for you by dynamo - you almost never ever want to invoke this yourself.\ndef increment_frame() -> None:\n    global curr_frame\n    curr_frame = curr_frame + 1\n\n\n# Note: Called for you by dynamo - you almost never ever want to invoke this yourself.\ndef reset_frame_count() -> None:\n    global curr_frame\n    cumulative_time_spent_ns.clear()\n    compilation_time_metrics.clear()\n    curr_frame = 0\n\n\n_recompile_user_contexts: Optional[list[Callable[[], str]]] = None\n\n\ndef register_hook_for_recompile_user_context(hook: Callable[[], str]) -> None:\n    \"\"\"\n    Register a hook to be called when a recompile is triggered. The hook\n    should return a string describing user contexts that are not available\n    to the compiler, such as the current training epoch. This is useful for\n    debugging and data analysis for recompile. For data retention purposes,\n    the user context string is capped at 256 characters.\n    \"\"\"\n    global _recompile_user_contexts\n    if _recompile_user_contexts is None:\n        _recompile_user_contexts = []\n    _recompile_user_contexts.append(hook)\n\n\ndef get_hook_for_recompile_user_context() -> Optional[list[Callable[[], str]]]:\n    return _recompile_user_contexts\n\n\nop_count = 0\n\n\ndef increment_op_count(cnt: int) -> None:\n    global op_count\n    op_count += cnt\n\n\n# Get the total time in seconds for each \"phase\"\n# For example, {'entire_frame_compile':8.574629999999999, 'backend_compile':5.26806}\ndef calculate_time_spent() -> dict[str, float]:\n    total_by_key = {}\n    for phase, timing in cumulative_time_spent_ns.items():\n        total_by_key[phase] = timing / 1e9\n\n    total_by_key[\"total_wall_time\"] = total_by_key.get(\n        \"entire_frame_compile\", 0\n    ) + total_by_key.get(\"entire_backward_compile\", 0)\n    return total_by_key\n\n\n# Print a report of time spent so far\n# Ex:\n# TIMING:\n# entire_frame_compile:8.574629999999999\n# backend_compile:5.26806\ndef print_time_report() -> None:\n    total_by_key = calculate_time_spent()\n\n    out = \"TIMING:\"\n    for key, value in total_by_key.items():\n        out = f\"{out} {key}:{round(value, 5)}\"\n\n    print(out)\n\n\n# Use the following singleton to capture and log CompilationMetrics. Entering the context\n# manager allocates a new record to be logged when it exits. (You should not need to use\n# this directly unless you introduce a new code path where compilation metrics would be\n# gathered). While compiling, use the setters or timer in MetricsContext to update fields\n# in the current context. For example:\n#\n# To set a single field once (use overwrite=True to overwrite):\n#   get_metrics_context().set(\"metric_name\", value)\n#\n# To set multiple fields at once (use overwrite=True to overwrite):\n#   get_metrics_context().update({\"name1\": val1, \"name2\": val2})\n#\n# To increment an integer field:\n#   get_metrics_context().increment(\"metric_name\", value)\n#\n# To record execution time, MetricsContext works with dynamo_timed:\n#    def foo(...):\n#        # Updates the \"metric_us\" field.\n#        with dynamo_timed(\"metric\", dynamo_compile_column_us=\"metric_us\")\n#            ...\n#\n_METRICS_CONTEXT: MetricsContext\n_RUNTIME_METRICS_CONTEXT: RuntimeMetricsContext\n\n\ndef get_metrics_context() -> MetricsContext:\n    return _METRICS_CONTEXT\n\n\ndef get_runtime_metrics_context() -> RuntimeMetricsContext:\n    return _RUNTIME_METRICS_CONTEXT\n\n\nclass CompileEventLogLevel(enum.Enum):\n    \"\"\"\n    Enum that loosely corresponds with a \"log level\" of a given event.\n\n    CHROMIUM_EVENT: Logs only to tlparse.\n    COMPILE_EVENT: Logs to tlparse + PT2 Compile Events\n    COMPILATION_METRIC: Logs to tlparse, PT2 Compile Events, and dynamo_compile\n    \"\"\"\n\n    CHROMIUM = 1\n    PT2_COMPILE = 2\n    COMPILATION_METRIC = 3\n\n\nclass CompileEventLogger:\n    \"\"\"\n    Helper class for representing adding metadata(i.e. columns) to various compile events.\n    Use CompileEventLogger to add event data to:\n    - Chromium events\n    - PT2 Compile Events\n    - CompilationMetrics\n\n    This should be used in conjunction with dynamo_timed() and metrics contexts, which create\n    timed spans and events. CompileEventLogger uses three log levels (described in CompileEventLogLevel),\n    where each log level logs to all sources below it in the hierarchy.\n\n    Example usages:\n    - I want to log to an existing chromium event within dynamo timed:\n    with dynamo_timed(\"my_event\"):\n        CompileEventLogger.chromium(\"my_event\", foo=bar)\n\n    - I want to log my event to both chromium + pt2_compile_events:\n    with dynamo_timed(\"my_event\", log_pt2_compile_event=True):\n        CompileEventLogger.pt2_compile(\"my_event\", foo=bar)\n\n    - I want to add information to dynamo events and dynamo_compile\n        CompileEventLogger.compilation_metric(foo=bar)\n    \"\"\"\n\n    @staticmethod\n    def log_instant_event(\n        event_name: str,\n        metadata: dict[str, Any],\n        time_ns: Optional[int] = None,\n        log_level: CompileEventLogLevel = CompileEventLogLevel.CHROMIUM,\n    ):\n        if time_ns is None:\n            time_ns = time.time_ns()\n        chromium_log = get_chromium_event_logger()\n        if log_level == CompileEventLogLevel.CHROMIUM:\n            log_pt2_compile_event = False\n        elif log_level == CompileEventLogLevel.PT2_COMPILE:\n            log_pt2_compile_event = True\n        else:\n            raise RuntimeError(\n                \"Cannot log instant event at COMPILATION_METRIC level. Please choose one of CHROMIUM_EVENT or COMPILE_EVENT\"\n            )\n        chromium_log.log_instant_event(\n            event_name, time_ns, metadata, log_pt2_compile_event\n        )\n\n    @staticmethod\n    def add_data(\n        event_name: str,\n        log_level: CompileEventLogLevel,\n        overwrite: bool = False,\n        **metadata: object,\n    ):\n        \"\"\"\n        Centralized API for adding data to various events\n        Log an event to a toplevel \"dynamo\" event or metrics context\n        depending on log level.\n        \"\"\"\n        chromium_log = get_chromium_event_logger()\n        pt2_compile_substack = chromium_log.get_pt2_compile_substack()\n\n        if log_level == CompileEventLogLevel.CHROMIUM:\n            chromium_log.add_event_data(event_name, **metadata)\n        elif log_level == CompileEventLogLevel.PT2_COMPILE:\n            pt2_compile_substack = chromium_log.get_pt2_compile_substack()\n            if event_name not in pt2_compile_substack:\n                raise RuntimeError(\n                    \"Error: specified log level PT2_COMPILE, but the event %s\"\n                    \" is not logged to pt2_compile_events. Make sure the event is active and you passed \"\n                    \"log_pt2_compile_event=True to dynamo_timed\",\n                    event_name,\n                )\n            chromium_log.add_event_data(event_name, **metadata)\n        else:\n            assert log_level == CompileEventLogLevel.COMPILATION_METRIC\n            top_event = chromium_log.get_outermost_event()\n\n            if event_name != top_event:\n                raise RuntimeError(\n                    \"Log level is COMPILATION_METRIC, but event_name isn't the toplevel event. \"\n                    \"CompilationMetrics must be logged to the toplevel event. Consider using `log_toplevel_event_data` directly.\"\n                )\n            metrics_context = get_metrics_context()\n            if not metrics_context.in_progress():\n                raise RuntimeError(\n                    \"No metrics context is in progress. Please only call this function within a metrics context.\"\n                )\n\n            # TODO: should we assert that the keys of metadata are in CompilationMetrics?\n            metrics_context.update(metadata, overwrite)\n            chromium_log.add_event_data(event_name, **metadata)\n\n    @staticmethod\n    def add_toplevel(\n        log_level: CompileEventLogLevel, overwrite: bool = False, **metadata: object\n    ):\n        \"\"\"\n        Syntactic sugar for logging to the toplevel event\n        \"\"\"\n        top_event = get_chromium_event_logger().get_outermost_event()\n        if top_event is None:\n            raise RuntimeError(\n                \"No toplevel event active. Please only call this function within a dynamo_timed context.\"\n            )\n        CompileEventLogger.add_data(top_event, log_level, overwrite, **metadata)\n\n    @staticmethod\n    def increment(\n        event_name: str, log_level: CompileEventLogLevel, key: str, value: int\n    ):\n        \"\"\"\n        Increments an existing field, or adds it\n        \"\"\"\n        chromium_log = get_chromium_event_logger()\n        if (\n            log_level == CompileEventLogLevel.CHROMIUM\n            or log_level == CompileEventLogLevel.PT2_COMPILE\n        ):\n            chromium_log.increment(event_name, key, value)\n        else:\n            assert log_level == CompileEventLogLevel.COMPILATION_METRIC\n            top_event = chromium_log.get_outermost_event()\n            if event_name != top_event:\n                raise RuntimeError(\n                    \"Log level is COMPILATION_METRIC, but event_name isn't the toplevel event. \"\n                    \"CompilationMetrics must be logged to the toplevel event. Consider using `increment_toplevel` directly.\"\n                )\n\n            metrics_context = get_metrics_context()\n            if not metrics_context.in_progress():\n                raise RuntimeError(\n                    \"No metrics context is in progress. Please only call this function within a metrics context/dynamo_timed.\"\n                )\n\n            metrics_context.increment(key, value)\n            chromium_log.increment(event_name, key, value)\n\n    @staticmethod\n    def increment_toplevel(\n        key: str,\n        value: int = 1,\n        log_level: CompileEventLogLevel = CompileEventLogLevel.COMPILATION_METRIC,\n    ):\n        \"\"\"\n        Increments a value on the toplevel metric. By default, logs to metric.\n        \"\"\"\n        chromium_log = get_chromium_event_logger()\n        top_event = chromium_log.get_outermost_event()\n        if top_event is None:\n            raise RuntimeError(\n                \"No toplevel event active. Please only call this function within a metrics context/dynamo_timed.\"\n            )\n        CompileEventLogger.increment(top_event, log_level, key, value)\n\n    @staticmethod\n    def add_to_set(\n        event_name: str, log_level: CompileEventLogLevel, key: str, value: Any\n    ):\n        \"\"\"\n        Add metadata <value> to a set of values with key <key>. Creates a set if it doesn't exist.\n        \"\"\"\n        chromium_log = get_chromium_event_logger()\n        if (\n            log_level == CompileEventLogLevel.CHROMIUM\n            or log_level == CompileEventLogLevel.PT2_COMPILE\n        ):\n            chromium_log.add_to_set(event_name, key, value)\n        else:\n            assert log_level == CompileEventLogLevel.COMPILATION_METRIC\n            top_event = chromium_log.get_outermost_event()\n            if event_name != top_event:\n                raise RuntimeError(\n                    \"Log level is COMPILATION_METRIC, but event_name isn't the toplevel event. \"\n                    \"CompilationMetrics must be logged to the toplevel event. Consider using `add_to_set_metric` directly.\"\n                )\n\n            metrics_context = get_metrics_context()\n            if not metrics_context.in_progress():\n                raise RuntimeError(\n                    \"No metrics context is in progress. Please only call this function within a metrics context/dynamo_timed.\"\n                )\n\n            metrics_context.add_to_set(key, value)\n            chromium_log.add_to_set(event_name, key, value)\n\n    @staticmethod\n    def add_to_set_toplevel(\n        key: str,\n        value: Any,\n        log_level: CompileEventLogLevel = CompileEventLogLevel.COMPILATION_METRIC,\n    ):\n        \"\"\"\n        Same as add to set, just does it automatically to the toplevel event instead of having to explicitly name it.\n        Defaults to COMPILATION_METRIC log level.\n        \"\"\"\n        chromium_log = get_chromium_event_logger()\n        top_event = chromium_log.get_outermost_event()\n        if top_event is None:\n            raise RuntimeError(\n                \"No toplevel event active. Please only call this function within a metrics context/dynamo_timed.\"\n            )\n        CompileEventLogger.add_to_set(top_event, log_level, key, value)\n\n    # Helper functions that are syntactic sugar\n\n    @staticmethod\n    def chromium(event_name: str, **metadata: object):\n        \"\"\"\n        Add <metadata> to <event_name> in chromium. Each key/value of metadata will appear in the chromium trace.\n        <event_name> should be the name of a timed event span passed to `dynamo_timed`.\n        \"\"\"\n        CompileEventLogger.add_data(\n            event_name, CompileEventLogLevel.CHROMIUM, overwrite=False, **metadata\n        )\n\n    @staticmethod\n    def pt2_compile(event_name: str, **metadata: object):\n        \"\"\"\n        Add <metadata> to <event_name> in chromium and PT2 Compile Events.\n        Each key/value of metadata will appear in the chromium trace. Each kwarg name becomes\n        a column in PT2 Compile Events, with the corresponding kwarg value.\n        <event_name> should be the name of a timed event span passed to `dynamo_timed`,\n        with log_to_pt2_compile_events=True.\n        \"\"\"\n        CompileEventLogger.add_data(\n            event_name, CompileEventLogLevel.PT2_COMPILE, overwrite=False, **metadata\n        )\n\n    @staticmethod\n    def compilation_metric(overwrite: bool = False, **metadata: object):\n        \"\"\"\n        Add <metadata> to the CompilationMetrics context. Also logs to PT2 Compile Events\n        and chromium.\n        Each key/value of metadata will appear in the chromium trace. Each kwarg name becomes\n        a column in PT2 Compile Events and Dynamo Compile, with the corresponding kwarg value.\n        \"\"\"\n        CompileEventLogger.add_toplevel(\n            CompileEventLogLevel.COMPILATION_METRIC, overwrite, **metadata\n        )\n\n    @staticmethod\n    def instant(\n        event_name: str, metadata: dict[str, Any], time_ns: Optional[int] = None\n    ):\n        \"\"\"\n        Log an instant event to chromium logs with name <event_name> at time <time_ns>. The `args` field in\n        Perfetto will point to metadata. <time_ns> should be a value obtained from time.time_ns().\n        \"\"\"\n        CompileEventLogger.log_instant_event(\n            event_name, metadata, time_ns, CompileEventLogLevel.CHROMIUM\n        )\n\n    @staticmethod\n    def try_add_pt2_compile(event_name: str, **metadata: object):\n        \"\"\"\n        Adds to an existing pt2_compile event, but silently returns if the event doesn't exist\n        or ChromiumEventLogger is not initialized.\n        This function is syntactic sugar for chromium_event_logger().try_add_event_data.\n        \"\"\"\n        if not chromium_event_log_active():\n            return\n        chromium_log = get_chromium_event_logger()\n        chromium_log.try_add_event_data(event_name, **metadata)\n\n    @staticmethod\n    def try_(method_fn, *args, **kwargs):\n        \"\"\"\n        Special function that quietly runs a given method, returning if CHROMIUM_EVENT_LOG is None or metrics context is not set\n        \"\"\"\n        if not chromium_event_log_active():\n            return\n        metrics_context = get_metrics_context()\n        if not metrics_context.in_progress():\n            return\n        method_fn(*args, **kwargs)\n\n\n_dynamo_timed_tls = threading.local()\n\n\n@contextmanager\ndef dynamo_timed(\n    key: str,\n    # TODO(masneral): Deprecate this param.\n    phase_name: Optional[str] = None,\n    log_pt2_compile_event: bool = False,\n    metadata: Optional[dict[str, object]] = None,\n    dynamo_compile_column_us: Optional[str] = None,\n    compile_id: Optional[CompileId] = None,\n    is_backward: Optional[bool] = None,\n    log_waitcounter: bool = False,\n    waitcounter_name_override: Optional[str] = None,\n) -> Generator[Any, None, None]:\n    \"\"\"\n    dynamo_timed is a context manager\n    By wrapping a function in dynamo_timed, we can get a few things:\n\n    1) Optionally log timings to pt2_compile_events.\n    2) Optionally log timings to CompilationMetrics (dynamo_compile).\n    3) Optionally log chromium events.\n    4) Optionally increment a WaitCounter.\n    5) Store a record in compilation_time_metrics\n       For example:\n\n        def _foo(...):\n            with dynamo_timed(\"_foo\"):\n                ...\n\n        Would show up as an entry in our timing dict:\n        OrderedDict([('_foo', [0.083690, 0.23949, 3.1425e-05])])\n        This is extremely useful for granular debugging.\n\n    Although it is tempting to use dynamo_timed as a decorator, please do not.\n    In its decorator form it makes cProfile traces less useful as dynamo_timed\n    suddenly becomes a bottleneck for lots of function calls (as only one parent\n    pointer is recorded).\n\n    Params:\n    - key: key into compile_time_metrics. If phase_name is not provided, this is\n      also the event name used for pt2_compile_events logs and chromium events.\n    - phase_name: Optional override for the event name.\n    - log_pt2_compile_event: Whether to log a pt2 compile event internally.\n    - metadata: Extra metadata to put in pt2_compile_events.\n    - dynamo_compile_column_us: If provided, updates the specified CompilationMetrics\n      field to be logged to dyname_compile column. We expect all columns to be _us;\n      therefore, the field name must end with \"_us\".\n    - compile_id: In the typical case, this parameter should not be needed. Use to\n      supply the compile_id for those cases where we want to log a compile_id where\n      it's not naturally available, e.g., for runtime autotuning.\n    - is_backward: Specify forward/backward directly when not available in a\n      CompileContext, e.g., during runtime autotuning.\n      that support it.\n    - log_waitcounter: If set, we'll log a waitcounter of the form \"pytorch.dynamo_timed.{key}\"\n    \"\"\"\n    if phase_name:\n        event_name = phase_name\n        fn_name = key\n    else:\n        event_name = key\n        fn_name = None\n\n    if key not in compilation_time_metrics:\n        compilation_time_metrics[key] = []\n\n    event_metadata = {}\n    if metadata:\n        event_metadata.update(metadata)\n    if fn_name:\n        event_metadata.update({\"fn_name\": fn_name})\n    if is_backward is not None:\n        event_metadata.update({\"is_backward\": is_backward})\n\n    chromium_log: ChromiumEventLogger = get_chromium_event_logger()\n    start_ns = time.time_ns()\n    chromium_log.log_event_start(\n        event_name, start_ns, event_metadata, log_pt2_compile_event, compile_id\n    )\n\n    cx_mgrs: list[typing.Any] = [\n        torch.profiler.record_function(f\"{key} (dynamo_timed)\")\n    ]\n    if log_waitcounter:\n        wc_name = waitcounter_name_override if waitcounter_name_override else key\n        cx_mgrs.append(_WaitCounter(f\"pytorch.wait_counter.{wc_name}\").guard())\n\n    is_compile_time = torch._guards.CompileContext.current_compile_id() is not None\n    if dynamo_compile_column_us:\n        # We're standardizing on microseconds for dynamo_compile timings.\n        assert dynamo_compile_column_us.endswith(\"_us\")\n\n        # Track nested dynamo_timed calls that update CompilationMetrics so we can\n        # bump a total duration only for the outermost metric.\n        if not hasattr(_dynamo_timed_tls, \"depth\"):\n            _dynamo_timed_tls.depth = 0\n        _dynamo_timed_tls.depth += 1\n\n        # The corresponding WaitCounters that we bump for all overheads\n        if _dynamo_timed_tls.depth == 1:\n            cx_mgrs.append(_WaitCounter(\"pytorch.wait_counter.dynamo_compile\").guard())\n            if not is_compile_time:\n                runtime_wc = \"pytorch.wait_counter.compile_runtime_overheads\"\n                cx_mgrs.append(_WaitCounter(runtime_wc).guard())\n\n    try:\n        with contextlib.ExitStack() as stack:\n            for cx in cx_mgrs:\n                stack.enter_context(cx)\n            yield\n    finally:\n        end_ns = time.time_ns()\n        time_spent_ns = end_ns - start_ns\n        compilation_time_metrics[key].append(time_spent_ns / 1e9)\n        chromium_log.log_event_end(\n            event_name, end_ns, {}, start_ns, log_pt2_compile_event, compile_id\n        )\n        if dynamo_compile_column_us:\n            # TODO: the events that we capture in calculate_time_spent() seem a little\n            # arbitrary. Currently, it's only those fields that are present in\n            # CompilationMetrics (but note that we accumulate by the associated event\n            # name, not the field name in CompilationMetrics). Do we want to keep it\n            # this way?\n            cumulative_time_spent_ns[event_name] += time_spent_ns\n\n            # Bump the total duration for every outer event.\n            _dynamo_timed_tls.depth -= 1\n            is_outer_event = _dynamo_timed_tls.depth == 0\n\n            duration_us = time_spent_ns // 1000\n            if is_compile_time:\n                metrics_context = get_metrics_context()\n                if metrics_context.in_progress():\n                    metrics_context.increment(dynamo_compile_column_us, duration_us)\n                    if is_outer_event:\n                        metrics_context.increment(\"duration_us\", duration_us)\n            else:\n                runtime_context = get_runtime_metrics_context()\n                runtime_context.increment(dynamo_compile_column_us, duration_us)\n                if is_outer_event:\n                    extra = {\n                        \"compile_id\": compile_id,\n                        \"is_runtime\": True,\n                        \"is_forward\": not is_backward,\n                    }\n                    runtime_context.increment(\"duration_us\", duration_us, extra)\n\n\n@overload\ndef compile_times(repr: Literal[\"str\"], aggregate: bool = False) -> str: ...\n\n\n@overload\ndef compile_times(\n    repr: Literal[\"csv\"], aggregate: bool = False\n) -> tuple[list[str], list[object]]: ...\n\n\ndef compile_times(repr=\"str\", aggregate: bool = False):\n    \"\"\"\n    Get metrics about torchdynamo frontend/backend compilation times.\n\n    Accumulates information from functions tagged with `dynamo_timed`.\n\n    repr='str' returns a printable string for user interaction, and 'csv'\n    returns headers, rows which can be logged for output\n\n    aggregate causes values from multiple compilations (e.g. split graphs)\n    to be accumulated into one value.  If false, expect more than one value\n    per metric.\n    \"\"\"\n\n    def fmt_fn(values, item_fn=lambda x: x):\n        if aggregate:\n            return item_fn(sum(values))\n        return \", \".join(map(item_fn, values))\n\n    if repr == \"str\":\n        rows = [\n            (k, fmt_fn(compilation_time_metrics[k], item_fn=lambda x: f\"{x:.4f}\"))\n            for k in compilation_time_metrics\n        ]\n        out = \"TorchDynamo compilation metrics:\\n\"\n        out += tabulate(rows, headers=(\"Function\", \"Runtimes (s)\"))\n        return out\n    elif repr == \"csv\":\n        values = [\n            fmt_fn(v, item_fn=lambda x: f\"{x:.6f}\")\n            for v in compilation_time_metrics.values()\n        ]\n        headers = list(compilation_time_metrics.keys())\n        return headers, values\n    return None\n\n\n@atexit.register\ndef dump_compile_times() -> None:\n    log.info(compile_times(repr=\"str\", aggregate=True))\n\n\ntensortype_to_dtype = {\n    torch.FloatTensor: (torch.float32, torch.float),\n    torch.DoubleTensor: (torch.float64, torch.double),\n    torch.HalfTensor: (torch.float16, torch.half),\n    torch.BFloat16Tensor: (torch.bfloat16,),\n    torch.ByteTensor: (torch.uint8,),\n    torch.CharTensor: (torch.int8,),\n    torch.LongTensor: (torch.int64, torch.long),\n    torch.IntTensor: (torch.int32, torch.int),\n    torch.ShortTensor: (torch.int16, torch.short),\n    torch.BoolTensor: (torch.bool,),\n}\n\n\nclass DuplicateWarningChecker:\n    def __init__(self, maxsize: int = 4096) -> None:\n        self.maxsize = maxsize\n        self.reset()\n\n    def reset(self):\n        self.set = OrderedDict()\n\n    def add(self, key: Union[str, tuple[object, object]]) -> bool:\n        if key in self.set:\n            self.set.move_to_end(key, last=True)\n            if not config.verbose:\n                return False\n        else:\n            self.set[key] = None\n            while len(self.set) > self.maxsize:\n                self.set.popitem(last=False)\n        return True\n\n\ngraph_break_dup_warning_checker = DuplicateWarningChecker()\n\n\ndef setup_compile_debug():\n    compile_debug = os.environ.get(\"TORCH_COMPILE_DEBUG\", \"0\") == \"1\"\n\n    if compile_debug:\n        return add_file_handler()\n\n    return contextlib.ExitStack()\n\n\ndef reset_graph_break_dup_checker() -> None:\n    graph_break_dup_warning_checker.reset()\n\n\ndef add_file_handler():\n    log_path = os.path.join(get_debug_dir(), \"torchdynamo\")\n    os.makedirs(log_path, exist_ok=True)\n\n    log_file_handler = logging.FileHandler(os.path.join(log_path, \"debug.log\"))\n    logger = logging.getLogger(\"torch._dynamo\")\n    logger.addHandler(log_file_handler)\n\n    exitstack = contextlib.ExitStack()\n    exitstack.callback(lambda: logger.removeHandler(log_file_handler))\n    return exitstack\n\n\ndef setup_log_file():\n    exitstack = contextlib.ExitStack()\n    if config.log_file_name is not None:\n        log_file_handler = logging.FileHandler(config.log_file_name)\n        for logger in torch._logging._internal.get_loggers():\n            logger.addHandler(log_file_handler)\n            exitstack.callback(lambda: logger.removeHandler(log_file_handler))\n        return exitstack\n\n    return exitstack\n\n\ndef gen_record_file_name(exc, code) -> str:\n    return f\"{get_debug_dir()}/error_recordings/\\\n{code.co_name}_{type(exc).__name__}_{code.co_firstlineno}.rec\"\n\n\ndef write_record_to_file(filename: str, exec_record) -> None:\n    try:\n        if os.path.exists(filename):\n            log.warning(\n                \"Unable to write execution record %s; file already exists.\", filename\n            )\n        else:\n            os.makedirs(os.path.dirname(filename), exist_ok=True)\n            with open(filename, \"wb\") as f:\n                exec_record.dump(f)\n    except Exception:\n        log.exception(\"Unable to write execution record %s\", filename)\n\n\ndef count_calls(g: fx.Graph) -> int:\n    c = 0\n    for n in g.nodes:\n        if \"call\" in n.op:\n            c += 1\n    return c\n\n\ndef identity(x: T) -> T:\n    return x\n\n\ndef hashable(x):\n    try:\n        hash(x)\n        return True\n    except TypeError:\n        return False\n    # cannot hash writable memoryview object\n    except ValueError:\n        return False\n\n\ndef nothing(*args, **kwargs):\n    pass\n\n\nclass ExactWeakKeyDictionary:\n    \"\"\"Similar to weakref.WeakKeyDictionary, but use `is`/`id` rather than `==` to compare equality\"\"\"\n\n    def __init__(self):\n        self.values = {}\n        self.refs = {}\n\n    def __getitem__(self, key):\n        return self.values[id(key)]\n\n    def get(self, key, default=None):\n        return self.values.get(id(key), default)\n\n    def __contains__(self, key):\n        return id(key) in self.values\n\n    def __setitem__(self, key, value):\n        idx = id(key)\n        if idx not in self.refs:\n            self.refs[idx] = weakref.ref(key, lambda ref: self._remove_id(idx))\n        self.values[idx] = value\n\n    def _remove_id(self, idx):\n        if idx in self.values:\n            del self.values[idx]\n        if idx in self.refs:\n            del self.refs[idx]\n\n    def clear(self):\n        self.refs.clear()\n        self.values.clear()\n\n\n@overload\ndef istype(obj: object, allowed_types: type[T]) -> TypeIs[T]: ...\n\n\n@overload\ndef istype(\n    obj: object, allowed_types: tuple[type[list[T]], type[tuple[T, ...]]]\n) -> TypeIs[T]: ...\n\n\n@overload\ndef istype(obj: object, allowed_types: Iterable[type]) -> bool: ...\n\n\ndef istype(obj, allowed_types):\n    \"\"\"isinstance() without subclasses\"\"\"\n    if isinstance(allowed_types, (tuple, list, set)):\n        return type(obj) in allowed_types\n    return type(obj) is allowed_types\n\n\nif sys.version_info >= (3, 12):\n    # Some typing classes moved to C in 3.12,\n    # which no longer have the _Final mixin.\n    _builtin_final_typing_classes = (\n        typing.ParamSpecArgs,\n        typing.ParamSpecKwargs,\n        typing.ParamSpec,\n        typing.TypeVar,\n        typing.TypeVarTuple,\n        typing.TypeAliasType,\n    )\n\n\ndef is_typing(value):\n    # _Final catches most of typing classes:\n    #   - Any\n    #   - Callable\n    #   - Union\n    #   ...\n    #\n    # NB: we intentionally ignore classes that inherit from Generic, since they\n    # can be used as both TypingVariable as well as UserDefinedClassVariable.\n    if sys.version_info >= (3, 12) and isinstance(value, _builtin_final_typing_classes):\n        return True\n    return isinstance(value, typing._Final) or value is typing.Generic  # type: ignore[attr-defined]\n\n\ndef is_numpy_int_type(value):\n    if not np:\n        return False\n\n    return istype(\n        value,\n        (\n            np.int8,\n            np.int16,\n            np.int32,\n            np.int64,\n            np.uint8,\n            np.uint16,\n            np.uint32,\n            np.uint64,\n        ),\n    )\n\n\ndef is_numpy_float_type(value):\n    if not np:\n        return False\n\n    return istype(\n        value,\n        (\n            np.float16,\n            np.float32,\n            np.float64,\n        ),\n    )\n\n\n@overload\ndef is_lru_cache_wrapped_function(\n    value: Callable[..., T],\n) -> TypeGuard[functools._lru_cache_wrapper[T]]: ...\n\n\n@overload\ndef is_lru_cache_wrapped_function(\n    value: Any,\n) -> TypeGuard[functools._lru_cache_wrapper[Any]]: ...\n\n\ndef is_lru_cache_wrapped_function(\n    value: Any,\n) -> bool:\n    return isinstance(value, functools._lru_cache_wrapper) and is_function(\n        inspect.getattr_static(value, \"__wrapped__\")\n    )\n\n\n_FuncTypes: TypeAlias = Union[\n    types.FunctionType,\n    types.BuiltinFunctionType,\n    types.MethodDescriptorType,\n    types.WrapperDescriptorType,\n]\n\n\ndef is_function_or_wrapper(\n    value: Any,\n) -> TypeIs[Union[_FuncTypes, torch._ops.OpOverloadPacket, torch._ops.OpOverload]]:\n    return is_function(value) or isinstance(\n        value, (torch._ops.OpOverloadPacket, torch._ops.OpOverload)\n    )\n\n\ndef is_function(\n    value: Any,\n) -> TypeIs[_FuncTypes]:\n    return isinstance(\n        value,\n        (\n            types.FunctionType,\n            types.BuiltinFunctionType,\n            types.MethodDescriptorType,\n            types.WrapperDescriptorType,\n        ),\n    )\n\n\ncmp_name_to_op_mapping = {\n    \"__eq__\": operator.eq,\n    \"__ne__\": operator.ne,\n    \"__lt__\": operator.lt,\n    \"__le__\": operator.le,\n    \"__gt__\": operator.gt,\n    \"__ge__\": operator.ge,\n}\n\n\ncmp_name_to_op_str_mapping = {\n    \"__eq__\": \"==\",\n    \"__ne__\": \"!=\",\n    \"__lt__\": \"<\",\n    \"__le__\": \"<=\",\n    \"__gt__\": \">\",\n    \"__ge__\": \">=\",\n}\n\n\ndef is_wrapper_or_member_descriptor(\n    value: Any,\n) -> TypeIs[\n    Union[\n        types.GetSetDescriptorType,\n        types.MethodDescriptorType,\n        types.WrapperDescriptorType,\n        types.MemberDescriptorType,\n        types.MethodWrapperType,\n    ]\n]:\n    return isinstance(\n        value,\n        (\n            # set up by PyGetSetDef\n            types.GetSetDescriptorType,\n            # set by PyMethodDef, e.g. list.append\n            types.MethodDescriptorType,\n            # slots - list.__add__\n            types.WrapperDescriptorType,\n            # set up by PyMemberDef\n            types.MemberDescriptorType,\n            # wrapper over C functions\n            types.MethodWrapperType,\n        ),\n    )\n\n\ndef unwrap_if_wrapper(fn):\n    return unwrap_with_attr_name_if_wrapper(fn)[0]\n\n\ndef unwrap_with_attr_name_if_wrapper(fn):\n    # TODO(anijain2305) - Investigate if we can get rid of this function\n    # unpack @torch._dynamo.optimize()(fn) wrapped function\n    if is_function(fn) and inspect.getattr_static(fn, \"_torchdynamo_inline\", False):\n        fn = inspect.getattr_static(fn, \"_torchdynamo_inline\", fn)\n        attr_name = \"_torchdynamo_inline\"\n    else:\n        attr_name = None\n    return fn, attr_name\n\n\ndef is_numpy_ndarray(value):\n    if not np:\n        return False\n\n    return istype(value, np.ndarray)\n\n\ndef istensor(obj):\n    \"\"\"Check of obj is a tensor\"\"\"\n    tensor_list: tuple[type, ...] = (\n        torch.Tensor,\n        torch.nn.Parameter,\n        *config.traceable_tensor_subclasses,\n    )\n    tensor_list = tensor_list + (torch._subclasses.FakeTensor,)\n    return istype(obj, tensor_list)\n\n\ndef is_lazy_module(mod):\n    return isinstance(mod, LazyModuleMixin)\n\n\n@functools.lru_cache(4096)\ndef print_once(*args):\n    print(*args)\n\n\ndef make_cell(val=None):\n    \"\"\"Some black magic to create a cell object that usually only exists in a closure\"\"\"\n    x = val\n\n    def f():\n        return x\n\n    assert f.__closure__ is not None and len(f.__closure__) == 1\n    return f.__closure__[0]\n\n\ndef proxy_args_kwargs(args, kwargs):\n    try:\n        proxy_args = tuple(arg.as_proxy() for arg in args)\n        proxy_kwargs = {key: arg.as_proxy() for key, arg in kwargs.items()}\n        return proxy_args, proxy_kwargs\n    except NotImplementedError as e:\n        from .exc import unimplemented_v2\n        from .variables.base import typestr\n\n        unimplemented_v2(\n            gb_type=\"Failed to convert args/kwargs to proxy\",\n            context=f\"call_function args: {typestr(*args)} {typestr(*list(kwargs.values()))}\",\n            explanation=\"Missing `as_proxy()` implementation for some arg/kwarg.\",\n            hints=[],\n            from_exc=e,\n        )\n\n\ndef to_int_ms(v: Optional[float]) -> Optional[int]:\n    return None if v is None else int(v * 1000)\n\n\n# float64 timestamp has a quarter microsecond precision in 2024, so while\n# this is suboptimal we shouldn't meaningfully lose precision\ndef to_int_us(v: Optional[float]) -> Optional[int]:\n    return None if v is None else int(v * 1_000_000)\n\n\n# Version field added to every log. Increment to make it easier to distinguish new\n# vs. old entries when you make a substantive change to how the logs are populated.\nLOG_FORMAT_VERSION = 3\n\n\n@dataclasses.dataclass\nclass CompilationMetrics:\n    compile_id: Optional[str] = None\n    frame_key: Optional[str] = None\n    co_name: Optional[str] = None\n    co_filename: Optional[str] = None\n    co_firstlineno: Optional[int] = None\n    cache_size: Optional[int] = None\n    accumulated_cache_size: Optional[int] = None\n    guard_count: Optional[int] = None\n    shape_env_guard_count: Optional[int] = None\n    graph_op_count: Optional[int] = None\n    graph_node_count: Optional[int] = None\n    graph_input_count: Optional[int] = None\n    start_time: Optional[float] = None\n    entire_frame_compile_time_s: Optional[float] = None\n    backend_compile_time_s: Optional[float] = None\n    inductor_compile_time_s: Optional[float] = None\n    code_gen_time_s: Optional[float] = None\n    fail_type: Optional[str] = None\n    fail_reason: Optional[str] = None\n    fail_user_frame_filename: Optional[str] = None\n    fail_user_frame_lineno: Optional[int] = None\n    non_compliant_ops: Optional[set[str]] = None\n    compliant_custom_ops: Optional[set[str]] = None\n    restart_reasons: Optional[set[str]] = None\n    dynamo_time_before_restart_s: Optional[float] = None\n    # Sometimes, we will finish analyzing a frame but conclude we don't want\n    # to install any guarded code.  True means we actually decided to install\n    # a compiled frame\n    has_guarded_code: Optional[bool] = None\n    remote_cache_time_saved_s: Optional[float] = None\n    structured_logging_overhead_s: Optional[float] = None\n    config_suppress_errors: Optional[bool] = None\n    config_inline_inbuilt_nn_modules: Optional[bool] = None\n    specialize_float: Optional[bool] = None\n    dynamo_config: Optional[str] = None\n    is_forward: Optional[bool] = None\n    num_triton_bundles: Optional[int] = None\n    remote_fx_graph_cache_get_time_ms: Optional[int] = None\n    remote_fx_graph_cache_put_time_ms: Optional[int] = None\n    start_time_us: Optional[int] = None\n    duration_us: Optional[int] = None\n    dynamo_cumulative_compile_time_us: Optional[int] = None\n    aot_autograd_cumulative_compile_time_us: Optional[int] = None\n    inductor_cumulative_compile_time_us: Optional[int] = None\n    inductor_code_gen_cumulative_compile_time_us: Optional[int] = None\n    triton_compile_time_us: Optional[int] = None\n    runtime_cudagraphify_time_us: Optional[int] = None\n    runtime_triton_autotune_time_us: Optional[int] = None\n    dynamo_compile_time_before_restart_us: Optional[int] = None\n    distributed_ephemeral_timeout_us: Optional[int] = None\n    structured_logging_overhead_us: Optional[int] = None\n    remote_fx_graph_cache_get_time_us: Optional[int] = None\n    remote_fx_graph_cache_put_time_us: Optional[int] = None\n    backward_cumulative_compile_time_us: Optional[int] = None\n    end_time_us: Optional[int] = None\n    pre_grad_pass_time_us: Optional[int] = None\n    post_grad_pass_time_us: Optional[int] = None\n    joint_graph_pass_time_us: Optional[int] = None\n    log_format_version: int = LOG_FORMAT_VERSION\n    inductor_config: Optional[str] = None\n    remote_cache_version: Optional[int] = None\n    inductor_fx_remote_cache_hit_count: Optional[int] = None\n    inductor_fx_remote_cache_miss_count: Optional[int] = None\n    inductor_fx_remote_cache_backend_type: Optional[str] = None\n    inductor_fx_remote_cache_hit_keys: Optional[str] = None\n    inductor_fx_remote_cache_miss_keys: Optional[str] = None\n    cuda_version: Optional[str] = None\n    triton_version: Optional[str] = None\n    feature_usage: Optional[dict[str, bool]] = None\n    compile_time_autotune_time_us: Optional[int] = None\n    is_runtime: Optional[bool] = False\n    gc_time_us: Optional[int] = None\n    tensorify_float_attempt: Optional[bool] = None\n    tensorify_float_success: Optional[bool] = None\n    tensorify_float_failure: Optional[set[str]] = None\n    guard_latency_us: Optional[float] = None\n    recompile_reason: Optional[str] = None\n    num_graph_breaks: Optional[int] = None\n    triton_kernel_compile_times_us: Optional[str] = None\n    ir_count: Optional[int] = None\n    cudagraph_skip_reason: Optional[str] = None\n    python_version: Optional[str] = None\n    pgo_put_remote_code_state_time_us: Optional[int] = None\n    pgo_get_remote_code_state_time_us: Optional[int] = None\n    # The number of elements within parameters. This is classically what people\n    # think of when they think of parameters in a ML model.\n    param_numel: Optional[int] = None\n    # The number of elements counted by bytes - i.e. a float32 is 4 bytes\n    # per element.\n    param_bytes: Optional[int] = None\n    # The number of parameters counted by fields. This is mostly a proxy for\n    # the number of distinct type of params.\n    param_count: Optional[int] = None\n    recompile_user_contexts: Optional[set[str]] = None\n\n    @classmethod\n    def create(cls, metrics: dict[str, Any]):\n        \"\"\"\n        Factory method to create a CompilationMetrics from a dict of fields.\n        Includes the logic to add legacy fields and any pre-processing, e.g.,\n        we transform some fields to comma-separated strings for scuba logging.\n        \"\"\"\n\n        def us_to_s(metric: Optional[int]) -> Optional[float]:\n            return metric / 1e6 if metric is not None else None\n\n        def us_to_ms(metric: Optional[int]) -> Optional[int]:\n            return metric // 1000 if metric is not None else None\n\n        def collection_to_str(metric: Optional[Any]) -> Optional[str]:\n            def safe_str(item: Any) -> str:\n                try:\n                    return str(item)\n                except Exception:\n                    return \"<unknown>\"\n\n            if metric is None:\n                return None\n\n            if not isinstance(metric, (set, list)):\n                return \"<unknown>\"\n\n            return \",\".join(safe_str(item) for item in sorted(metric))\n\n        def collection_to_json_str(metric: Optional[Any]) -> Optional[str]:\n            if metric is None:\n                return None\n            try:\n                return json.dumps(list(metric))\n            except Exception:\n                return \"<unknown>\"\n\n        # TODO: The following are legacy fields, populated from the fields that replace\n        # them. Remove these when we decide we can really deprecate them.\n        legacy_metrics = {\n            \"start_time\": us_to_s(metrics.get(\"start_time_us\")),\n            \"entire_frame_compile_time_s\": us_to_s(\n                metrics.get(\"dynamo_cumulative_compile_time_us\")\n            ),\n            \"backend_compile_time_s\": us_to_s(\n                metrics.get(\"aot_autograd_cumulative_compile_time_us\")\n            ),\n            \"inductor_compile_time_s\": us_to_s(\n                metrics.get(\"inductor_cumulative_compile_time_us\")\n            ),\n            \"code_gen_time_s\": us_to_s(\n                metrics.get(\"inductor_code_gen_cumulative_compile_time_us\")\n            ),\n            \"remote_cache_time_saved_s\": us_to_s(\n                metrics.get(\"distributed_ephemeral_timeout_us\")\n            ),\n            \"remote_fx_graph_cache_get_time_ms\": us_to_ms(\n                metrics.get(\"remote_fx_graph_cache_get_time_us\")\n            ),\n            \"remote_fx_graph_cache_put_time_ms\": us_to_ms(\n                metrics.get(\"remote_fx_graph_cache_put_time_us\")\n            ),\n            \"structured_logging_overhead_s\": us_to_s(\n                metrics.get(\"structured_logging_overhead_us\")\n            ),\n        }\n\n        all_metrics = {**legacy_metrics, **metrics}\n\n        # Processing before logging:\n        all_metrics[\"inductor_fx_remote_cache_hit_keys\"] = collection_to_str(\n            all_metrics.get(\"inductor_fx_remote_cache_hit_keys\")\n        )\n        all_metrics[\"inductor_fx_remote_cache_miss_keys\"] = collection_to_str(\n            all_metrics.get(\"inductor_fx_remote_cache_miss_keys\")\n        )\n        all_metrics[\"triton_kernel_compile_times_us\"] = collection_to_json_str(\n            all_metrics.get(\"triton_kernel_compile_times_us\")\n        )\n        compile_id = all_metrics.get(\"compile_id\")\n        all_metrics[\"compile_id\"] = str(compile_id) if compile_id else None\n\n        return cls(**all_metrics)\n\n\nDEFAULT_COMPILATION_METRICS_LIMIT = 64\n\n\n_compilation_metrics: collections.deque[CompilationMetrics] = collections.deque(\n    maxlen=DEFAULT_COMPILATION_METRICS_LIMIT\n)\n\n\ndef add_compilation_metrics_to_chromium(c: CompilationMetrics) -> None:\n    \"\"\"\n    These are the common fields in CompilationMetrics that existed before\n    metrics_context, and aren't set by MetricsContext.set(). We add the subset\n    of them that make sense in `dynamo`/toplevel events in PT2 Compile Events\n    directly.\n\n    If you're tempted to add to this list, consider using CompileEventLogger.compilation_metric()\n    instead, which will automatically also add it to tlparse and PT2 Compile Events.\n    TODO: Get rid of this function and replace it with CompileEventLogger directly instead.\n    \"\"\"\n    event_logger = get_chromium_event_logger()\n    event_name = event_logger.get_outermost_event()\n    if not event_name:\n        return\n    event_logger.add_event_data(\n        event_name=event_name,\n        frame_key=c.frame_key,\n        co_name=c.co_name,\n        co_filename=c.co_filename,\n        co_firstlineno=c.co_firstlineno,\n        cache_size=c.cache_size,\n        accumulated_cache_size=c.accumulated_cache_size,\n        guard_count=c.guard_count,\n        shape_env_guard_count=c.shape_env_guard_count,\n        graph_op_count=c.graph_op_count,\n        graph_node_count=c.graph_node_count,\n        graph_input_count=c.graph_input_count,\n        fail_type=c.fail_type,\n        fail_reason=c.fail_reason,\n        fail_user_frame_filename=c.fail_user_frame_filename,\n        fail_user_frame_lineno=c.fail_user_frame_lineno,\n        # Sets aren't JSON serializable\n        non_compliant_ops=list(c.non_compliant_ops)\n        if c.non_compliant_ops is not None\n        else None,\n        compliant_custom_ops=list(c.compliant_custom_ops)\n        if c.compliant_custom_ops is not None\n        else None,\n        restart_reasons=list(c.restart_reasons)\n        if c.restart_reasons is not None\n        else None,\n        dynamo_time_before_restart_s=c.dynamo_time_before_restart_s,\n        has_guarded_code=c.has_guarded_code,\n        dynamo_config=c.dynamo_config,\n    )\n\n\ndef _get_dynamo_config_for_logging() -> Optional[str]:\n    def clean_for_json(d: dict[str, Any]) -> dict[str, Any]:\n        blocklist = {\n            \"TYPE_CHECKING\",\n            \"log_file_name\",\n            \"verbose\",\n            \"repro_after\",\n            \"repro_level\",\n            \"repro_forward_only\",\n            \"repro_tolerance\",\n            \"repro_ignore_non_fp\",\n            \"same_two_models_use_fp64\",\n            \"base_dir\",\n            \"debug_dir_root\",\n            \"_save_config_ignore\",\n            \"log_compilation_metrics\",\n            \"inject_BUILD_SET_unimplemented_TESTING_ONLY\",\n            \"_autograd_backward_strict_mode_banned_ops\",\n            \"reorderable_logging_functions\",\n            \"ignore_logger_methods\",\n            \"traceable_tensor_subclasses\",\n            \"nontraceable_tensor_subclasses\",\n            \"_custom_ops_profile\",\n        }\n\n        return {\n            key: sorted(value) if isinstance(value, set) else value\n            for key, value in d.items()\n            if key not in blocklist\n        }\n\n    config_dict = clean_for_json(config.get_config_copy())\n    return json.dumps(config_dict, sort_keys=True)\n\n\ndef _scrubbed_inductor_config_for_logging() -> Optional[str]:\n    \"\"\"\n    Method to parse and scrub uninteresting configs from inductor config\n    \"\"\"\n\n    # TypeSafeSerializer for json.dumps()\n    # Skips complex types as values in config dict\n    class TypeSafeSerializer(json.JSONEncoder):\n        def default(self, o):\n            try:\n                return super().default(o)\n            except Exception:\n                return \"Value is not JSON serializable\"\n\n    keys_to_scrub: set[Any] = set()\n    inductor_conf_str = None\n    inductor_config_copy = (\n        torch._inductor.config.get_config_copy() if torch._inductor.config else None\n    )\n    if inductor_config_copy is not None:\n        try:\n            for key, val in inductor_config_copy.items():\n                if not isinstance(key, str):\n                    keys_to_scrub.add(key)\n                # Convert set() to list for json.dumps()\n                if isinstance(val, set):\n                    inductor_config_copy[key] = list(val)\n            # Evict unwanted keys\n            for key in keys_to_scrub:\n                del inductor_config_copy[key]\n            # Stringify Inductor config\n            inductor_conf_str = json.dumps(\n                inductor_config_copy,\n                cls=TypeSafeSerializer,\n                skipkeys=True,\n                sort_keys=True,\n            )\n        except Exception:\n            # Don't crash because of runtime logging errors\n            inductor_conf_str = \"Inductor Config is not JSON serializable\"\n    return inductor_conf_str\n\n\ndef record_compilation_metrics(\n    start_time_ns: int,\n    end_time_ns: int,\n    metrics: dict[str, Any],\n    exc_type: Optional[type[BaseException]],\n    exc_value: Optional[BaseException],\n):\n    if torch._inductor.utils.should_use_remote_fx_graph_cache():\n        try:\n            from torch._inductor.fb.remote_cache import REMOTE_CACHE_VERSION\n\n            remote_cache_version = REMOTE_CACHE_VERSION\n            inductor_fx_remote_cache_backend_type = \"_ManifoldCache\"\n        except ModuleNotFoundError:\n            remote_cache_version = None\n            inductor_fx_remote_cache_backend_type = None\n    else:\n        inductor_fx_remote_cache_backend_type = None\n        remote_cache_version = None\n\n    # Populate the compile_id from the metrics context if it's set. Otherwise,\n    # look for it in the current compile context.\n    compile_id = metrics.get(\"compile_id\")\n    if not compile_id:\n        compile_id = torch._guards.CompileContext.current_compile_id()\n\n    common_metrics = {\n        \"compile_id\": compile_id,\n        \"start_time_us\": start_time_ns // 1000,\n        \"end_time_us\": end_time_ns // 1000,\n        \"fail_type\": exc_type.__qualname__ if exc_type else None,\n        \"fail_reason\": str(exc_value) if exc_value else None,\n        \"structured_logging_overhead_us\": to_int_us(\n            torch._logging.get_structured_logging_overhead()\n        ),\n        \"dynamo_config\": _get_dynamo_config_for_logging(),\n        \"config_suppress_errors\": config.suppress_errors,\n        \"config_inline_inbuilt_nn_modules\": config.inline_inbuilt_nn_modules,\n        \"inductor_config\": _scrubbed_inductor_config_for_logging(),\n        \"cuda_version\": torch.version.cuda,\n        \"triton_version\": triton.__version__ if has_triton() else \"\",\n        \"remote_cache_version\": remote_cache_version,\n        \"inductor_fx_remote_cache_backend_type\": inductor_fx_remote_cache_backend_type,\n        \"python_version\": sys.version,\n    }\n\n    compilation_metrics = CompilationMetrics.create({**common_metrics, **metrics})\n    _compilation_metrics.append(compilation_metrics)\n\n    name = \"compilation_metrics\"\n    if compilation_metrics.is_forward is False:\n        name = \"bwd_\" + name\n    if compilation_metrics.is_runtime is True:\n        name = name + \"_runtime\"\n\n    torch._logging.trace_structured(\n        name,\n        lambda: {\n            k: list(v) if isinstance(v, set) else v\n            for k, v in dataclasses.asdict(compilation_metrics).items()\n        },\n        # NB: Because compilation metrics *includes* the logging overhead time,\n        # we can't both *measure* the logging overhead of compilation metrics\n        # without making it inconsistent with compilation metrics itself, so\n        # we ignore the (hopefully small) time spent logging compilation metrics\n        record_logging_overhead=False,\n        # These may be runtime logs, e.g., runtime autotunning, so we provide\n        # the CompileId from the compilation metrics in case it's not available\n        # in the current trace.\n        compile_id=compile_id,\n    )\n\n    # If there's a chromium event in flight, add the CompilationMetrics to it.\n    add_compilation_metrics_to_chromium(compilation_metrics)\n\n    # Finally log the compilation metrics.\n    if config.log_compilation_metrics:\n        log_compilation_event(compilation_metrics)\n\n\n# record_compilation_metrics is called by the singleton MetricsContext exit handler.\n_METRICS_CONTEXT = MetricsContext(on_exit=record_compilation_metrics)\n_RUNTIME_METRICS_CONTEXT = RuntimeMetricsContext(on_exit=record_compilation_metrics)\n\n\ndef set_compilation_metrics_limit(new_size: int) -> None:\n    global _compilation_metrics\n    while len(_compilation_metrics) > new_size:\n        _compilation_metrics.popleft()\n    new_deque = collections.deque(_compilation_metrics, maxlen=new_size)\n    _compilation_metrics = new_deque\n\n\ndef clear_compilation_metrics() -> None:\n    global _compilation_metrics\n    _compilation_metrics.clear()\n\n\ndef get_compilation_metrics() -> list[CompilationMetrics]:\n    return list(_compilation_metrics)\n\n\nclass ChromiumEventLogger:\n    \"\"\"Logs chromium events to structured logs. tlparse will concatenate these into a perfetto UI link.\n\n    See https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview#heading=h.yr4qxyxotyw for\n    a specification of the Chromium Event JSON format.\n    \"\"\"\n\n    def get_stack(self) -> list[str]:\n        \"\"\"\n        The main event stack, with every chromium event.\n        Logged to tlparse.\n        \"\"\"\n        if hasattr(self.tls, \"stack\"):\n            return self.tls.stack\n        else:\n            self.tls.stack = []\n            return self.tls.stack\n\n    def get_outermost_event(self) -> Optional[str]:\n        \"\"\"\n        Get the outermost event name (i.e. the longest running event)\n        or None if the stack is empty.\n        \"\"\"\n        stack = self.get_stack()\n        return stack[0] if stack else None\n\n    def get_pt2_compile_substack(self):\n        \"\"\"\n        A smaller subset of the main stack that gets used to log\n        PT2 Compile Events internally.\n        \"\"\"\n        if hasattr(self.tls, \"pt2_compile_substack\"):\n            return self.tls.pt2_compile_substack\n        else:\n            self.tls.pt2_compile_substack = []\n            return self.tls.pt2_compile_substack\n\n    def get_event_data(self) -> dict[str, Any]:\n        if not hasattr(self.tls, \"event_data\"):\n            self.tls.event_data = {}\n        return self.tls.event_data\n\n    def __init__(self):\n        self.tls = threading.local()\n        # Generate a unique id for this logger, which we can use in scuba to filter down\n        # to a single python run.\n        self.id_ = str(uuid.uuid4())\n\n        # TODO: log to init/id tlparse after I add support for it\n        log.info(\"ChromiumEventLogger initialized with id %s\", self.id_)\n\n    def try_add_event_data(self, event_name: str, **kwargs) -> None:\n        \"\"\"\n        Same as add_event_data, but will silently not log if the event isn't in the stack.\n        \"\"\"\n        if event_name not in self.get_stack():\n            return\n        self.add_event_data(event_name, **kwargs)\n\n    def add_event_data(\n        self,\n        event_name: str,\n        **kwargs,\n    ) -> None:\n        \"\"\"\n        Adds additional metadata info to an in-progress event\n        This metadata is recorded in the END event\n        \"\"\"\n        if event_name not in self.get_stack():\n            raise RuntimeError(\n                f\"Event {repr(event_name)} not in {self.get_stack()}. \"\n                \"Cannot add metadata to events that aren't in progress. \"\n                \"Please make sure the event has started and hasn't ended.\"\n            )\n        event_data = self.get_event_data()\n        if event_name not in event_data:\n            event_data[event_name] = {}\n        event_data[event_name].update(kwargs)\n\n    def increment(self, event_name: str, key: str, value: int):\n        \"\"\"\n        Increment an integer event data field by the given amount\n        \"\"\"\n        if event_name not in self.get_stack():\n            raise RuntimeError(\n                f\"Event {repr(event_name)} not in {self.get_stack()}. \"\n                \"Cannot add metadata to events that aren't in progress. \"\n                \"Please make sure the event has started and hasn't ended.\"\n            )\n\n        event_data = self.get_event_data()\n        if event_name not in event_data:\n            event_data[event_name] = {}\n        if key not in event_data[event_name]:\n            event_data[event_name][key] = 0\n        event_data[event_name][key] += value\n\n    def add_to_set(\n        self,\n        event_name: str,\n        key: str,\n        value: Any,\n    ):\n        \"\"\"\n        Add a value to a set within a event_name's metadata if it exists\n        \"\"\"\n        if event_name not in self.get_stack():\n            raise RuntimeError(\n                f\"Event {repr(event_name)} not in {self.get_stack()}. \"\n                \"Cannot add metadata to events that aren't in progress. \"\n                \"Please make sure the event has started and hasn't ended.\"\n            )\n        event_data = self.get_event_data()\n        if event_name not in event_data:\n            event_data[event_name] = {}\n        if key not in event_data[event_name]:\n            event_data[event_name][key] = set()\n        event_data[event_name][key].add(value)\n\n    def log_event_start(\n        self,\n        event_name: str,\n        time_ns: int,\n        metadata: dict[str, Any],\n        log_pt2_compile_event: bool = False,\n        compile_id: Optional[CompileId] = None,\n    ) -> None:\n        \"\"\"\n        Logs the start of a single event.\n        :param str event_name Name of event to appear in trace\n        :param time_ns Timestamp in nanoseconds\n        :param metadata: Any extra metadata associated with this event\n        :param log_pt2_compile_event: If True, log to pt2_compile_events\n        :param compile_id: Explicit compile_id (rather than using the current context)\n        \"\"\"\n        compile_id = compile_id or torch._guards.CompileContext.current_compile_id()\n        metadata[\"compile_id\"] = str(compile_id)\n        self._log_timed_event(\n            event_name,\n            time_ns,\n            \"B\",\n            metadata,\n        )\n        self.get_stack().append(event_name)\n        # Add metadata from start event\n        self.add_event_data(event_name, **metadata)\n        if log_pt2_compile_event:\n            self.get_pt2_compile_substack().append(event_name)\n\n    def reset(self) -> None:\n        # We this on every compile in case a compile crashes or restarts and we haven't\n        # cleared the stack.\n        stack = self.get_stack()\n        substack = self.get_pt2_compile_substack()\n        stack.clear()\n        substack.clear()\n        event_data = self.get_event_data()\n        event_data.clear()\n\n    def log_event_end(\n        self,\n        event_name: str,\n        time_ns: int,\n        metadata: dict[str, Any],\n        start_time_ns: int,\n        log_pt2_compile_event: bool,\n        compile_id: Optional[CompileId] = None,\n    ) -> None:\n        \"\"\"\n        Logs the end of a single event. This function should only be\n        called after log_event_start with the same event_name.\n        :param event_name: Name of event to appear in trace\n        :param time_ns: Timestamp in nanoseconds\n        :param metadata: Any extra metadata associated with this event\n        :param start_time_ns: The start time timestamp in nanoseconds\n        :param log_pt_compile_event: If True, log to pt2_compile_events\n        :param compile_id: Explicit compile_id (rather than using the current context)\n        \"\"\"\n        compile_id = compile_id or torch._guards.CompileContext.current_compile_id()\n        metadata[\"compile_id\"] = str(compile_id)\n\n        # Grab metadata collected during event span\n        all_event_data = self.get_event_data()\n        if event_name in all_event_data:\n            event_metadata = all_event_data[event_name]\n            del all_event_data[event_name]\n        else:\n            event_metadata = {}\n        # Add the passed in metadata\n        event_metadata.update(metadata)\n\n        event = self._log_timed_event(\n            event_name,\n            time_ns,\n            \"E\",\n            event_metadata,\n        )\n\n        def pop_stack(stack):\n            while event_name != stack[-1]:\n                # If the event isn't the most recent one to end, pop\n                # off the stack until it is.\n                # Since event_name in self.stack, this pop is always safe\n                log.warning(\n                    \"ChromiumEventLogger: Detected overlapping events, fixing stack\"\n                )\n                stack.pop()\n\n        event_stack = self.get_stack()\n        # These stack health checks currently never happen,\n        # but they're written this way to future proof any weird event\n        # overlaps in the future.\n        if event_name not in event_stack:\n            # Something went wrong, we never called start on this event,\n            # or it was skipped due to overlapping events below\n            log.warning(\"ChromiumEventLogger: Start event not in stack, ignoring\")\n            return\n\n        pop_stack(event_stack)\n\n        if log_pt2_compile_event:\n            pt2_compile_substack = self.get_pt2_compile_substack()\n            pop_stack(pt2_compile_substack)\n            log_chromium_event_internal(\n                event, pt2_compile_substack, self.id_, start_time_ns\n            )\n            # Pop actual event off of stack\n            pt2_compile_substack.pop()\n\n        # Finally pop the actual event off the stack\n        event_stack.pop()\n\n    def _log_timed_event(\n        self,\n        event_name: str,\n        time_ns: int,\n        phase: str,\n        metadata: Optional[dict[str, Any]] = None,\n    ) -> dict[str, Any]:\n        \"\"\"\n        Logs a timed event in chromium format. See log_event_start, log_event_end, etc.\n        \"\"\"\n        event = {\n            \"name\": event_name,\n            \"ts\": time_ns / 1000,  # Chromium events are in micro seconds\n            \"args\": metadata,\n            \"ph\": phase,\n            # These categories are needed in all chromium traces\n            \"cat\": \"dynamo_timed\",\n            \"tid\": 0,\n            \"pid\": 0,  # pid should be specified on all logs, we don't personally care about the actual process id\n        }\n        torch._logging.trace_structured(\n            \"chromium_event\",\n            payload_fn=lambda: event,\n            suppress_context=False,\n            expect_trace_id=False,  # Not every chromium event will have a trace_id\n        )\n        record_chromium_event_internal(event)\n        return event\n\n    def log_instant_event(\n        self,\n        event_name: str,\n        time_ns: int,\n        metadata: Optional[dict[str, Any]] = None,\n        # By default, an instant event isn't logged internally, only to structured logging.\n        log_pt2_compile_event: bool = False,\n    ) -> None:\n        \"\"\"\n        Log an instant event with no associated duration.\n        :param str event_name: Name of event to appear in trace\n        :param int time_ns Timestamp in nanoseconds\n        :param Optional[Dict[str, Any]] metadata: Any extra metadata associated with this event\n        :param str cname optional color for the arrow in the trace\n        \"\"\"\n        if metadata is None:\n            metadata = {}\n        compile_id = str(torch._guards.CompileContext.current_compile_id())\n        metadata[\"compile_id\"] = compile_id\n        event = {\n            \"name\": event_name,\n            \"ts\": time_ns / 1000,\n            \"args\": metadata,\n            \"ph\": \"i\",\n            # These categories are needed in all chromium traces\n            \"cat\": \"dynamo_timed\",\n            \"tid\": 0,\n            \"pid\": 0,\n            \"s\": \"p\",  # We use \"process\" level instant events so they all appear on the same row in the trace.\n        }\n        torch._logging.trace_structured(\n            \"chromium_event\",\n            payload_fn=lambda: event,\n            suppress_context=False,\n            expect_trace_id=True,\n        )\n        if log_pt2_compile_event:\n            # Log an instant event with the same start and end time\n            log_chromium_event_internal(\n                event, self.get_pt2_compile_substack(), self.id_, time_ns\n            )\n\n\nCHROMIUM_EVENT_LOG: Optional[ChromiumEventLogger] = None\n\n\ndef get_chromium_event_logger() -> ChromiumEventLogger:\n    global CHROMIUM_EVENT_LOG\n    if CHROMIUM_EVENT_LOG is None:\n        CHROMIUM_EVENT_LOG = ChromiumEventLogger()\n    return CHROMIUM_EVENT_LOG\n\n\ndef chromium_event_log_active() -> bool:\n    global CHROMIUM_EVENT_LOG\n    return CHROMIUM_EVENT_LOG is not None\n\n\n@contextmanager\ndef chromium_event_timed(\n    event_name: str,\n    reset_event_log_on_exit: bool = False,\n    log_pt2_compile_event: bool = False,\n) -> Generator[Any, None, None]:\n    \"\"\"\n    Context manager that creates a chromium start and end event. Chromium event\n    logging is integrated with dynamo_timed, so you probably want to use that\n    instead. Use this context manager only if you want to avoid dynamo_timed.\n    \"\"\"\n    chromium_event_log = get_chromium_event_logger()\n    chromium_start_time = time.time_ns()\n    chromium_event_log.log_event_start(\n        event_name,\n        chromium_start_time,\n        {},\n        log_pt2_compile_event,\n    )\n    try:\n        yield\n    finally:\n        chromium_event_log.log_event_end(\n            event_name,\n            time.time_ns(),\n            {},\n            chromium_start_time,\n            log_pt2_compile_event,\n        )\n        if reset_event_log_on_exit:\n            chromium_event_log.reset()\n\n\n@dataclasses.dataclass\nclass CleanupHook:\n    \"\"\"Remove a global variable when hook is called\"\"\"\n\n    scope: dict[str, Any]\n    name: str\n\n    def __call__(self, *args):\n        # Make sure we're not shutting down\n        if CleanupManager is not None:\n            CleanupManager.count -= 1\n        del self.scope[self.name]\n\n    @staticmethod\n    def create(scope, name, val):\n        assert name not in scope\n        CleanupManager.count += 1\n        scope[name] = val\n        return CleanupHook(scope, name)\n\n\nclass CleanupManager(ExactWeakKeyDictionary):\n    count = 0\n    instance: ClassVar[CleanupManager]\n\n    def _remove_id(self, idx):\n        for hook in self.values[idx]:\n            hook()\n        super()._remove_id(idx)\n\n\nCleanupManager.instance = CleanupManager()\n\n\ndef clone_tensor(x):\n    \"\"\"Clone the tensor and its gradient\"\"\"\n    y = x.clone().requires_grad_(x.requires_grad)\n    if x.is_leaf and x.grad is not None:\n        y.grad = x.grad.clone()\n    return y\n\n\ndef clone_input(x, *, dtype=None):\n    \"\"\"copy while preserving strides\"\"\"\n    # TODO: this is questionable\n    if is_fake(x):\n        # this func fails on fake tensors in __torch_dispatch__\n        return x\n\n    def torch_clone(x):\n        y = torch.clone(x)\n        if x.is_leaf:\n            y.requires_grad_(x.requires_grad)\n        if x.is_leaf and x.grad is not None:\n            y.grad = clone_input(x.grad, dtype=dtype)\n        if hasattr(x, \"_dynamo_dynamic_indices\"):\n            y._dynamo_dynamic_indices = x._dynamo_dynamic_indices.copy()  # type: ignore[attr-defined]\n        return y\n\n    with torch.no_grad():\n        if x.device.type == \"xla\":\n            # Access data_ptr() for a xla tensor will cause crash\n            return torch_clone(x)\n\n        # Handle sparse storage (no stride).\n        if x.layout is torch.sparse_coo:\n            return torch.sparse_coo_tensor(\n                torch_clone(x._indices()),\n                torch_clone(x._values()),\n                x.shape,\n                is_coalesced=x.is_coalesced(),\n            )\n        elif is_sparse_compressed(x):\n            if x.layout in {torch.sparse_csr, torch.sparse_bsr}:\n                compressed_indices = x.crow_indices()\n                plain_indices = x.col_indices()\n            else:\n                compressed_indices = x.ccol_indices()\n                plain_indices = x.row_indices()\n            return torch.sparse_compressed_tensor(\n                torch_clone(compressed_indices),\n                torch_clone(plain_indices),\n                torch_clone(x.values()),\n                x.shape,\n                layout=x.layout,\n            )\n\n        needed_size = sum(\n            (shape - 1) * stride for shape, stride in zip(x.size(), x.stride())\n        )\n        if x.is_quantized:\n            result = torch.empty_quantized((needed_size + 32,), x)\n        else:\n            result = torch.empty(\n                needed_size + 32, dtype=dtype or x.dtype, device=x.device\n            )\n        cache_line_offset = (\n            (x.data_ptr() - result.data_ptr()) % 32\n        ) // x.element_size()\n        result.as_strided_(x.size(), x.stride(), cache_line_offset)\n        try:\n            result.copy_(x.clone())\n            if x.is_leaf:\n                result.requires_grad_(x.requires_grad)\n            if x.is_leaf and x.grad is not None:\n                result.grad = clone_input(x.grad, dtype=dtype)\n        except RuntimeError:\n            # RuntimeError: unsupported operation: more than one element of the written-to\n            # tensor refers to a single memory location. Please clone() the tensor before\n            # performing the operation.\n            return torch_clone(x)\n        if hasattr(x, \"_dynamo_dynamic_indices\"):\n            result._dynamo_dynamic_indices = x._dynamo_dynamic_indices.copy()  # type: ignore[attr-defined]\n        return result\n\n\ndef clone_inputs(example_inputs):\n    res: Union[dict[Any, Any], list[Any]]\n    if type(example_inputs) is dict:\n        res = dict(example_inputs)\n        for key, value in res.items():\n            if isinstance(value, tuple):\n                res[key] = clone_inputs(value)\n            else:\n                assert isinstance(value, torch.Tensor), type(value)\n                res[key] = clone_input(value)\n        return res\n\n    res = list(example_inputs)\n    for i in range(len(res)):\n        if isinstance(res[i], torch.Tensor):\n            res[i] = clone_input(res[i])\n    return res\n\n\ndef skip_frame_if_in_functorch_mode(val: torch.Tensor):\n    try:\n        val.data_ptr()  # will throw for functorch tensors\n    except RuntimeError as e:\n        from .exc import SkipFrame\n\n        # This will be GradTrackingTensor/BatchedTensor/etc\n        functorch_subclass_name = re.sub(r\"\\(.*\", \"\", repr(val))\n        raise SkipFrame(\n            f\"torch.compile cannot be run in context: {functorch_subclass_name}\"\n        ) from e\n\n\n@contextmanager\ndef preserve_rng_state():\n    disable_functorch = torch._C._DisableFuncTorch\n    disable_current_modes = torch.utils._python_dispatch._disable_current_modes\n    with disable_current_modes(), disable_functorch():\n        rng_state = torch.clone(torch.random.get_rng_state())\n        skip_frame_if_in_functorch_mode(rng_state)\n        if torch.cuda.is_available():\n            cuda_rng_state = torch.clone(torch.cuda.get_rng_state())\n    try:\n        yield\n    finally:\n        with torch.utils._python_dispatch._disable_current_modes():\n            torch.random.set_rng_state(rng_state)\n            if torch.cuda.is_available():\n                torch.cuda.set_rng_state(cuda_rng_state)  # type: ignore[possibly-undefined]\n\n\ndef is_jit_model(\n    model0,\n):\n    return isinstance(\n        model0,\n        (\n            torch.jit._trace.TopLevelTracedModule,\n            torch.jit._script.RecursiveScriptModule,\n            torch.jit.ScriptFunction,\n            torch.jit.ScriptModule,\n        ),\n    )\n\n\ndef torchscript(model, example_inputs, verbose=False):\n    if is_jit_model(model):\n        # already done?\n        return model\n\n    try:\n        return torch.jit.trace(model, example_inputs)\n    except Exception:\n        try:\n            return torch.jit.script(model)\n        except Exception:\n            if verbose:\n                log.exception(\"jit error\")\n            else:\n                log.error(\"Both torch.jit.trace and torch.jit.script failed\")\n    return None\n\n\ndef getfile(obj):\n    try:\n        return inspect.getfile(obj)\n    except (TypeError, OSError):\n        return None\n\n\ndef is_namedtuple(obj):\n    \"\"\"Test if an object is a namedtuple or a torch.return_types.* quasi-namedtuple\"\"\"\n    return is_namedtuple_cls(type(obj))\n\n\ndef is_namedtuple_cls(cls):\n    \"\"\"Test if an object is a namedtuple or a (torch.return_types|torch.autograd.forward_ad).* quasi-namedtuple\"\"\"\n    try:\n        if issubclass(cls, tuple):\n            module = getattr(cls, \"__module__\", None)\n            if module in (\"torch.return_types\", \"torch.autograd.forward_ad\"):\n                return True\n            if isinstance(getattr(cls, \"_fields\", None), tuple) and callable(\n                getattr(cls, \"_make\", None)\n            ):\n                # The subclassing style namedtuple can have an extra base `typing.Generic`\n                bases = tuple(t for t in cls.__bases__ if t is not Generic)\n                if bases == (tuple,):\n                    # This is a namedtuple type directly created by `collections.namedtuple(...)`\n                    return True\n                if bases and any(\n                    (\n                        # Subclass of namedtuple\n                        is_namedtuple_cls(t)\n                        # For subclasses of namedtuple, the __new__ method should not be customized\n                        and cls.__new__ is t.__new__\n                    )\n                    for t in bases\n                ):\n                    return True\n    except TypeError:\n        pass\n    return False\n\n\n@functools.lru_cache(1)\ndef namedtuple_fields(cls) -> tuple[str, ...]:\n    \"\"\"Get the fields of a namedtuple or a torch.return_types.* quasi-namedtuple\"\"\"\n    if cls is slice:\n        return (\"start\", \"stop\", \"step\")\n\n    assert issubclass(cls, tuple)\n    if hasattr(cls, \"_fields\"):\n        # normal namedtuples\n        return cls._fields\n\n    @dataclasses.dataclass\n    class Marker:\n        index: int\n\n    # frustrating ones e.g. torch.return_types.max\n    assert cls.__module__ == \"torch.return_types\"\n    obj = cls(map(Marker, range(cls.n_fields)))\n    fields: dict[str, int] = {}\n    for name in dir(obj):\n        if name[0] != \"_\" and isinstance(getattr(obj, name), Marker):\n            fields[name] = getattr(obj, name).index\n    assert len(fields) == cls.n_fields\n    return tuple(sorted(fields, key=fields.get))  # type: ignore[arg-type]\n\n\ndef checkpoint_params(gm):\n    with torch.no_grad():\n        rng_state = torch.clone(torch.random.get_rng_state())\n        if torch.cuda.is_available():\n            cuda_rng_state = torch.clone(torch.cuda.get_rng_state())\n        saved_state = [\n            (param, param._version, torch.clone(param))\n            for param in itertools.chain(gm.parameters(), gm.buffers())\n        ]\n\n    def restore():\n        with torch.no_grad():\n            torch.random.set_rng_state(rng_state)\n            if torch.cuda.is_available():\n                torch.cuda.set_rng_state(cuda_rng_state)\n            for param, version, original_value in saved_state:\n                if param._version != version:\n                    param.copy_(original_value)\n\n    return restore\n\n\ndef timed(model, example_inputs, times=1):\n    if torch.cuda.is_available():\n        synchronize = torch.cuda.synchronize\n    else:\n        synchronize = nothing\n\n    synchronize()\n    gc.collect()\n    torch.manual_seed(1337)\n    t0 = time.perf_counter()\n    for _ in range(times):\n        result = model(*example_inputs)\n        synchronize()\n    t1 = time.perf_counter()\n    return result, t1 - t0  # type: ignore[possibly-undefined]\n\n\ndef check_is_cuda(gm, example_inputs):\n    return all(x.is_cuda for x in itertools.chain(example_inputs, gm.parameters(True)))\n\n\n@lru_cache(32)\ndef rot_n_helper(n):\n    assert n > 1\n    vars = [f\"v{i}\" for i in range(n)]\n    rotated = reversed(vars[-1:] + vars[:-1])\n    fn = eval(f\"lambda {','.join(vars)}: ({','.join(rotated)})\")\n    fn.__name__ = f\"rot_{n}_helper\"\n    return fn\n\n\ncommon_constant_types: set[type] = {\n    int,\n    float,\n    complex,\n    bool,\n    str,\n    bytes,\n    type(None),\n    Ellipsis.__class__,\n    NotImplemented.__class__,\n    types.CodeType,\n    # Commonly used immutable types from torch.\n    torch.device,\n    torch.dtype,\n    torch.memory_format,\n    torch.layout,\n    torch.finfo,\n    torch.iinfo,\n    torch.nn.attention.SDPBackend,\n    torch.cuda._CudaDeviceProperties,\n}\n\nif has_triton_package():\n    import triton\n\n    common_constant_types.add(triton.language.dtype)\n\n\"\"\"\n    Difference between is_safe_constant and common_constant_types.\n    * common_constant_types: Constants would be wrapped by VariableBuilder.wrap_literal\n                             as ConstantVariable.\n    * is_safe_constant: Constants can be loaded by LOAD_CONST bytecode.\n\"\"\"\n\n\ndef is_safe_constant(v):\n    if istype(v, (tuple, frozenset)):\n        return all(map(is_safe_constant, v))\n    return isinstance(\n        v,\n        (\n            enum.Enum,\n            type,\n            torch.Size,\n            typing._GenericAlias,  # type: ignore[attr-defined]\n            types.GenericAlias,\n        ),\n    ) or istype(\n        v,\n        common_constant_types | {slice},\n    )\n\n\n@functools.cache\ndef common_constants():\n    return {\n        # We zero-one specialize shapes, so specialize these constants\n        # too\n        0,\n        1,\n    }\n\n\ndef is_torch_sym(value: Any) -> TypeGuard[Union[torch.SymBool, torch.SymInt]]:\n    return isinstance(value, (torch.SymBool, torch.SymInt)) and not isinstance(\n        value.node, torch.nested._internal.nested_int.NestedIntNode\n    )\n\n\ndef is_int_specialization_case(value, source):\n    from .source import is_from_defaults\n\n    return not TracingContext.get().force_unspec_int_unbacked_size_like and (\n        # Assume integers from global variables want to be specialized\n        not source.guard_source().is_local()\n        # Assume that integers that came from NN modules want to be\n        # specialized (as we don't expect users to be changing the\n        # NN modules on the fly), unless explicitly disabled\n        or (\n            source.guard_source().is_specialized_nn_module()\n            and not config.allow_unspec_int_on_nn_module\n        )\n        or (\n            source.guard_source().is_unspecialized_builtin_nn_module()\n            and not config.allow_unspec_int_on_nn_module\n        )\n        or (\n            source.guard_source().is_unspecialized_nn_module()\n            and not config.allow_unspec_int_on_nn_module\n        )\n        or is_from_defaults(source)\n        # TODO: Delete this condition when rollout is done.  NB: this\n        # condition never evaluates True in open source\n        or (\n            not justknobs_check(\"pytorch/dynamo:enable_unspecialize_zero_one_plain_int\")\n            and value in common_constants()\n        )\n    )\n\n\ndef specialize_symnode(arg):\n    from .variables import ConstantVariable, LazyVariableTracker, SymNodeVariable\n\n    # Guard and specialize\n    if isinstance(arg, LazyVariableTracker) and not arg.is_realized():\n        # Find if the arg would be realized as SymNodeVariable later on. If yes,\n        # realize it and specialize. Else return the arg.\n\n        source = arg.original_source()\n        value = arg.original_value()\n\n        is_symnode_vt = is_torch_sym(value) or (\n            not config.specialize_int\n            and type(value) is int\n            and not is_int_specialization_case(value, source)\n        )\n\n        if not is_symnode_vt:\n            return arg\n\n    if isinstance(arg, SymNodeVariable):\n        return ConstantVariable.create(arg.evaluate_expr())\n    return arg\n\n\ndef guard_if_dyn(arg):\n    from .variables import ConstantVariable\n\n    arg = specialize_symnode(arg)\n\n    if isinstance(arg, ConstantVariable):\n        return arg.as_python_constant()\n\n    return arg\n\n\ndef check_constant_args(args, kwargs):\n    return all(x.is_python_constant() for x in itertools.chain(args, kwargs.values()))\n\n\ndef check_unspec_python_args(args, kwargs):\n    from .variables.constant import ConstantVariable\n    from .variables.tensor import UnspecializedPythonVariable\n\n    unspec_count = 0\n    for x in itertools.chain(args, kwargs.values()):\n        if isinstance(x, UnspecializedPythonVariable):\n            unspec_count += 1\n        elif not isinstance(x, ConstantVariable):\n            return False\n    return unspec_count > 0\n\n\ndef check_unspec_or_constant_args(args, kwargs):\n    # A fused version of:\n    # return check_constant_args(args, kwargs) or check_unspec_python_args(args, kwargs)\n    from .variables.tensor import UnspecializedPythonVariable\n\n    for x in itertools.chain(args, kwargs.values()):\n        if not (x.is_python_constant() or isinstance(x, UnspecializedPythonVariable)):\n            return False\n    return True\n\n\ndef check_numpy_ndarray_args(args, kwargs):\n    from .variables.tensor import NumpyNdarrayVariable\n\n    return any(\n        isinstance(x, NumpyNdarrayVariable)\n        for x in itertools.chain(args, kwargs.values())\n    )\n\n\ndict_keys: type[KeysView[Any]] = type({}.keys())\ndict_values: type[ValuesView[Any]] = type({}.values())\ndict_items: type[ItemsView[Any, Any]] = type({}.items())\nodict_values: type[ValuesView[Any]] = type(OrderedDict().values())\ntuple_iterator: type[Iterator[Any]] = type(iter(()))\nrange_iterator: type[Iterator[Any]] = type(iter(range(0)))\ntuple_iterator_len = tuple_iterator.__length_hint__  # type: ignore[attr-defined]\nobject_new = object.__new__\ndict_new = dict.__new__\ndict_methods = {\n    method\n    for method in itertools.chain(dict.__dict__.values(), OrderedDict.__dict__.values())\n    if callable(method)\n}\nset_methods = {method for method in set.__dict__.values() if callable(method)}\nfrozenset_methods = {\n    method for method in frozenset.__dict__.values() if callable(method)\n}\n\ntuple_new = tuple.__new__\ntuple_methods = {method for method in tuple.__dict__.values() if callable(method)}\nlist_methods = {method for method in list.__dict__.values() if callable(method)}\nlist_getitem = list.__getitem__\n\nstr_methods = {method for method in str.__dict__.values() if callable(method)}\n\n\ndef builtin_dict_keys(d):\n    # Avoids overridden keys method of the dictionary\n    assert isinstance(d, dict)\n    return dict.keys(d)\n\n\ndef get_items_from_dict(obj):\n    # Get items without calling the user defined __getitem__ or keys method.\n    assert isinstance(obj, dict)\n    if istype(obj, (dict, OrderedDict)):\n        return obj.items()\n    elif isinstance(obj, OrderedDict):\n        return [(k, OrderedDict.__getitem__(obj, k)) for k in OrderedDict.keys(obj)]\n    else:\n        return [(k, dict.__getitem__(obj, k)) for k in dict.keys(obj)]\n\n\ndef nn_module_new(cls):\n    obj = object_new(cls)\n    torch.nn.Module.__init__(obj)\n    return obj\n\n\ndef product(it):\n    return functools.reduce(operator.mul, it, 1)\n\n\ndef tuple_iterator_getitem(it, index):\n    _, (obj,), start = it.__reduce__()\n    return obj[start + index]\n\n\ndef dataclass_fields(cls):\n    return torch._dynamo.disable(dataclasses.fields)(cls)\n\n\niter_next = next\n\n\ndef normalize_range_iter(range_iter) -> tuple[int, int, int]:\n    _, (range_obj,), maybe_idx = range_iter.__reduce__()\n    # In 3.12+, `maybe_idx` could be None, and `range_obj.start` would've been\n    # already incremented by the current index.\n    start = range_obj.start + (maybe_idx or 0)\n    stop = range_obj.stop\n    step = range_obj.step\n    return (start, stop, step)\n\n\ndef to_subclass(t, cls):\n    return t.as_subclass(cls)\n\n\ndict_getitem = dict.__getitem__\n\n\ndef dict_keys_getitem(d, n):\n    # Call dict(d) to prevent calling overridden __iter__/keys\n    dict_class = dict\n    if isinstance(d, OrderedDict):\n        dict_class = OrderedDict\n    return next(itertools.islice(dict_class.keys(d), n, n + 1))\n\n\ndef set_getitem(s, n):\n    # Set ordering might not be stable\n    return list(s)[n]\n\n\ndef enum_repr(value, local):\n    # enum class can override __str__ method. Use __class__ and name attribute\n    # to extract the class name and key name.\n    name = value.__class__.__name__\n    val = value.name\n    scope = \"L\" if local else \"G\"\n    local_name = f'{scope}[\"{name}\"].{val}'\n    return local_name\n\n\ndef set_example_value(node, example_value):\n    # NB: example_value is a bit of a misnomer, because this is always a fake\n    # tensor of some sort.  Furthermore, these example values serve as the\n    # runtime state of Dynamo tracing, which means if metadata mutation\n    # occurs, the example_value gets directly updated (so you can't rely on\n    # this to accurately reflect what the state of the value was at the time\n    # the program was traced).\n    node.meta[\"example_value\"] = example_value\n    shape_env = TracingContext.get().fake_mode.shape_env\n    if (\n        symbol_to_path\n        := torch.fx.experimental.symbolic_shapes.compute_unbacked_bindings(\n            shape_env, example_value\n        )\n    ):\n        node.meta[\"unbacked_bindings\"] = symbol_to_path\n\n\ndef _get_fake_tensor(vt):\n    fake_tensor = vt.as_proxy().node.meta.get(\"example_value\")\n    if not is_fake(fake_tensor):\n        from . import graph_break_hints\n        from .exc import unimplemented_v2\n\n        unimplemented_v2(\n            gb_type=\"Cannot check Tensor object identity without its fake value\",\n            context=str(fake_tensor),\n            explanation=\"TensorVariable is missing a fake example_value.\",\n            hints=[*graph_break_hints.DYNAMO_BUG],\n        )\n    return fake_tensor\n\n\ndef slice_length(s: slice, seq_len: int) -> int:\n    start, stop, step = s.indices(seq_len)\n    return max(0, (stop - start + (step - (1 if step > 0 else -1))) // step)\n\n\ndef raise_args_mismatch(tx, name):\n    from torch._dynamo.exc import raise_observed_exception\n    from torch._dynamo.variables import ConstantVariable\n\n    raise_observed_exception(\n        TypeError,\n        tx,\n        args=[ConstantVariable(f\"wrong number of arguments for {name}() call\")],\n    )\n\n\ndef iter_contains(items, search, tx, check_tensor_identity=False):\n    from .variables import (\n        BuiltinVariable,\n        ConstantVariable,\n        TensorVariable,\n        VariableTracker,\n    )\n\n    if search.is_python_constant():\n        found_const = any(\n            x.is_python_constant()\n            and x.as_python_constant() == search.as_python_constant()\n            for x in items\n        )\n        return ConstantVariable.create(found_const)\n\n    must_check_tensor_id = False\n    if check_tensor_identity and isinstance(search, TensorVariable):\n        must_check_tensor_id = True\n        # Match of Tensor means match of FakeTensor\n        search = _get_fake_tensor(search)\n\n    found: Optional[VariableTracker] = None\n    for x in items:\n        if must_check_tensor_id:\n            if isinstance(x, TensorVariable):\n                if search is _get_fake_tensor(x):  # Object equivalence\n                    return ConstantVariable.create(True)\n        else:\n            check = BuiltinVariable(operator.eq).call_function(tx, [x, search], {})\n            if found is None:\n                found = check\n            else:\n                found = BuiltinVariable(operator.or_).call_function(\n                    tx, [check, found], {}\n                )\n    if found is None:\n        found = ConstantVariable.create(False)\n    return found\n\n\ndef key_is_id(\n    k: Any,\n) -> TypeIs[Union[torch.Tensor, torch.nn.Module, MethodWrapperType]]:\n    \"\"\"Returns whether it indexes dictionaries using its id\"\"\"\n    return isinstance(k, (torch.Tensor, torch.nn.Module, MethodWrapperType))\n\n\ndef key_to_id(value):\n    return [id(k) if key_is_id(k) else k for k in value.keys()]\n\n\ndef const_repr(x, *, local) -> str:\n    from .trace_rules import is_builtin_callable\n\n    if isinstance(x, (list, tuple)):\n        elems_repr = \",\".join(const_repr(s, local=local) for s in x)\n        if isinstance(x, list):\n            return f\"[{elems_repr}]\"\n        else:\n            assert isinstance(x, tuple)\n            if len(x) == 1:\n                return f\"({elems_repr},)\"\n            else:\n                return f\"({elems_repr})\"\n    elif isinstance(x, enum.Enum):\n        # To workaround repr(Enum) returning invalid global reference before python 3.11\n        # by calling enum_repr and removing quotes to render enum in guard code.\n        return enum_repr(x, local=local).replace(\"'\", \"\")\n    elif is_builtin_callable(x):\n        return x.__name__\n    elif isinstance(x, type):\n\n        def fullname(o):\n            klass = o.__class__\n            module = klass.__module__\n            if module == \"builtins\":\n                return klass.__qualname__  # avoid outputs like 'builtins.str'\n            return module + \".\" + klass.__qualname__\n\n        return fullname(x)\n    else:\n        return f\"{x!r}\"\n\n\ndef dict_keys_repr(const_keys, *, local) -> str:\n    keys_str = \",\".join(const_repr(s, local=local) for s in const_keys)\n    return \"[\" + keys_str + \"]\"\n\n\nGLOBAL_KEY_PREFIX = \"__dict_key\"\n\n\nfrom torch._subclasses import UnsupportedFakeTensorException  # noqa: F401\n\n\ndef get_safe_global_name(tx, root, obj):\n    # The global_mangled_class_name should be different for different\n    # invocations of torch.compile. Otherwise, we can run into a situation\n    # where multiple torch.compile invocations reuse the same global name,\n    # but the global's lifetime is tied to the first invocation (and\n    # may be deleted when the first torch.compile invocation is deleted)\n    # We mangle it based off of the output_graph's id.\n    return f\"{root}_{id(obj)}_c{tx.output.compile_id}\"\n\n\ndef is_in(item: Any, *containers) -> bool:\n    for container in containers:\n        if item in container:\n            return True\n    return False\n\n\ndef get_unique_name_wrt(prefix: str, *containers, requires_suffix=False) -> str:\n    \"\"\"\n    Return a name that starts with `prefix` and is not in any of the\n    `containers` (e.g., map, set).\n    \"\"\"\n    if not requires_suffix and not is_in(prefix, *containers):\n        return prefix\n\n    for i in itertools.count():\n        candidate = f\"{prefix}_{i}\"\n        if not is_in(candidate, *containers):\n            return candidate\n\n    raise AssertionError(\"unreachable\")\n\n\ndef wrap_fake_exception(fn):\n    try:\n        return fn()\n    except UnsupportedFakeTensorException as e:\n        from .exc import unimplemented_v2\n\n        msg = f\"Encountered exception ({e.reason}) during fake tensor propagation.\"\n        log.warning(msg)\n        unimplemented_v2(\n            gb_type=\"Fake tensor propagation exception\",\n            context=str(e.reason),\n            explanation=msg,\n            hints=[],\n            from_exc=e,\n        )\n\n\ndef deepcopy_to_fake_tensor(obj, fake_mode):\n    with torch._subclasses.fake_tensor.FakeCopyMode(fake_mode):\n        return wrap_fake_exception(lambda: copy.deepcopy(obj))\n\n\ndef rmse(ref, res):\n    \"\"\"\n    Calculate root mean squared error\n    \"\"\"\n    return torch.sqrt(torch.mean(torch.square(ref - res)))\n\n\ndef same(\n    ref,\n    res,\n    fp64_ref=None,\n    cos_similarity=False,\n    tol=1e-4,\n    equal_nan=False,\n    exact_dtype=True,\n    relax_numpy_equality=False,\n    ignore_non_fp=False,\n    log_error=log.error,\n    use_larger_multiplier_for_smaller_tensor=False,\n    force_max_multiplier: bool = False,\n):\n    \"\"\"Check correctness to see if ref and res match\"\"\"\n    if fp64_ref is None:\n        fp64_ref = ref\n    if isinstance(\n        ref, (list, tuple, collections.deque, torch.nn.ParameterList, torch.Size)\n    ):\n        assert isinstance(res, (list, tuple, collections.deque)), (\n            f\"type mismatch {type(ref)} {type(res)}\"\n        )\n        if len(ref) != len(res):\n            log_error(\"Length mismatch\")\n            return False\n        return len(ref) == len(res) and all(\n            same(\n                ai,\n                bi,\n                fp64_refi,\n                cos_similarity,\n                tol,\n                equal_nan,\n                exact_dtype,\n                relax_numpy_equality,\n                ignore_non_fp,\n                log_error=log_error,\n                use_larger_multiplier_for_smaller_tensor=use_larger_multiplier_for_smaller_tensor,\n                force_max_multiplier=force_max_multiplier,\n            )\n            for ai, bi, fp64_refi in zip(ref, res, fp64_ref)\n        )\n    elif type(ref).__name__ == \"QuestionAnsweringModelOutput\":\n        # This skips checking accuracy for start_logits/end_logits.\n        # Tentatively, start_logits/end_logits appear to be very prone to\n        # inaccuracies and is somewhat subsumed by checking the loss.\n        return same(\n            ref.loss,\n            res.loss,\n            fp64_ref.loss,\n            cos_similarity,\n            tol,\n            equal_nan,\n            exact_dtype,\n            relax_numpy_equality,\n            ignore_non_fp,\n            log_error=log_error,\n            use_larger_multiplier_for_smaller_tensor=use_larger_multiplier_for_smaller_tensor,\n            force_max_multiplier=force_max_multiplier,\n        )\n    elif isinstance(ref, dict):\n        assert isinstance(res, dict)\n        assert set(ref.keys()) == set(res.keys()), (\n            f\"keys mismatch {set(ref.keys())} == {set(res.keys())}\"\n        )\n        for k in sorted(ref.keys()):\n            if not (\n                same(\n                    ref[k],\n                    res[k],\n                    fp64_ref[k],\n                    cos_similarity=cos_similarity,\n                    tol=tol,\n                    equal_nan=equal_nan,\n                    exact_dtype=exact_dtype,\n                    relax_numpy_equality=relax_numpy_equality,\n                    ignore_non_fp=ignore_non_fp,\n                    log_error=log_error,\n                    use_larger_multiplier_for_smaller_tensor=use_larger_multiplier_for_smaller_tensor,\n                    force_max_multiplier=force_max_multiplier,\n                )\n            ):\n                log_error(\"Accuracy failed for key name %s\", k)\n                return False\n        return True\n    elif isinstance(ref, set):\n        assert isinstance(res, set)\n        assert set(ref) == set(res), f\"elements mismatch {set(ref)} == {set(res)}\"\n        return True\n    elif isinstance(ref, (torch.Tensor, float)):\n        assert not isinstance(ref, torch._subclasses.FakeTensor)\n        assert not isinstance(res, torch._subclasses.FakeTensor)\n\n        def to_tensor(t):\n            return t if isinstance(t, torch.Tensor) else torch.tensor(t)\n\n        ref, res, fp64_ref = (to_tensor(val) for val in (ref, res, fp64_ref))\n\n        if ref.is_sparse:\n            assert res.is_sparse\n            ref = ref.to_dense()\n            res = res.to_dense()\n        assert isinstance(res, torch.Tensor), f\"type mismatch {type(ref)} {type(res)}\"\n        if exact_dtype:\n            if ref.dtype != res.dtype:\n                log_error(\"dtype mismatch %s, %s\", ref.dtype, res.dtype)\n                return False\n            if ref.dtype == torch.bool:\n                if ignore_non_fp:\n                    return True\n                # triton stores bool as int8, so add this for more accurate checking\n                r = torch.allclose(\n                    ref.to(dtype=torch.uint8),\n                    res.to(dtype=torch.uint8),\n                    atol=tol,\n                    rtol=tol,\n                    equal_nan=equal_nan,\n                )\n                if not r:\n                    log_error(\"Accuracy failed: uint8 tensor did not match\")\n                return r\n\n        if cos_similarity:\n            ref = ref.flatten().to(torch.float32)\n            res = res.flatten().to(torch.float32)\n            if torch.allclose(ref, res, atol=tol, rtol=tol, equal_nan=True):\n                # early exit that handles zero/nan better\n                # cosine_similarity(zeros(10), zeros(10), dim=0) is 0\n                return True\n            score = torch.nn.functional.cosine_similarity(ref, res, dim=0, eps=1e-6)\n            if score < 0.99:\n                log.warning(\"Similarity score=%s\", score.detach().cpu().item())\n            return score >= 0.99\n        else:\n            if not exact_dtype:\n                ref = ref.to(res.dtype)\n\n            # First try usual allclose\n            if torch.allclose(ref, res, atol=tol, rtol=tol, equal_nan=equal_nan):\n                return True\n\n            # Check error from fp64 version\n            if fp64_ref.dtype == torch.float64:\n                # Fix a corner case that res and fp64_ref does not contains NaN and match (with loose tolerance)\n                # while the ref contains NaN. In this case, RMSE should not match any ways.\n                # But res is 'BETTER' than ref so we count it pass.\n                #\n                # This happens for Super_SloMo when loop ordering after fusion is enabled:\n                # https://gist.github.com/shunting314/11f235c70f7db0d52718d26f4a701cab\n                loose_tol = 1e-2 * 4\n                if (\n                    not fp64_ref.isnan().any()\n                    and not res.isnan().any()\n                    and ref.isnan().any()\n                    and torch.allclose(\n                        fp64_ref.to(dtype=res.dtype),\n                        res,\n                        atol=loose_tol,\n                        rtol=loose_tol,\n                        equal_nan=equal_nan,\n                    )\n                ):\n                    return True\n                ref_error = rmse(fp64_ref, ref).item()\n                # ref unable to produce this with stable numerics in this precision, ignore\n                if math.isnan(ref_error):\n                    log.warning(\n                        \"Found nan in reference. Consider running in higher precision.\"\n                    )\n\n                res_error = rmse(fp64_ref, res).item()\n\n                def get_multiplier():\n                    # In some particular cases, we expect high difference in results.\n                    # At the moment one of this cases is inductor freezing bfloat16 convolution const folding.\n                    # In case of it the res_error is at least one order of magnitude higher.\n                    if force_max_multiplier:\n                        return 10.0\n                    # In the case of using AMP (Automatic Mixed Precision), certain models have\n                    # failed the benchmark's correctness check. However, the end-to-end model's\n                    # accuracy when comparing AMP with FP32 is within a difference of less than 0.1%.\n                    # Thus, it's possible that the correctness check failures for these models are\n                    # false alarms. We use multiplier of 3 instead of 2 to avoid these false alarms.\n                    multiplier = (\n                        3.0 if res.dtype in (torch.float16, torch.bfloat16) else 2.0\n                    )\n\n                    if use_larger_multiplier_for_smaller_tensor and (\n                        fp64_ref.numel() <= 10\n                    ):\n                        multiplier = 10.0\n                    elif use_larger_multiplier_for_smaller_tensor and (\n                        fp64_ref.numel() <= 500\n                    ):\n                        multiplier = 8.0\n                    elif (\n                        fp64_ref.numel() < 1000\n                        or (ref.ndim == 4 and ref.shape[-1] == ref.shape[-2] == 1)\n                        # large tol means a benchmark has been specified as REQUIRE_HIGHER_TOLERANCE\n                        or tol >= 2 * 1e-2\n                    ):\n                        # In the presence of noise, noise might dominate our error\n                        # metric for smaller tensors.\n                        # Similarly, for 1x1 kernels, there seems to be high noise with amp.\n                        multiplier = 3.0\n                    return multiplier\n\n                multiplier = get_multiplier()\n\n                passes_test = res_error <= (multiplier * ref_error + tol / 10.0)\n                if (\n                    not passes_test\n                    and equal_nan\n                    and math.isnan(ref_error)\n                    and math.isnan(res_error)\n                    # Some unit test for the accuracy minifier relies on\n                    # returning false in this case.\n                    and not torch._inductor.config.cpp.inject_relu_bug_TESTING_ONLY\n                ):\n                    passes_test = True\n                if not passes_test:\n                    log_error(\n                        \"RMSE (res-fp64): %.5f, (ref-fp64): %.5f and shape=%s. res.dtype: %s, multiplier: %f, tol: %f\"\n                        \", use_larger_multiplier_for_smaller_tensor: %d\",\n                        res_error,\n                        ref_error,\n                        res.size(),\n                        res.dtype,\n                        multiplier,\n                        tol,\n                        use_larger_multiplier_for_smaller_tensor,\n                    )\n                return passes_test\n\n            if ignore_non_fp:\n                return True\n\n            log_error(\"Accuracy failed: allclose not within tol=%s\", tol)\n            return False\n    elif isinstance(ref, (str, int, type(None), bool, torch.device)):\n        if ignore_non_fp:\n            return True\n        r = ref == res\n        if not r:\n            log_error(\"Accuracy failed (%s): %s != %s\", type(ref), ref, res)\n        return r\n    elif is_numpy_int_type(ref) or is_numpy_float_type(ref):\n        if relax_numpy_equality and not (\n            is_numpy_int_type(res) or is_numpy_float_type(res)\n        ):\n            ref = ref.item()\n        r = (type(ref) is type(res)) and (ref == res)\n        if not r:\n            log_error(\"Accuracy failed (numpy): %s != %s\", ref, res)\n        return r\n    elif is_numpy_ndarray(ref):\n        return (type(ref) is type(res)) and same(\n            torch.as_tensor(ref),\n            torch.as_tensor(res),\n            fp64_ref,\n            cos_similarity=cos_similarity,\n            tol=tol,\n            equal_nan=equal_nan,\n            exact_dtype=exact_dtype,\n            relax_numpy_equality=relax_numpy_equality,\n            ignore_non_fp=ignore_non_fp,\n            log_error=log_error,\n            use_larger_multiplier_for_smaller_tensor=use_larger_multiplier_for_smaller_tensor,\n        )\n    elif type(ref).__name__ in (\n        \"MaskedLMOutput\",\n        \"Seq2SeqLMOutput\",\n        \"CausalLMOutputWithCrossAttentions\",\n        \"LongformerMaskedLMOutput\",\n        \"Instances\",\n        \"SquashedNormal\",\n        \"Boxes\",\n        \"Normal\",\n        \"TanhTransform\",\n        \"Foo\",\n        \"Variable\",\n    ):\n        assert type(ref) is type(res)\n        return all(\n            same(\n                getattr(ref, key),\n                getattr(res, key),\n                getattr(fp64_ref, key),\n                cos_similarity=cos_similarity,\n                tol=tol,\n                equal_nan=equal_nan,\n                exact_dtype=exact_dtype,\n                relax_numpy_equality=relax_numpy_equality,\n                ignore_non_fp=ignore_non_fp,\n                log_error=log_error,\n                use_larger_multiplier_for_smaller_tensor=use_larger_multiplier_for_smaller_tensor,\n            )\n            for key in ref.__dict__.keys()\n        )\n    else:\n        raise RuntimeError(f\"unsupported type: {type(ref).__name__}\")\n\n\ndef format_func_info(code):\n    short_filename = code.co_filename.split(\"/\")[-1]\n    return f\"'{code.co_name}' ({short_filename}:{code.co_firstlineno})\"\n\n\n@contextlib.contextmanager\ndef disable_cache_limit():\n    prior = config.recompile_limit\n    config.recompile_limit = sys.maxsize\n    prior_acc_limit = config.accumulated_recompile_limit\n    config.accumulated_recompile_limit = sys.maxsize\n\n    try:\n        yield\n    finally:\n        config.recompile_limit = prior\n        config.accumulated_recompile_limit = prior_acc_limit\n\n\n# map from transformed code back to original user code\norig_code_map = ExactWeakKeyDictionary()\n\n# keep a record of code_obj -> list of guard failure reasons for logging\nguard_failures: collections.defaultdict[Any, list[Any]] = collections.defaultdict(list)\n\n# Keep a record of graph break reasons for logging\ngraph_break_reasons: list[torch._dynamo.output_graph.GraphCompileReason] = []\n\n# keep record of compiled code, if we are in \"error if recompile\"\n# to track code that dynamo has compiled previously\nseen_code_map = ExactWeakKeyDictionary()\n\n\n# return same dir unless user changes config between calls\n@functools.cache\ndef _get_debug_dir(root_dir):\n    dir_name = (\n        \"run_\"\n        + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n        # use pid to avoid conflicts among ranks\n        + \"-pid_\"\n        + str(os.getpid())\n    )\n    return os.path.join(root_dir, dir_name)\n\n\ndef get_debug_dir():\n    debug_root = config.debug_dir_root\n    return _get_debug_dir(debug_root)\n\n\ndef extract_fake_example_value(node, required=True):\n    if \"example_value\" in node.meta and is_fake(node.meta[\"example_value\"]):\n        return node.meta[\"example_value\"]\n    elif required:\n        from torch._dynamo.exc import unimplemented_v2\n\n        from . import graph_break_hints\n\n        unimplemented_v2(\n            gb_type=\"Missing FakeTensor example value\",\n            context=str(node),\n            explanation=f\"`FakeTensor` example value was required for {node} but not available.\",\n            hints=[*graph_break_hints.DYNAMO_BUG],\n        )\n    else:\n        return None\n\n\ndef ensure_graph_fake(e, tx):\n    assert maybe_get_fake_mode(e) is tx.fake_mode\n    return e\n\n\ndef get_fake_values_from_nodes(tx, nodes, allow_non_graph_fake):\n    def visit(n: torch.fx.Node):\n        if n.op == \"call_function\" and \"example_value\" not in n.meta:\n            # fake tensor validity is checked inside get_fake_value using\n            # ensure_graph_fake\n            return get_fake_value(n, tx, allow_non_graph_fake)\n\n        elif n.op == \"get_attr\" and \"example_value\" not in n.meta:\n            assert n.target in tx.output.nn_modules\n            gm = tx.output.nn_modules[n.target]\n            assert isinstance(gm, torch.fx.GraphModule)\n            return gm\n\n        out = n.meta[\"example_value\"]\n        if not allow_non_graph_fake and isinstance(out, torch.Tensor):\n            return ensure_graph_fake(out, tx)\n        return out\n\n    return torch.fx.node.map_arg(nodes, visit)\n\n\ndef get_fake_value(node, tx, allow_non_graph_fake=False):\n    \"\"\"\n    Run the computation represented by `node` using fake tensors and return the result.\n\n    allow_non_graph_fake: whether to allow the return result to be:\n        1. non-fake or 2. fake that is not created by this instance of Dynamo.\n        If `True`, you must be prepared to deal with such return values, ideally\n        by further wrapping them as this graph's fakes.\n    \"\"\"\n    from torch.utils._sympy.value_ranges import ValueRangeError\n\n    from .exc import (\n        TorchRuntimeError,\n        unimplemented_v2,\n        Unsupported,\n        UserError,\n        UserErrorType,\n    )\n\n    op = node.op\n\n    # FX Node should always return the same fake value\n    if \"example_value\" in node.meta and is_fake(node.meta[\"example_value\"]):\n        return node.meta[\"example_value\"]\n\n    args, kwargs = get_fake_values_from_nodes(\n        tx, (node.args, node.kwargs), allow_non_graph_fake\n    )\n\n    if (\n        torch._dynamo.config.use_graph_deduplication\n        or torch._dynamo.config.track_nodes_for_deduplication\n    ):\n        flat_args_kwargs = get_fake_values_from_nodes(\n            tx, _get_flat_args(node, {}), allow_non_graph_fake\n        )\n        id_to_initial_version = {\n            id(arg): arg._version for arg in flat_args_kwargs if is_fake(arg)\n        }\n    else:\n        flat_args_kwargs = []\n        id_to_initial_version = {}\n\n    nnmodule = None\n    if op == \"call_method\" and len(args) > 0 and isinstance(args[0], torch.nn.Module):\n        # If the first argument is nn.Module, should copy to fake mode.\n        args = (deepcopy_to_fake_tensor(args[0], tx.fake_mode),) + tuple(args[1:])\n\n    if op == \"call_module\":\n        nnmodule = tx.output.nn_modules[node.target]\n\n        if is_lazy_module(nnmodule) and hasattr(nnmodule, \"_initialize_hook\"):\n            # In the case of a lazy module, we want to run\n            # the pre-hooks which initialize it.\n            # Afterwards, lazy module deletes its pre-hooks\n            # to avoid treating it as lazy on subsequent recompile.\n            nnmodule._infer_parameters(nnmodule, args)\n\n        # no matter it's lazy module or not, we should copy to fake mode.\n        nnmodule = deepcopy_to_fake_tensor(nnmodule, tx.fake_mode)\n\n    if node.name in [\"interpolate\", \"is_integer\", \"wrapped_gradient\"] or any(\n        isinstance(a, complex) for a in args\n    ):\n        # We need to specialize symfloats for now. Eventually we should do a tensorify pass in dynamo.\n        args = tuple(\n            float(arg)\n            if isinstance(arg, torch.SymFloat) and arg.node.hint is not None\n            else arg\n            for arg in args\n        )\n\n    try:\n        with tx.fake_mode, enable_python_dispatcher():\n            ret_val = wrap_fake_exception(\n                lambda: run_node(tx.output, node, args, kwargs, nnmodule)\n            )\n    except Unsupported:\n        raise\n    except RuntimeError as e:\n        cause: BaseException = e\n        if e.__cause__ is not None:\n            cause = e.__cause__\n\n        if isinstance(\n            cause, torch._subclasses.fake_tensor.DataDependentOutputException\n        ):\n            # capture_scalar_outputs only works for these ops right now\n            # see torch/_subclasses/fake_impls.py\n            if cause.func in (\n                torch.ops.aten.item.default,\n                torch.ops.aten._local_scalar_dense.default,\n            ):\n                # does this actually get triggered?\n                hints = [\n                    \"Enable tracing of data-dependent output operators with \"\n                    \"`torch._dynamo.config.capture_scalar_outputs = True`\",\n                ]\n            else:\n                hints = [\n                    \"Consider wrapping the operator into a PyTorch-understood custom operator \"\n                    \"(see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html)\",\n                ]\n            unimplemented_v2(\n                gb_type=\"Data dependent operator\",\n                context=str(cause.func),\n                explanation=f\"Operator `{cause.func}` has a non-Tensor output \"\n                \"whose value is dependent on the data of Tensor inputs.\",\n                hints=hints,\n            )\n        elif isinstance(\n            cause, torch._subclasses.fake_tensor.DynamicOutputShapeException\n        ):\n            if not torch._dynamo.config.capture_dynamic_output_shape_ops:\n                unimplemented_v2(\n                    gb_type=\"Dynamic shape operator\",\n                    context=str(cause.func),\n                    explanation=f\"Operator `{cause.func}`'s output shape depends on input Tensor data.\",\n                    hints=[\n                        \"Enable tracing of dynamic shape operators with \"\n                        \"`torch._dynamo.config.capture_dynamic_output_shape_ops = True`\",\n                    ],\n                )\n            else:\n                unimplemented_v2(\n                    gb_type=\"Dynamic shape operator (no meta kernel)\",\n                    context=str(cause.func),\n                    explanation=f\"Operator `{cause.func}` does not have a meta kernel that supports dynamic output shapes\",\n                    hints=[\n                        \"Please report an issue to PyTorch\",\n                    ],\n                )\n        elif isinstance(\n            cause, torch._subclasses.fake_tensor.UnsupportedOperatorException\n        ):\n            op = cause.func\n            import_suggestion = \"\"\n            if isinstance(op, torch._ops.OpOverload):\n                maybe_pystub = torch._C._dispatch_pystub(\n                    op._schema.name, op._schema.overload_name\n                )\n                if maybe_pystub is not None:\n                    module, ctx = maybe_pystub\n                    import_suggestion = (\n                        f\"It's possible that the support was implemented in \"\n                        f\"module `{module}` and you may need to `import {module}`\"\n                        f\"({ctx}), otherwise \"\n                    )\n            unimplemented_v2(\n                gb_type=\"Operator does not support running with fake tensors\",\n                context=f\"unsupported operator: {cause.func}\",\n                explanation=\"\",\n                hints=[\n                    f\"{import_suggestion}see \"\n                    \"https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit#heading=h.64r4npvq0w0\"\n                    \" for how to fix\",\n                ],\n            )\n        elif isinstance(\n            cause, torch.fx.experimental.symbolic_shapes.GuardOnDataDependentSymNode\n        ):\n            raise UserError(  # noqa: B904\n                UserErrorType.CONSTRAINT_VIOLATION,\n                str(cause),\n                case_name=\"constrain_as_size_example\",\n            )\n        elif isinstance(cause, ValueRangeError):\n            raise UserError(UserErrorType.CONSTRAINT_VIOLATION, e.args[0]) from e\n        elif isinstance(cause, TypeError) and \"argument\" in str(cause):\n            unimplemented_v2(\n                gb_type=\"TypeError when making fake tensor call\",\n                context=f\"TypeError {node.target}: {cause}\",\n                explanation=\"\",\n                hints=[],\n            )\n\n        raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None\n\n    if not allow_non_graph_fake:\n        _ = pytree.tree_map_only(\n            torch.Tensor, functools.partial(ensure_graph_fake, tx=tx), ret_val\n        )\n\n    if (\n        torch._dynamo.config.use_graph_deduplication\n        or torch._dynamo.config.track_nodes_for_deduplication\n    ):\n        tx.output.region_tracker.track_node_mutations(\n            node,\n            flat_args_kwargs,\n            id_to_initial_version,\n        )\n\n    return ret_val\n\n\n_current_node = threading.local()\n\n\ndef get_current_node():\n    return getattr(_current_node, \"value\", None)\n\n\n@contextmanager\ndef set_current_node(node):\n    old = get_current_node()\n    _current_node.value = node\n    try:\n        yield\n    finally:\n        _current_node.value = old\n\n\ndef run_node(tracer, node, args, kwargs, nnmodule):\n    \"\"\"\n    Runs a given node, with the given args and kwargs.\n\n    Behavior is dictated by a node's op.\n\n    run_node is useful for extracting real values out of nodes.\n    See get_real_value for more info on common usage.\n\n    Note: The tracer arg is only used for 'get_attr' ops\n    Note: The nnmodule arg is only used for 'call_module' ops\n\n    Nodes that are not call_function, call_method, call_module, or get_attr will\n    raise an AssertionError.\n    \"\"\"\n    op = node.op\n\n    with set_current_node(node):\n\n        def make_error_message(e):\n            return (\n                f\"Dynamo failed to run FX node with fake tensors: {op} {node.target}(*{args}, **{kwargs}): got \"\n                + repr(e)\n            )\n\n        from .exc import Unsupported\n\n        try:\n            if op == \"call_function\":\n                return node.target(*args, **kwargs)\n            elif op == \"call_method\":\n                if not hasattr(args[0], node.target):\n                    from .exc import unimplemented_v2\n\n                    unimplemented_v2(\n                        gb_type=\"Missing attribute when running call_method node\",\n                        context=\"\",\n                        explanation=make_error_message(\"attribute not defined\"),\n                        hints=[],\n                    )\n                return getattr(args[0], node.target)(*args[1:], **kwargs)\n            elif op == \"call_module\":\n                assert nnmodule is not None\n                return nnmodule(*args, **kwargs)\n            elif op == \"get_attr\":\n                return tracer.output_graph.get_submodule(node.target)\n            elif op == \"placeholder\":\n                assert \"example_value\" in node.meta\n                return node.meta[\"example_value\"]\n\n        except (NotImplementedError, UnsupportedFakeTensorException) as e:\n            # NB: mimic how wrap_fake_exception does it\n            from .exc import unimplemented_v2\n\n            hints = []\n            if isinstance(e, NotImplementedError):\n                hints = [\n                    \"If the op is a PyTorch op, please file an issue to PyTorch.\",\n                ]\n\n            unimplemented_v2(\n                gb_type=\"NotImplementedError/UnsupportedFakeTensorException when running FX node\",\n                context=\"\",\n                explanation=make_error_message(e),\n                hints=hints,\n                from_exc=e,\n            )\n        except Unsupported:\n            raise\n        except Exception as e:\n            raise RuntimeError(make_error_message(e)).with_traceback(\n                e.__traceback__\n            ) from e\n\n    raise AssertionError(op)\n\n\ndef get_real_value(node, tracer):\n    \"\"\"\n    Run the actual computation represented by `node` and return the result.\n    This will execute any dependent nodes in the graph as well.\n    \"\"\"\n    from .exc import TorchRuntimeError\n\n    cache = tracer.real_value_cache\n    if node in cache:\n        return cache[node]\n\n    op = node.op\n    args, kwargs = torch.fx.node.map_arg(  # type: ignore[misc]\n        (node.args, node.kwargs),\n        lambda n: get_real_value(n, tracer),\n    )\n\n    if op == \"placeholder\" and \"grapharg\" in node.meta:\n        return node.meta[\"grapharg\"].example\n\n    if op == \"call_module\":\n        nn_module = tracer.output_graph.nn_modules[node.target]\n        if not is_lazy_module(nn_module):\n            nn_module = copy.deepcopy(nn_module)\n        else:\n            # In the case of a lazy module, we want to run\n            # the pre-hooks which initialize it\n            nn_module(*args, **kwargs)\n    else:\n        nn_module = None\n\n    try:\n        real_value = run_node(tracer, node, args, kwargs, nn_module)\n        cache[node] = real_value\n    except RuntimeError as e:\n        raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None\n    return real_value\n\n\ndef assert_no_fake_params_or_buffers(gm):\n    from torch._subclasses.fake_tensor import FakeTensorConfig, is_fake\n\n    def stack_or_hint(t):\n        if FakeTensorConfig.debug:\n            import traceback\n\n            return f\"FAKE TENSOR CREATION TRACEBACK: \\n {traceback.format_list(t._debug_trace)}\"\n        else:\n            return \"Enable TORCH_FAKE_TENSOR_DEBUG=1 to get creation stack traces on fake tensors.\"\n\n    for name, buffer in gm.named_buffers():\n        assert not is_fake(buffer), (\n            f\"Unexpected fake buffer {name} {stack_or_hint(buffer)}\"\n        )\n    for name, param in gm.named_parameters():\n        assert not is_fake(param), (\n            f\"Unexpected fake param {name} {stack_or_hint(param)}\"\n        )\n\n\ndef fqn(obj: Any):\n    \"\"\"\n    Returns the fully qualified name of the object.\n    \"\"\"\n    return f\"{obj.__module__}.{obj.__qualname__}\"\n\n\ndef ifdynstaticdefault(count1, count2):\n    if torch._dynamo.config.assume_static_by_default:\n        return count1\n    else:\n        return count2\n\n\ndef import_submodule(mod: types.ModuleType):\n    \"\"\"\n    Ensure all the files in a given submodule are imported\n    \"\"\"\n    for filename in sorted(os.listdir(os.path.dirname(cast(str, mod.__file__)))):\n        if filename.endswith(\".py\") and filename[0] != \"_\":\n            importlib.import_module(f\"{mod.__name__}.{filename[:-3]}\")\n\n\ndef object_has_getattribute(value: Any):\n    return class_has_getattribute(type(value))\n\n\ndef object_setattr_ignore_descriptor(obj, name, value):\n    # https://github.com/python/cpython/blob/3.11/Objects/object.c#L1286-L1335\n    d = object.__getattribute__(obj, \"__dict__\")\n    d[name] = value\n\n\ndef class_has_getattribute(cls: type):\n    try:\n        if isinstance(\n            inspect.getattr_static(cls, \"__getattribute__\"),\n            types.FunctionType,\n        ):\n            return True\n    except AttributeError:\n        pass\n    return False\n\n\ndef get_custom_getattr(value: Any, ignore_nn_module_getattr: bool = False):\n    try:\n        getattr_fn = inspect.getattr_static(type(value), \"__getattr__\")\n    except AttributeError:\n        getattr_fn = None\n    if ignore_nn_module_getattr and getattr_fn is torch.nn.Module.__getattr__:\n        # ignore this case of getattr\n        getattr_fn = None\n    return getattr_fn\n\n\nclass TensorStaticReason(enum.Enum):\n    PARAMETER = 2\n    NOT_TENSOR = 4\n    NN_MODULE_PROPERTY = 5\n\n\ndef tensor_static_reason_to_message(reason: TensorStaticReason):\n    if reason == TensorStaticReason.PARAMETER:\n        return \"mark_dynamic on parameter, parameters are always static today.\"\n    if reason == TensorStaticReason.NOT_TENSOR:\n        return \"mark_dynamic on a non tensor, how did this happen?\"\n    if reason == TensorStaticReason.NN_MODULE_PROPERTY:\n        return \"tensor is static because it is nn module associated.\"\n    raise AssertionError(f\"Illegal reason {reason}\")\n\n\ndef tensor_always_has_static_shape(\n    tensor: Union[torch.Tensor, Any],\n    is_tensor: bool,\n    tensor_source: Source,\n) -> tuple[bool, Optional[TensorStaticReason]]:\n    \"\"\"\n    Given a tensor, source, and is_tensor flag, determine if a shape should be static.\n\n    Args:\n    tensor - the real tensor to evaluate, parameters force a static shape.\n    is_tensor - internal dynamo check, essentially \"is_tensor\": target_cls is TensorVariable,\n    tensors not in a TensorVariable for whatever reason are forced static.\n\n    Returns a tuple, where the first element is the bool of whether or not this tensor should have a static shape.\n    The second element is a TensorStaticReason, useful for passing to tensor_static_reason_to_message if needed.\n    \"\"\"\n    from .source import is_from_unspecialized_param_buffer_source\n\n    if (\n        tensor_source.guard_source().is_specialized_nn_module()\n        or tensor_source.guard_source().is_unspecialized_builtin_nn_module()\n    ) and config.force_nn_module_property_static_shapes:\n        return True, TensorStaticReason.NN_MODULE_PROPERTY\n\n    if (\n        type(tensor) is torch.nn.Parameter\n        or is_from_unspecialized_param_buffer_source(tensor_source)\n    ) and config.force_parameter_static_shapes:\n        return True, TensorStaticReason.PARAMETER\n    if not is_tensor:\n        return True, TensorStaticReason.NOT_TENSOR\n    return False, None\n\n\ndef lazy_format_graph_tabular(fn_name, gm):\n    def inner():\n        try:\n            from tabulate import tabulate  # TODO: Check that this is installed\n        except ImportError:\n            return (\n                \"Tabulate module missing, please install tabulate to log the graph in tabular format, logging code instead:\\n\"\n                + str(lazy_format_graph_code(fn_name, gm))\n            )\n\n        node_specs = [\n            [n.op, n.name, n.target, n.args, n.kwargs] for n in gm.graph.nodes\n        ]\n        graph_str = tabulate(\n            node_specs, headers=[\"opcode\", \"name\", \"target\", \"args\", \"kwargs\"]\n        )\n        return _format_graph_code(fn_name, gm.forward.__code__.co_filename, graph_str)\n\n    return LazyString(inner)\n\n\ndef format_bytecode(prefix, name, filename, line_no, code):\n    return f\"{prefix} {name} {filename} line {line_no} \\n{dis.Bytecode(code).dis()}\\n\"\n\n\nforward_hook_names = [\"_forward_pre_hooks\", \"_forward_hooks\"]\nbackward_hook_names = [\"_backward_pre_hooks\", \"_backward_hooks\"]\nstate_dict_hook_names = [\n    \"_state_dict_pre_hooks\",\n    \"_state_dict_hooks\",\n    \"_load_state_dict_pre_hooks\",\n    \"_load_state_dict_post_hooks\",\n]\nall_hook_names = forward_hook_names + backward_hook_names + state_dict_hook_names\n\n\ndef nn_module_has_global_hooks():\n    # This is limited to backward hooks for now because NNModuleVariable\n    # supports fwd hooks underneath.\n    return len(torch.nn.modules.module._global_backward_hooks) or len(\n        torch.nn.modules.module._global_backward_pre_hooks\n    )\n\n\ndef nn_module_get_all_hooks(\n    mod,\n    check_forward_hooks=False,\n    check_backward_hooks=False,\n    check_state_dict_hooks=False,\n):\n    \"\"\"\n    Sometimes its useful to differentiate between types of hooks such as forward/backward/pre\n    hooks executed during module.__call__, and state_dict hooks which are executed separately.\n    \"\"\"\n    hook_dicts_to_check = []\n    check_all_hooks = (\n        not check_forward_hooks\n        and not check_backward_hooks\n        and not check_state_dict_hooks\n    )\n    if check_forward_hooks or check_all_hooks:\n        hook_dicts_to_check.extend(forward_hook_names)\n    if check_backward_hooks or check_all_hooks:\n        hook_dicts_to_check.extend(backward_hook_names)\n    if check_state_dict_hooks:\n        hook_dicts_to_check.extend(state_dict_hook_names)\n\n    all_hooks = []\n    for hook_dict_name in hook_dicts_to_check:\n        hooks = getattr(mod, hook_dict_name, [])\n        for hook_name in hooks:\n            hook = hooks[hook_name]\n\n            all_hooks.append(hook)\n    return all_hooks\n\n\ndef nnmodule_has_hooks(\n    mod,\n    check_forward_hooks=False,\n    check_backward_hooks=False,\n    check_state_dict_hooks=False,\n):\n    \"\"\"\n    Helper function to check if a module has any hooks attached to it.\n    \"\"\"\n    hooks = nn_module_get_all_hooks(\n        mod,\n        check_forward_hooks=check_forward_hooks,\n        check_backward_hooks=check_backward_hooks,\n        check_state_dict_hooks=check_state_dict_hooks,\n    )\n    return bool(hooks)\n\n\ndef to_numpy_helper(value):\n    \"\"\"Convert tensor and tnp.ndarray to numpy.ndarray.\"\"\"\n    if is_fake(value):\n        return value\n    if isinstance(value, tnp.ndarray):\n        return to_numpy_helper(value.tensor)\n    elif isinstance(value, torch.Tensor):\n        return value.numpy(force=True)\n    elif isinstance(value, (tuple, list)):\n        return type(value)(to_numpy_helper(obj) for obj in value)\n    else:\n        return value\n\n\ndef numpy_to_tensor(value):\n    \"\"\"Convert tnp.ndarray to tensor, leave other types intact. If a list/tuple, loop through it to convert.\"\"\"\n    assert np is not None\n    if isinstance(value, np.ndarray):\n        return torch.as_tensor(value)\n    if isinstance(value, tnp.ndarray):\n        return value.tensor\n    elif isinstance(value, (tuple, list)):\n        return type(value)(numpy_to_tensor(obj) for obj in value)\n    else:\n        return value\n\n\nclass numpy_to_tensor_wrapper:\n    def __init__(self, f):\n        self.f = f\n        self.__name__ = \"wrapped_\" + self.f.__name__\n\n    def __repr__(self) -> str:\n        return f\"<Wrapped function <original {self.f.__name__}>>\"\n\n    def __call__(self, *args, **kwargs):\n        out = self.f(*args, **kwargs)\n        return numpy_to_tensor(out)\n\n\ndef numpy_attr_wrapper(obj, name):\n    if isinstance(obj, tnp.ndarray):\n        out = getattr(obj, name)\n        return numpy_to_tensor(out)\n    elif isinstance(obj, torch.Tensor):\n        out = getattr(tnp.ndarray(obj), name)\n        return numpy_to_tensor(out)\n\n\nclass numpy_method_wrapper:\n    \"\"\"Convert obj from torch.Tensor to tnp.ndarray and call method. Then convert result back to torch.Tensor.\"\"\"\n\n    def __init__(self, method: str):\n        self.method = method\n        self.__name__ = \"wrapped_\" + self.method\n\n    def __repr__(self) -> str:\n        return f\"<Wrapped method <original {self.method}>>\"\n\n    def __call__(self, *args, **kwargs):\n        obj = args[0]\n        if isinstance(obj, torch.Tensor):\n            obj = tnp.ndarray(obj)\n        method_callable = getattr(obj, self.method)\n        out = method_callable(*args[1:], **kwargs)\n        return numpy_to_tensor(out)\n\n\nclass numpy_operator_wrapper:\n    \"\"\"Implements dunder methods for tnp.ndarray via functions from the operator library\"\"\"\n\n    def __init__(self, op: Callable[..., Any]):\n        self.op = op\n        self.__name__ = f\"wrapped_{op.__name__}\"\n\n    def __repr__(self) -> str:\n        return f\"<Wrapped operator <original {self.__name__}>>\"\n\n    def __call__(self, *args, **kwargs):\n        assert not kwargs\n\n        args = (\n            tnp.ndarray(arg) if isinstance(arg, torch.Tensor) else arg for arg in args\n        )\n        out = self.op(*args)\n        return numpy_to_tensor(out)\n\n\ndef defake(x):\n    if not isinstance(x, FakeTensor):\n        return x\n    size: torch._prims_common.ShapeType\n    stride: torch._prims_common.StrideType\n    if x._has_symbolic_sizes_strides:\n        size = []\n        for s in x.size():\n            if isinstance(s, torch.SymInt):\n                size.append(s.node.shape_env.size_hint(s.node.expr))\n            else:\n                size.append(s)\n        stride = []\n        for s in x.stride():\n            if isinstance(s, torch.SymInt):\n                stride.append(s.node.shape_env.size_hint(s.node.expr))\n            else:\n                stride.append(s)\n    else:\n        size = x.size()\n        stride = x.stride()\n    y = torch.empty_strided(\n        size,\n        stride,\n        dtype=x.dtype,\n        device=x.device,\n        requires_grad=x.requires_grad,\n    )\n    y.zero_()\n    return y\n\n\ndef _disable_side_effect_safety_checks_for_current_subtracer(fn, *args, **kwargs):\n    return fn(*args, **kwargs)\n\n\ndef is_utils_checkpoint(obj):\n    # Lazy import to avoid circular dependencies\n    import torch.utils.checkpoint\n\n    return obj is torch.utils.checkpoint.checkpoint\n\n\ndef is_invoke_subgraph(obj):\n    from torch._higher_order_ops.invoke_subgraph import invoke_subgraph_placeholder\n\n    return obj is invoke_subgraph_placeholder\n\n\ndef build_invoke_subgraph_variable(**options):\n    from .variables.higher_order_ops import TorchHigherOrderOperatorVariable\n\n    return TorchHigherOrderOperatorVariable.make(\n        torch._higher_order_ops.invoke_subgraph,\n        **options,\n    )\n\n\ndef build_checkpoint_variable(**options):\n    import torch._higher_order_ops.wrap as higher_order_ops\n\n    from .variables.higher_order_ops import TorchHigherOrderOperatorVariable\n\n    # TODO - This is a temporary situation where we have two versions of\n    # checkpointing implementation. We will converge on one and remove the other.\n    activation_checkpoint_op: torch._ops.HigherOrderOperator = (\n        higher_order_ops.tag_activation_checkpoint\n    )\n    if torch._functorch.config.functionalize_rng_ops:\n        activation_checkpoint_op = higher_order_ops.wrap_activation_checkpoint\n\n    return TorchHigherOrderOperatorVariable.make(\n        activation_checkpoint_op,\n        **options,\n    )\n\n\ndef is_compile_supported(device_type):\n    from .eval_frame import is_dynamo_supported\n\n    type = torch.device(device_type).type\n    compile_supported = is_dynamo_supported()\n    if type == \"cpu\":\n        pass\n    elif type in [\"cuda\", \"xpu\"] and compile_supported:\n        compile_supported = has_triton()\n    else:\n        compile_supported = False\n    return compile_supported\n\n\n# The following 3.11 source code functions are adapted from\n# https://github.com/python/cpython/blob/v3.11.4/Lib/traceback.py\n# in order to output source code corresponding to bytecode in 3.11+.\n# We need our own versions since we want to support multiline expressions.\ndef _fix_offset(str: str, offset: int) -> int:\n    \"\"\"\n    Convert byte offset `offset` of `str` into character offset.\n    Byte offset is used for 3.11+ instruction column data.\n    Takes things like unicode characters into consideration.\n\n    Unchanged from CPython implementation.\n    \"\"\"\n    as_utf8 = str.encode(\"utf-8\")\n    return len(as_utf8[:offset].decode(\"utf-8\", errors=\"replace\"))\n\n\n@dataclasses.dataclass\nclass _Anchors:\n    # inclusive\n    left_end_lineno: int\n    left_end_offset: int\n    right_start_lineno: int\n    # exclusive\n    right_start_offset: int\n\n\ndef _extract_anchors_from_expr(segment: str) -> Optional[_Anchors]:\n    \"\"\"\n    Given source code `segment` corresponding to a bytecode\n    instruction, determine:\n        - for binary ops, the location of the binary op\n        - for indexing, the location of the brackets.\n    `segment` is expected to be a valid Python expression\n    \"\"\"\n    assert sys.version_info >= (3, 11)\n\n    import ast\n\n    try:\n        # Without brackets, `segment` is parsed as a statement.\n        # We expect an expression, so wrap `segment` in\n        # brackets to handle multi-line expressions.\n        tree = ast.parse(\"(\\n\" + segment + \"\\n)\")\n    except SyntaxError:\n        return None\n\n    if len(tree.body) != 1:\n        return None\n\n    lines = segment.split(\"\\n\")\n\n    # get character index given byte offset\n    def normalize(lineno, offset):\n        return _fix_offset(lines[lineno], offset)\n\n    # Gets the next valid character index in `lines`, if\n    # the current location is not valid. Handles empty lines.\n    def next_valid_char(lineno, col):\n        while lineno < len(lines) and col >= len(lines[lineno]):\n            col = 0\n            lineno += 1\n        assert lineno < len(lines) and col < len(lines[lineno])\n        return lineno, col\n\n    # Get the next valid character index in `lines`.\n    def increment(lineno, col):\n        col += 1\n        lineno, col = next_valid_char(lineno, col)\n        assert lineno < len(lines) and col < len(lines[lineno])\n        return lineno, col\n\n    # Get the next valid character at least on the next line\n    def nextline(lineno, col):\n        col = 0\n        lineno += 1\n        lineno, col = next_valid_char(lineno, col)\n        assert lineno < len(lines) and col < len(lines[lineno])\n        return lineno, col\n\n    statement = tree.body[0]\n    if isinstance(statement, ast.Expr):\n        expr = statement.value\n        if isinstance(expr, ast.BinOp):\n            # ast gives locations for BinOp subexpressions, e.g.\n            # ( left_expr ) + ( right_expr )\n            #   left^^^^^       right^^^^^\n            # -2 since end_lineno is 1-indexed and because we added an extra\n            # bracket to `segment` when calling ast.parse\n            cur_lineno = cast(int, expr.left.end_lineno) - 2\n            cur_col = normalize(cur_lineno, expr.left.end_col_offset)\n            cur_lineno, cur_col = next_valid_char(cur_lineno, cur_col)\n\n            # Heuristic to find the operator character.\n            # The original CPython implementation did not look for ), \\, or #,\n            # leading to incorrect anchor location, e.g.\n            # (x) + (y)\n            # ~~^~~~~~~\n            while (ch := lines[cur_lineno][cur_col]).isspace() or ch in \")\\\\#\":\n                if ch in \"\\\\#\":\n                    cur_lineno, cur_col = nextline(cur_lineno, cur_col)\n                else:\n                    cur_lineno, cur_col = increment(cur_lineno, cur_col)\n\n            # binary op is 1 or 2 characters long, on the same line\n            right_col = cur_col + 1\n            if (\n                right_col < len(lines[cur_lineno])\n                and not (ch := lines[cur_lineno][right_col]).isspace()\n                and ch not in \"\\\\#\"\n            ):\n                right_col += 1\n            # right_col can be invalid since it is exclusive\n\n            return _Anchors(cur_lineno, cur_col, cur_lineno, right_col)\n        elif isinstance(expr, ast.Subscript):\n            # ast gives locations for value and slice subexpressions, e.g.\n            # ( value_expr ) [ slice_expr ]\n            #   value^^^^^     slice^^^^^\n            # subscript^^^^^^^^^^^^^^^^^^^^\n            # find left bracket (first '[' after value)\n            left_lineno = cast(int, expr.value.end_lineno) - 2\n            left_col = normalize(left_lineno, expr.value.end_col_offset)\n            left_lineno, left_col = next_valid_char(left_lineno, left_col)\n            while lines[left_lineno][left_col] != \"[\":\n                left_lineno, left_col = increment(left_lineno, left_col)\n            # find right bracket (final character of expression)\n            right_lineno = cast(int, expr.end_lineno) - 2\n            right_col = normalize(right_lineno, expr.end_col_offset)\n            return _Anchors(left_lineno, left_col, right_lineno, right_col)\n        elif isinstance(expr, ast.Call):\n            # ( func_expr ) (args, kwargs)\n            #   func^^^^^\n            # call^^^^^^^^^^^^^^^^^^^^^^^^\n            # find left bracket (first '(' after func)\n            left_lineno = cast(int, expr.func.end_lineno) - 2\n            left_col = normalize(left_lineno, expr.func.end_col_offset)\n            left_lineno, left_col = next_valid_char(left_lineno, left_col)\n            while lines[left_lineno][left_col] != \"(\":\n                left_lineno, left_col = increment(left_lineno, left_col)\n            # find right bracket (final character of expression)\n            right_lineno = cast(int, expr.end_lineno) - 2\n            right_col = normalize(right_lineno, expr.end_col_offset)\n            return _Anchors(left_lineno, left_col, right_lineno, right_col)\n\n    return None\n\n\ndef get_instruction_source_311(code: types.CodeType, inst: dis.Instruction) -> str:\n    \"\"\"\n    Python 3.11+ only. Returns lines of source code (from code object `code`)\n    corresponding to `inst`'s location data, and underlines relevant code to `inst`.\n\n    Example: CALL on `g`:\n    f(g(\n      ^^\n        h(x)))\n        ^^^^^\n\n    We need our own implementation in < 3.13 since `format_frame_summary` in\n    Python's `traceback` module doesn't handle multi-line expressions\n    (and their anchor extraction code is not completely correct).\n    \"\"\"\n    if sys.version_info >= (3, 13):\n        # multiline traceback implemented in 3.13+\n        frame_summary = traceback.FrameSummary(\n            code.co_filename,\n            inst.positions.lineno,\n            code.co_name,\n            end_lineno=inst.positions.end_lineno,\n            colno=inst.positions.col_offset,\n            end_colno=inst.positions.end_col_offset,\n        )\n        result = traceback.format_list([frame_summary])[0]\n        # remove first line containing filename info\n        result = \"\\n\".join(result.splitlines()[1:])\n        # indent lines with original indentation\n        orig_lines = [\n            linecache.getline(code.co_filename, lineno).rstrip()\n            for lineno in range(inst.positions.lineno, inst.positions.end_lineno + 1)\n        ]\n        orig_lines_dedent = textwrap.dedent(\"\\n\".join(orig_lines)).splitlines()\n        indent_len = len(orig_lines[0]) - len(orig_lines_dedent[0])\n        indent = orig_lines[0][:indent_len]\n        result = textwrap.indent(textwrap.dedent(result), indent)\n        return result\n\n    assert inst.positions is not None\n    if inst.positions.lineno is None:\n        return \"\"\n    # The rstrip + \"\\n\" pattern is used throughout this function to handle\n    # linecache.getline errors. Error lines are treated as empty strings \"\", but we want\n    # to treat them as blank lines \"\\n\".\n    first_line = linecache.getline(code.co_filename, inst.positions.lineno).rstrip()\n    if inst.positions.end_lineno is None:\n        return first_line\n    if inst.positions.col_offset is None or inst.positions.end_col_offset is None:\n        return first_line\n\n    # character index of the start of the instruction\n    start_offset = _fix_offset(first_line, inst.positions.col_offset)\n    # character index of the end of the instruction\n    # compute later since end may be a different line\n    end_offset = None\n    # expression corresponding to the instruction so we can get anchors\n    segment = \"\"\n    # underline markers to be printed - start with `~` marker and replace with `^` later\n    markers = []\n\n    # Compute segment and initial markers\n    if inst.positions.end_lineno == inst.positions.lineno:\n        end_offset = _fix_offset(first_line, inst.positions.end_col_offset)\n        segment = first_line[start_offset:end_offset]\n        markers.append(\" \" * start_offset + \"~\" * (end_offset - start_offset))\n    else:\n        segment = first_line[start_offset:] + \"\\n\"\n        markers.append(\" \" * start_offset + \"~\" * (len(first_line) - start_offset))\n        last_line = linecache.getline(\n            code.co_filename, inst.positions.end_lineno\n        ).rstrip()\n        end_offset = _fix_offset(last_line, inst.positions.end_col_offset)\n        for lineno in range(inst.positions.lineno + 1, inst.positions.end_lineno):\n            line = linecache.getline(code.co_filename, lineno).rstrip()\n            segment += line + \"\\n\"\n            # don't underline leading spaces\n            num_spaces = len(line) - len(line.lstrip())\n            markers.append(\" \" * num_spaces + \"~\" * (len(line) - num_spaces))\n        segment += last_line[:end_offset]\n        num_spaces = len(last_line) - len(last_line.lstrip())\n        markers.append(\" \" * num_spaces + \"~\" * (end_offset - num_spaces))\n\n    anchors: Optional[_Anchors] = None\n    try:\n        anchors = _extract_anchors_from_expr(segment)\n    except AssertionError:\n        pass\n\n    # replace `~` markers with `^` where necessary\n    if anchors is None:\n        markers = [marker.replace(\"~\", \"^\") for marker in markers]\n    else:\n        # make markers mutable\n        mutable_markers: list[list[str]] = [list(marker) for marker in markers]\n\n        # anchor positions do not take start_offset into account\n        if anchors.left_end_lineno == 0:\n            anchors.left_end_offset += start_offset\n        if anchors.right_start_lineno == 0:\n            anchors.right_start_offset += start_offset\n\n        # Turn `~`` markers between anchors to `^`\n        for lineno in range(len(markers)):\n            for col in range(len(mutable_markers[lineno])):\n                if lineno < anchors.left_end_lineno:\n                    continue\n                if lineno == anchors.left_end_lineno and col < anchors.left_end_offset:\n                    continue\n                if (\n                    lineno == anchors.right_start_lineno\n                    and col >= anchors.right_start_offset\n                ):\n                    continue\n                if lineno > anchors.right_start_lineno:\n                    continue\n                if mutable_markers[lineno][col] == \"~\":\n                    mutable_markers[lineno][col] = \"^\"\n\n        # make markers into strings again\n        markers = [\"\".join(marker) for marker in mutable_markers]\n\n    result = \"\"\n    for i in range(len(markers)):\n        result += (\n            linecache.getline(code.co_filename, inst.positions.lineno + i).rstrip()\n            + \"\\n\"\n        )\n        result += markers[i] + \"\\n\"\n    return result\n\n\ndef get_static_address_type(t):\n    if isinstance(t, torch.Tensor):\n        return getattr(t, \"_dynamo_static_input_type\", None)\n\n    return None\n\n\ndef is_rng_state_getter_or_setter(value):\n    getters = (\n        # The following two functions are not identical, so don't remove anyone!\n        torch._C.Generator.get_state,\n        torch.default_generator.get_state,\n        torch.get_rng_state,\n        torch.cuda.get_rng_state,\n    )\n    setters = (\n        torch._C.Generator.set_state,\n        torch.default_generator.set_state,\n        torch.set_rng_state,\n        torch.cuda.set_rng_state,\n    )\n    return value in (*setters, *getters)\n\n\ndef is_tensor_base_attr_getter(value):\n    return (\n        isinstance(value, types.MethodWrapperType)\n        and value.__name__ == \"__get__\"\n        and value.__self__.__objclass__ is torch._C._TensorBase  # type: ignore[attr-defined]\n    )\n\n\ndef is_tensor_getset_descriptor(name):\n    try:\n        attr = inspect.getattr_static(torch.Tensor, name)\n        return type(attr) is types.GetSetDescriptorType\n    except AttributeError:\n        return False\n\n\ndef is_torch_function_object(value):\n    return hasattr(value, \"__torch_function__\")\n\n\ndef has_torch_function(vt: torch._dynamo.variables.base.VariableTracker) -> bool:\n    # This emulates\n    # https://github.com/pytorch/pytorch/blob/8d81806211bc3c0ee6c2ef235017bacf1d775a85/torch/csrc/utils/disable_torch_function.cpp#L315-L323\n    from torch._dynamo.variables import UserDefinedObjectVariable\n    from torch._dynamo.variables.torch_function import TensorWithTFOverrideVariable\n\n    # Note on lazy vars: The value will either be realized or not throughout the course of execution\n    # if the value has a torch function, it will eventually be realized so we can realize it here\n    # if the value does not have a torch function, it may or may not be realized\n    # if it is realized it will be used and guards will be installed properly\n    # if it is not used, guards won't be installed, and it doesn't matter\n    # if the value has a torch function or not, so we should *not* realize it.\n    # NB: We technically know that if is_realized is False, LazyVariableTracker has the peek_value method\n    # but mypy does not unfortunately\n    if vt.is_realized() or (\n        hasattr(vt, \"peek_value\") and hasattr(vt.peek_value(), \"__torch_function__\")\n    ):\n        func = None\n        if isinstance(vt, TensorWithTFOverrideVariable):\n            func = getattr(vt.class_type, \"__torch_function__\", None)\n\n        elif isinstance(vt, UserDefinedObjectVariable):\n            func = getattr(vt.value, \"__torch_function__\", None)\n\n        return func not in (None, torch._C._disabled_torch_function_impl)\n\n    return False\n\n\n# see note [Tensor Fakification and Symbol Caching]\ndef to_fake_tensor(t, fake_mode):\n    symbolic_context = None\n    source = None\n    if tracing_context := torch._guards.TracingContext.try_get():\n        if t in tracing_context.tensor_to_context:\n            symbolic_context = tracing_context.tensor_to_context[t]\n            source = symbolic_context.tensor_source\n\n    return fake_mode.from_tensor(\n        t, static_shapes=False, symbolic_context=symbolic_context, source=source\n    )\n\n\n# NB: this works for both classes and instances\ndef is_frozen_dataclass(value):\n    return (\n        not object_has_getattribute(value)\n        and not class_has_getattribute(value)\n        and is_dataclass(value)\n        and hasattr(value, \"__dataclass_params__\")\n        and hasattr(value.__dataclass_params__, \"frozen\")\n        and value.__dataclass_params__.frozen\n    )\n\n\ndef get_first_attr(obj, *attrs):\n    \"\"\"\n    Return the first available attribute or throw an exception if none is present.\n    \"\"\"\n    for attr in attrs:\n        if hasattr(obj, attr):\n            return getattr(obj, attr)\n\n    raise AssertionError(f\"{obj} does not has any of the attributes: {attrs}\")\n\n\n@contextlib.contextmanager\ndef maybe_enable_compiled_autograd(should_enable, fullgraph=True, dynamic=True):\n    if not should_enable:\n        yield\n    else:\n\n        def compiler_fn(gm):\n            def inner_compiler(gm_, example_inputs_):\n                torch._dynamo.utils.counters[\"compiled_autograd\"][\"compiles\"] += 1\n                return torch._inductor.compile(gm_, example_inputs_)\n\n            return torch.compile(\n                gm, backend=inner_compiler, fullgraph=fullgraph, dynamic=dynamic\n            )\n\n        with torch._dynamo.compiled_autograd._enable(compiler_fn) as ctx:\n            yield ctx\n\n\ndef invalid_removeable_handle():\n    # need a subclass so weakref works\n    class Invalid(dict):  # type: ignore[type-arg]\n        pass\n\n    return RemovableHandle(Invalid())\n\n\n# Returns a \"proxy\" (new object with the same class and dict) for (non-GraphModule) nn.Module's.\n# Attribute changes to the original object/proxy will be reflected in the other.\n# This is useful for cases where we want a keep-alive reference to a module without increasing\n# its reference count.\ndef nn_module_proxy(mod):\n    if not isinstance(mod, torch.nn.Module):\n        return mod\n    if isinstance(mod, torch.fx.GraphModule):\n        # Dynamo-generated GM's shouldn't contain user-created GM's\n        return mod\n    proxy = mod.__class__.__new__(mod.__class__)\n    proxy.__dict__ = mod.__dict__\n    return proxy\n\n\nclass GmWrapper(torch.nn.Module):\n    def __init__(self, gm, unflatten_fn):\n        super().__init__()\n        self.gm = gm\n        self.unflatten_fn = unflatten_fn\n\n    def forward(self, *args):\n        args: list[Any] = list(args)\n        return self.gm(*self.unflatten_fn(args))\n\n\ndef flatten_graph_inputs(gm: torch.fx.GraphModule, inputs, compile_gm):\n    \"\"\"\n    Mutate inputs so that they are flat and wrap gm such that it\n    accepts those inputs.  This is needed for graphs that take\n    bumpy inputs.\n    \"\"\"\n    inputs_idx_to_clear = [\n        i\n        for i, node in enumerate(gm.graph.nodes)\n        if node.op == \"placeholder\" and node.meta.get(\"steal_arg\", False)\n    ]\n\n    if torch._dynamo.compiled_autograd.in_compiled_autograd_region:\n        # fast path, avoid pytree overhead\n        # compiled autograd inputs are always a list of tensors, maybe followed by symints\n        assert inputs_idx_to_clear == [0]\n        assert isinstance(inputs[0], list)\n        boxed_inputs_count = len(inputs[0])\n\n        def flatten_fn(args):\n            return args[0] + list(args[1:])\n\n        def unflatten_fn(flat_args):\n            return (flat_args[:boxed_inputs_count], *flat_args[boxed_inputs_count:])\n\n        compiled_fn = compile_gm(GmWrapper(gm, unflatten_fn), flatten_fn(inputs))\n    else:\n        # slow path, don't know inputs structure\n        flat_inputs, spec = pytree.tree_flatten(inputs)\n        unflatten_fn = functools.partial(pytree.tree_unflatten, treespec=spec)\n        compiled_fn = compile_gm(GmWrapper(gm, unflatten_fn), flat_inputs)\n        # note this doesn't check the spec, assuming it is the same\n        flatten_fn = pytree.arg_tree_leaves\n\n    def wrapper(*args):\n        flat_args = flatten_fn(args)\n\n        # flat_args is a new list, so we need to clear references from the old list\n        for i in inputs_idx_to_clear:\n            args[i].clear()\n\n        # this call is boxed to avoid increasing refcount until we reach aot_module_simplified forward\n        return compiled_fn(flat_args)\n\n    return wrapper\n\n\ndef get_locals_to_steal(maybe_gm):\n    if not isinstance(maybe_gm, torch.fx.GraphModule) or not hasattr(maybe_gm, \"meta\"):\n        return []\n    return maybe_gm.meta.get(\"locals_to_steal\", [])\n\n\ndef set_locals_to_steal(gm, locals_to_steal):\n    gm.meta[\"locals_to_steal\"] = locals_to_steal\n\n\nclass Lit:\n    def __init__(self, s):\n        self.s = s\n\n    def __repr__(self) -> str:\n        return self.s\n\n\nwarn_once_cache: set[str] = set()\n\n\ndef warn_once(msg, stacklevel=1):\n    # Dynamo causes all warnings.warn (in user code and in Dynamo code) to print all the time.\n    # https://github.com/pytorch/pytorch/issues/128427.\n    # warn_once is a workaround: if the msg has been warned on before, then we will not\n    # warn again.\n    # NB: it's totally ok to store a cache of all the strings: this is what warnings.warn does as well.\n    if msg in warn_once_cache:\n        return\n    warn_once_cache.add(msg)\n    warnings.warn(msg, stacklevel=stacklevel + 1)\n\n\ndef strip_color_from_string(text):\n    # This regular expression matches ANSI escape codes\n    ansi_escape = re.compile(r\"\\x1B[@-_][0-?]*[ -/]*[@-~]\")\n    return ansi_escape.sub(\"\", text)\n\n\n@contextlib.contextmanager\ndef _disable_saved_tensors_hooks_during_tracing():\n    # See NOTE: [Deferring tensor pack/unpack hooks until runtime]\n    try:\n        prior = torch._C._autograd._saved_tensors_hooks_set_tracing(True)\n        yield\n    finally:\n        torch._C._autograd._saved_tensors_hooks_set_tracing(prior)\n\n\ndef is_parameter_freezing():\n    return torch._inductor.config.freezing and not torch.is_grad_enabled()\n\n\ndef get_torch_function_mode_stack():\n    return [\n        get_torch_function_mode_stack_at(i) for i in range(_len_torch_function_stack())\n    ]\n\n\ndef get_torch_function_mode_stack_at(ind):\n    assert ind < _len_torch_function_stack() and ind >= 0\n    return torch._C._get_function_stack_at(ind)\n\n\ndef set_torch_function_mode_stack(stack):\n    for _ in range(_len_torch_function_stack()):\n        _pop_torch_function_stack()\n\n    for mode in stack:\n        _push_on_torch_function_stack(mode)\n\n\ndef clear_torch_function_mode_stack():\n    for _ in range(_len_torch_function_stack()):\n        _pop_torch_function_stack()\n\n\n# call from C dynamo in order to inspect values in pdb\ndef _breakpoint_for_c_dynamo(*args):\n    breakpoint()\n\n\ndef verify_guard_fn_signature(value):\n    fn = value.__metadata_guard__\n    sig = inspect.signature(fn)\n    if len(sig.parameters) != 2:\n        from .exc import InternalTorchDynamoError\n\n        raise InternalTorchDynamoError(\n            \"Tensor subclass method __metadata_guard__ must take exactly two subclass metadata arguments\"\n        )\n    if fn.__self__ != value.__class__:\n        from .exc import InternalTorchDynamoError\n\n        raise InternalTorchDynamoError(\n            \"Tensor subclass method __metadata_guard__ must be a classmethod\"\n        )\n\n\ndef does_not_override_dict_iter_methods(user_cls):\n    return (\n        user_cls.items in (dict.items, OrderedDict.items)\n        and user_cls.values in (dict.values, OrderedDict.values)\n        and user_cls.keys in (dict.keys, OrderedDict.keys)\n        and user_cls.__iter__ in (dict.__iter__, OrderedDict.__iter__)\n    )\n\n\n# Helper functions below are to prevent TorchDynamo to prevent tracing of\n# __torch_function__ calls triggered on tensor properties in the pre graph\n# bytecode.\n@torch._disable_dynamo\ndef call_size(x, i):\n    return x.size(i)\n\n\n@torch._disable_dynamo\ndef call_stride(x, i):\n    return x.stride(i)\n\n\n@torch._disable_dynamo\ndef call_storage_offset(x):\n    return x.storage_offset()\n\n\n# Helper function to extract relevant parts of a tensor's __dict__ to store in node meta.\n# To avoid ref cycles, it's important that no tensors are present here, so leave those out.\ndef _extract_tensor_dict(t):\n    KEYS_TO_COPY = [\n        \"_dynamo_static_input_type\",\n        \"tag\",\n    ]\n\n    tensor_dict = {\n        key: copy.copy(t.__dict__[key]) for key in KEYS_TO_COPY if key in t.__dict__\n    }\n\n    return tensor_dict\n\n\n# This is useful for reconstructing within the Dynamo graph the non-graph-input objects\n# whose lifetime is governed by the user.\n# e.g. torch.cuda.Event is a prime example.\nuser_obj_id_to_weakref: dict[int, weakref.ReferenceType[object]] = {}\n\n\ndef get_user_object_from_id(obj_id):\n    obj = user_obj_id_to_weakref[obj_id]()\n    assert obj is not None, \"User object is no longer alive\"\n    return obj\n\n\ndef store_user_object_weakref(obj):\n    obj_id = id(obj)\n    user_obj_id_to_weakref[obj_id] = weakref.ref(obj)\n\n\nclass CompileTimeInstructionCounter:\n    _counter: int = 0\n    _id: int = -1\n    _depth = 0\n\n    @classmethod\n    def start(cls) -> None:\n        cls._depth = cls._depth + 1\n        if cls._depth == 1:\n            cls._id = _instruction_counter.start()\n\n    @classmethod\n    def end(cls) -> None:\n        cls._depth = cls._depth - 1\n        if cls._depth == 0:\n            cls._counter += _instruction_counter.end(cls._id)\n            cls._id = -1\n\n    @classmethod\n    def clear(cls) -> None:\n        cls._counter = 0\n\n    @classmethod\n    def value(cls) -> int:\n        return cls._counter\n\n    @classmethod\n    @contextmanager\n    def record(cls):\n        try:\n            if config.record_compile_time_instruction_count:\n                cls.start()\n            yield\n        finally:\n            if config.record_compile_time_instruction_count:\n                cls.end()\n\n\ndef set_feature_use(feature: str, usage: bool):\n    \"\"\"\n    Records whether we are using a feature\n    Generally a feature is a JK.\n    \"\"\"\n    # Note that sometimes (tests etc...) we're not in a context which we can record into\n    if get_metrics_context().in_progress():\n        get_metrics_context().set_key_value(\"feature_usage\", feature, usage)\n\n\n_ddp_optimization_mode: tuple[str, ...] = (\n    \"ddp_optimizer\",\n    \"python_reducer\",  # experimental mode\n    \"python_reducer_without_compiled_forward\",\n    \"no_optimization\",\n)\n\n\ndef get_optimize_ddp_mode():\n    optimize_ddp = config.optimize_ddp\n    if isinstance(optimize_ddp, bool):\n        mode = \"ddp_optimizer\" if optimize_ddp else \"no_optimization\"\n    elif isinstance(optimize_ddp, str):\n        mode = optimize_ddp\n    else:\n        raise ValueError(\n            f\"Invalid dynamo config optimize_ddp type {type(optimize_ddp)=}\"\n        )\n\n    assert mode in _ddp_optimization_mode, (\n        f\"Invalid dynamo config optimize_ddp value {mode=}\"\n    )\n    return mode\n\n\n@contextmanager\ndef maybe_disable_inference_mode() -> Generator[None, None, None]:\n    \"\"\"\n    Disables torch.inference_mode for the compilation (still on at runtime).\n    This simplifies the compile stack where we can assume that inference_mode\n    will always be off.\n\n    Since inference_mode is equivalent to no_grad + some optimizations (version\n    counts etc), we turn on no_grad here. The other optimizations are not\n    relevant to torch.compile.\n    \"\"\"\n    is_inference_mode_on = (\n        config.fake_tensor_disable_inference_mode and torch.is_inference_mode_enabled()\n    )\n    if is_inference_mode_on:\n        with (\n            torch.inference_mode(False),\n            torch.no_grad(),\n        ):\n            yield\n    else:\n        yield\n\n\n@contextmanager\ndef maybe_disable_inference_mode_for_fake_prop() -> Generator[None, None, None]:\n    \"\"\"\n    Turns off tracking of inference_mode for fake tensor propagation. With this\n    context manager, when a real tensor is converted to fake tensor, the fake\n    tensor looses its inference-ness.\n    \"\"\"\n    if config.fake_tensor_disable_inference_mode:\n        with torch._subclasses.meta_utils.disable_inference_mode_for_fake_prop():\n            yield\n    else:\n        yield\n\n\ndef is_node_meta_valid(node: Optional[torch.fx.Node]) -> bool:\n    return node is None or \"example_value\" in node.meta or \"val\" in node.meta\n\n\n# If True, enforce fullgraph=True - raise errors on graph break\n_error_on_graph_break = False\n\n\ndef _get_error_on_graph_break() -> bool:\n    return _error_on_graph_break\n\n\ndef _set_error_on_graph_break(value: bool) -> None:\n    global _error_on_graph_break\n    _error_on_graph_break = value\n\n\n@torch._disable_dynamo\ndef record_pregraph_bytecode_enter() -> AbstractContextManager[None]:\n    cm: AbstractContextManager[None] = (\n        torch._C._profiler._RecordFunctionFast(\"Pregraph bytecode\")\n        if torch.autograd.profiler._is_profiler_enabled\n        else contextlib.nullcontext()\n    )\n    cm.__enter__()\n    return cm\n\n\n@torch._disable_dynamo\ndef record_pregraph_bytecode_exit(cm: AbstractContextManager[None]) -> None:\n    cm.__exit__(None, None, None)\n\n\n# Returns a set of code objects present traced in the current TracingContext, or None\n# if there is no current TracingContext.\ndef get_traced_code() -> list[CodeType]:\n    from torch._guards import TracingContext\n\n    return TracingContext.get_traced_code()\n", 4754], "/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py": ["# mypy: allow-untyped-defs\nimport contextlib\nimport functools\nfrom typing import Callable\nfrom typing_extensions import deprecated\n\nimport torch\nfrom torch._library.utils import Kernel, RegistrationHandle\n\n\nclass FakeImplHolder:\n    \"\"\"A holder where one can register an fake impl to.\"\"\"\n\n    def __init__(self, qualname: str):\n        self.qualname: str = qualname\n        # kernels stores all registered fake kernels, ordered by registration\n        # time ascendingly (newer registration after older registration). If an\n        # operator library gets loaded that overrides an existing fake kernel,\n        # both kernels will be in the list, but the newest one will be the one\n        # that is run. If the library is unloaded, we will remove the kernel\n        # from this list.\n        self.kernels: list[Kernel] = []\n\n    @property\n    def kernel(self):\n        if len(self.kernels) == 0:\n            return None\n        return self.kernels[-1]\n\n    @kernel.setter\n    def kernel(self, value):\n        raise RuntimeError(\"Unable to directly set kernel.\")\n\n    def register(\n        self, func: Callable, source: str, lib, *, allow_override=False\n    ) -> RegistrationHandle:\n        \"\"\"Register an fake impl.\n\n        Returns a RegistrationHandle that one can use to de-register this\n        fake impl.\n        \"\"\"\n\n        if not allow_override:\n            if self.kernel is not None:\n                raise RuntimeError(\n                    f\"register_fake(...): the operator {self.qualname} \"\n                    f\"already has an fake impl registered at \"\n                    f\"{self.kernel.source}.\"\n                )\n            if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, \"Meta\"):\n                raise RuntimeError(\n                    f\"register_fake(...): the operator {self.qualname} \"\n                    f\"already has an DispatchKey::Meta implementation via a \"\n                    f\"pre-existing torch.library or TORCH_LIBRARY registration. \"\n                    f\"Please either remove that registration or don't call \"\n                    f\"register_fake.\"\n                )\n\n            if torch._C._dispatch_has_kernel_for_dispatch_key(\n                self.qualname, \"CompositeImplicitAutograd\"\n            ):\n                raise RuntimeError(\n                    f\"register_fake(...): the operator {self.qualname} \"\n                    f\"already has an implementation for this device type via a \"\n                    f\"pre-existing registration to \"\n                    f\"DispatchKey::CompositeImplicitAutograd.\"\n                    f\"CompositeImplicitAutograd operators do not need an fake \"\n                    f\"impl; \"\n                    f\"instead, the operator will decompose into its constituents \"\n                    f\"and those \"\n                    f\"can have fake impls defined on them.\"\n                )\n\n        # Store the kernel in this holder\n        kernel = Kernel(func, source)\n        self.kernels.append(kernel)\n\n        def deregister_fake_kernel():\n            self.kernels.remove(kernel)\n\n        meta_kernel = construct_meta_kernel(self.qualname, self)\n        lib.impl(self.qualname, meta_kernel, \"Meta\", allow_override=allow_override)\n\n        handle = RegistrationHandle(deregister_fake_kernel)\n        return handle\n\n\ndef construct_meta_kernel(qualname: str, fake_impl_holder: FakeImplHolder) -> Callable:\n    assert fake_impl_holder.kernel is not None\n\n    @functools.wraps(fake_impl_holder.kernel.func)\n    def meta_kernel(*args, **kwargs):\n        assert fake_impl_holder.kernel is not None\n        source = fake_impl_holder.kernel.source\n\n        def error_on_ctx():\n            raise RuntimeError(\n                f\"{qualname} ({source}): You're trying to run this operator \"\n                f\"with meta Tensors (as opposed to FakeTensors), but this \"\n                f\"operator may return an output Tensor with data-dependent shape. Meta \"\n                f\"Tensors don't support operators with outputs that have data-dependent shapes \"\n                f\"but FakeTensors do. \"\n                f\"If your operator does not return an output with data-dependent shape, \"\n                f\"make sure the FakeTensor and/or meta kernel does not call \"\n                f\"torch.library.get_ctx(). Otherwise, please use FakeTensors.\"\n            )\n\n        with set_ctx_getter(error_on_ctx):\n            return fake_impl_holder.kernel(*args, **kwargs)\n\n    return meta_kernel\n\n\ndef get_none():\n    return None\n\n\nglobal_ctx_getter: Callable = get_none\n\n\n@contextlib.contextmanager\ndef set_ctx_getter(ctx_getter):\n    global global_ctx_getter\n    prev = global_ctx_getter\n    try:\n        global_ctx_getter = ctx_getter\n        yield\n    finally:\n        global_ctx_getter = prev\n\n\nclass FakeImplCtx:\n    \"\"\"\n    Context object for writing fake implementations for custom operators.\n    \"\"\"\n\n    def __init__(self, _fake_mode, _op):\n        self._fake_mode = _fake_mode\n        self._shape_env = _fake_mode.shape_env\n        self._op = _op\n\n    @deprecated(\n        \"`create_unbacked_symint` is deprecated, please use `new_dynamic_size` instead\",\n        category=FutureWarning,\n    )\n    def create_unbacked_symint(self, *, min=2, max=None) -> torch.SymInt:\n        return self.new_dynamic_size(min=min, max=max)\n\n    def new_dynamic_size(self, *, min=0, max=None) -> torch.SymInt:\n        \"\"\"Constructs a new symint (symbolic int) representing a data-dependent value.\n\n        This is useful for writing the fake implementation (which is necessary\n        for torch.compile) for a CustomOp where an output Tensor has a size\n        that depends on the data of the input Tensors.\n\n        Args:\n            min (int): A statically known inclusive lower bound for this symint. Default: 0\n            max (Optional[int]): A statically known inclusive upper bound for this\n                symint. Default: None\n\n        .. warning:\n\n            It is important that the ``min`` and ``max`` (if not None) values are set\n            correctly, otherwise, there will be undefined behavior under\n            torch.compile. The default value of ``min`` is 2 due to torch.compile\n            specializing on 0/1 sizes.\n\n            You must also verify that your implementation on concrete Tensors\n            (e.g. CPU/CUDA) only returns Tensors where the size that corresponds\n            to the symint also has respects these constraint.\n            The easiest way to do this is to add an assertion in the CPU/CUDA/etc\n            implementation that the size follows these bounds.\n\n        Example::\n\n            >>> # An operator with data-dependent output shape\n            >>> lib = torch.library.Library(\"mymodule\", \"FRAGMENT\")\n            >>> lib.define(\"mymodule::custom_nonzero(Tensor x) -> Tensor\")\n            >>>\n            >>> @torch.library.register_fake(\"mymodule::custom_nonzero\")\n            >>> def _(x):\n            >>>     # Number of nonzero-elements is data-dependent.\n            >>>     # Since we cannot peek at the data in an fake impl,\n            >>>     # we use the ctx object to construct a new symint that\n            >>>     # represents the data-dependent size.\n            >>>     ctx = torch.library.get_ctx()\n            >>>     nnz = ctx.new_dynamic_size()\n            >>>     shape = [nnz, x.dim()]\n            >>>     result = x.new_empty(shape, dtype=torch.int64)\n            >>>     return result\n            >>>\n            >>> @torch.library.impl(lib, \"custom_nonzero\", \"CPU\")\n            >>> def _(x):\n            >>>     x_np = x.numpy()\n            >>>     res = np.stack(np.nonzero(x_np), axis=1)\n            >>>     return torch.tensor(res, device=x.device)\n\n        \"\"\"\n        if (\n            self._shape_env is None\n            or not self._shape_env.allow_dynamic_output_shape_ops\n        ):\n            raise torch._subclasses.fake_tensor.DynamicOutputShapeException(self._op)\n\n        if isinstance(min, torch.SymInt) or isinstance(max, torch.SymInt):\n            raise ValueError(\n                f\"ctx.new_dynamic_size(min={min}, max={max}): expected \"\n                f\"min and max to be statically known ints but got SymInt. \"\n                f\"This is not supported.\"\n            )\n\n        if min < 0:\n            raise ValueError(\n                f\"ctx.new_dynamic_size(min={min}, ...): expected min to be \"\n                f\"greater than or equal to 0: this API can only create \"\n                f\"non-negative sizes.\"\n            )\n\n        return allocate_size(self._shape_env, min, max)\n\n\ndef allocate_size(shape_env, min_val=0, max_val=None):\n    result = shape_env.create_unbacked_symint()\n    torch.fx.experimental.symbolic_shapes._constrain_range_for_size(\n        result, min=min_val, max=max_val\n    )\n    return result\n", 227], "/data/wangyingqi/code/pytorch/torch/_library/utils.py": ["# mypy: allow-untyped-defs\nimport dataclasses\nimport inspect\nimport sys\nfrom collections.abc import Iterable, Iterator\nfrom typing import Any, Callable, Union\n\nimport torch\nimport torch.utils._pytree as pytree\nfrom torch import _C, _utils_internal\nfrom torch._ops import OpOverload\n\n\n@dataclasses.dataclass\nclass Kernel:\n    \"\"\"Models a (function, source location)\"\"\"\n\n    func: Callable\n    source: str\n\n    def __call__(self, *args, **kwargs):\n        return self.func(*args, **kwargs)\n\n\nclass RegistrationHandle:\n    \"\"\"Does something when someone calls .destroy() on it\"\"\"\n\n    def __init__(self, on_destroy: Callable):\n        self._on_destroy = on_destroy\n\n    def destroy(self) -> None:\n        self._on_destroy()\n\n\ndef get_source(stacklevel: int) -> str:\n    \"\"\"Get a string that represents the caller.\n\n    Example: \"/path/to/foo.py:42\"\n\n    Use stacklevel=1 to get the caller's source\n    Use stacklevel=2 to get the caller's caller's source\n    etc.\n    \"\"\"\n    frame = inspect.getframeinfo(sys._getframe(stacklevel))\n    source = f\"{frame.filename}:{frame.lineno}\"\n    return source\n\n\ndef parse_namespace(qualname: str) -> tuple[str, str]:\n    splits = qualname.split(\"::\")\n    if len(splits) != 2:\n        raise ValueError(\n            f\"Expected `qualname` to be of the form \"\n            f'\"namespace::name\", but got {qualname}. '\n            f\"The qualname passed to the torch.library APIs must consist \"\n            f\"of a namespace and a name, e.g. aten::sin\"\n        )\n    return splits[0], splits[1]\n\n\ndef lookup_op(qualname: str) -> OpOverload:\n    namespace, name = parse_namespace(qualname)\n    if \".\" in name:\n        name, overload = name.split(\".\")\n    else:\n        overload = \"default\"\n    ns = getattr(torch.ops, namespace)\n    packet = getattr(ns, name)\n    return getattr(packet, overload)\n\n\ndef is_builtin(op: OpOverload) -> bool:\n    assert isinstance(op, OpOverload)\n    return op.namespace in {\"aten\", \"prim\", \"prims\"}\n\n\ndef is_functional_schema(schema: Any) -> bool:\n    \"\"\"Check if the schema is functional.\n\n    An operator is functional if:\n    - it does not mutate any of its inputs\n    - it does not return a view on any of its inputs\n    - it has at least one return\n    \"\"\"\n\n    def is_functional(schema):\n        if schema.is_mutable:\n            return False\n        rets = schema.returns\n        is_non_mutating_view = len(rets) > 0 and any(\n            r.alias_info is not None and not r.alias_info.is_write for r in rets\n        )\n        if is_non_mutating_view:\n            return False\n        if not schema.returns:\n            return False\n        return True\n\n    if isinstance(schema, torch._C.FunctionSchema):\n        return is_functional(schema)\n\n    # Lazy import because not all PyTorch builds have torchgen\n    from torchgen.model import FunctionSchema\n\n    if isinstance(schema, str):\n        schema = FunctionSchema.parse(schema)\n    assert isinstance(schema, FunctionSchema)\n    return is_functional(schema)\n\n\n# should be torch._C.JitType but that annotation is busted\ndef is_tensorlist_like_type(typ: Any) -> bool:\n    return (\n        typ == _C.ListType(_C.TensorType.get())\n        or typ == _C.ListType(_C.OptionalType(_C.TensorType.get()))\n        or typ == _C.OptionalType(_C.ListType(_C.TensorType.get()))\n        or typ == _C.OptionalType(_C.ListType(_C.OptionalType(_C.TensorType.get())))\n    )\n\n\n# should be torch._C.JitType but that annotation is busted\ndef is_tensor_like_type(typ: Any) -> bool:\n    return typ == _C.TensorType.get() or typ == _C.OptionalType(_C.TensorType.get())\n\n\ndef mutates_and_returns_first_arg(op: OpOverload):\n    \"\"\"Check if an op is an inplace aten op, i.e. it mutates and returns the first arg.\n\n    TODO: torchgen/model.py's FunctionSchema.parse is the source of truth for this,\n    but not all PyTorch builds have torchgen (due to the yaml dependency being weird).\n    Figure this out.\n\n    Example: add_(Tensor(a!) x, Tensor y) -> Tensor(a)\n    \"\"\"\n    if op.namespace != \"aten\":\n        return False\n    schema = op._schema\n    if not len(schema.returns) == 1:\n        return False\n    if schema.returns[0].alias_info is None:\n        return False\n    alias_set = schema.returns[0].alias_info.after_set\n    if len(alias_set) != 1:\n        return False\n    loc = next(iter(alias_set))\n    if len(schema.arguments) < 1:\n        return False\n    first_arg = schema.arguments[0]\n    if first_arg.alias_info is None:\n        return False\n    if not first_arg.alias_info.is_write:\n        return False\n    alias_set = first_arg.alias_info.after_set\n    if len(alias_set) != 1:\n        return False\n    if loc != next(iter(alias_set)):\n        return False\n    for arg in schema.arguments[1:]:\n        if arg.alias_info is not None:\n            return False\n    return True\n\n\ndef fill_defaults(schema, args, kwargs):\n    new_args = []\n    new_kwargs = {}\n    for i in range(len(schema.arguments)):\n        info = schema.arguments[i]\n        if info.kwarg_only:\n            if info.name in kwargs:\n                new_kwargs[info.name] = kwargs[info.name]\n            else:\n                new_kwargs[info.name] = info.default_value\n        else:\n            if i < len(args):\n                new_args.append(args[i])\n            else:\n                new_args.append(info.default_value)\n    return tuple(new_args), new_kwargs\n\n\ndef zip_schema(\n    schema: _C.FunctionSchema, args: tuple[Any, ...], kwargs: dict[str, Any]\n) -> Iterable[tuple[_C.Argument, Any]]:\n    \"\"\"zips schema.arguments and (args, kwargs) together.\n\n    Assumes that (args, kwargs) were the inputs to some torch._ops.OpOverload:\n    that is, (args, kwargs) must be bindable to the schema (args, kwargs).\n    \"\"\"\n    assert len(schema.arguments) >= len(args) + len(kwargs)\n    for i in range(len(schema.arguments)):\n        info = schema.arguments[i]\n        if info.kwarg_only:\n            if info.name in kwargs:\n                yield info, kwargs[info.name]\n            continue\n        if i >= len(args):\n            if not info.kwarg_only and info.name in kwargs:\n                yield info, kwargs[info.name]\n            # args that are equal to their default values are not populated\n            # if they are followed by args that are equal to their defaults.\n            # Skip these.\n            continue\n        yield info, args[i]\n    return\n\n\ndef hop_schema_from_fx_node(node):\n    from torchgen.gen_schema_utils import FunctionSchemaGen\n\n    hop = node.target\n    if not isinstance(hop, torch._ops.HigherOrderOperator):\n        raise RuntimeError(\"fx_node's target must be a hop.\")\n\n    def _collect_example_val(node):\n        meta_val = node.meta.get(\"val\", None)\n        if meta_val is None:\n            assert node.op == \"get_attr\"\n            meta_val = getattr(node.graph.owning_module, node.target)\n        return meta_val\n\n    example_inputs = []\n    for arg in node.args:\n        if isinstance(arg, (torch.fx.Node, torch.fx.node.Node)):\n            example_inputs.append(_collect_example_val(arg))\n        elif isinstance(\n            arg, (torch.fx.immutable_collections.immutable_list, list, tuple)\n        ):\n            example_inputs.append([_collect_example_val(x) for x in arg])\n        else:\n            raise RuntimeError(f\"Unsupported arg type {type(arg)}\")\n\n    # Bound the arguments to make sure number of inputs are correct\n    bound_args: inspect.BoundArguments = inspect.signature(hop.__call__).bind(\n        *example_inputs\n    )\n\n    # We treat example_output as a single value in return. This is to differentiate 1. return a single val\n    # vs 2. return a tuple with one element.\n    example_output = _collect_example_val(node)\n    return FunctionSchemaGen.from_example(\n        hop._name, tuple(bound_args.arguments.items()), (list(example_output),)\n    )\n\n\ndef can_generate_trivial_fake_impl(op: OpOverload) -> bool:\n    assert isinstance(op, OpOverload)\n    if is_builtin(op):\n        # We control the built-ins. These may (in rare cases)\n        # do input metadata mutation (which we have banned on custom ops)\n        return False\n    schema = op._schema\n    # It's suspicious if the op is not mutable but returns nothing, so we return False out of an abundance of caution\n    if not schema.is_mutable:\n        return False\n    if len(schema.returns) > 0:\n        return False\n    # If the op returns nothing, then it has a trivial fake impl.\n    return True\n\n\ndef requires_set_python_module() -> bool:\n    \"\"\"If an op was defined in C++ and extended from Python using the\n    torch.library APIs, returns if we require that there have been a\n    m.set_python_module(\"mylib.ops\") call from C++ that associates\n    the C++ op with a python module.\n    \"\"\"\n    return getattr(_utils_internal, \"REQUIRES_SET_PYTHON_MODULE\", True)\n\n\ndef handle_dispatch_mode(curr_mode, op_overload, *args, **kwargs):\n    assert isinstance(curr_mode, torch.utils._python_dispatch.TorchDispatchMode)\n    args_flattened, _ = torch.utils._pytree.tree_flatten((args, kwargs.values()))\n    # TODO: need to double check the semantics of the \"types\" argument to torch_dispatch.\n    # It's generated in PyInterpreter.cpp, but seems to be generated in two places,\n    # where in one case we only include tensors with the python key, and in another\n    # we include **all** tensors.\n    overload_types = [\n        type(a)\n        for a in args_flattened\n        if isinstance(a, torch.Tensor)\n        and torch._C._dispatch_keys(a).has(torch._C.DispatchKey.Python)\n    ]\n    # TODO: check that I got these args correct (in C++, we pass in \"0000\"??)\n\n    return curr_mode.__torch_dispatch__(op_overload, overload_types, args, kwargs)\n\n\ndef has_kwarg_only_args(schema: _C.FunctionSchema):\n    return any(a.kwarg_only for a in schema.arguments)\n\n\ndef has_kwarg_only_tensors(schema: _C.FunctionSchema):\n    for a in schema.arguments:\n        if not (is_tensor_like_type(a.type) or is_tensorlist_like_type(a.type)):\n            continue\n        if not a.kwarg_only:\n            continue\n        return True\n    return False\n\n\ndef has_tensor_arg(schema: _C.FunctionSchema) -> bool:\n    \"\"\"\n    Given a schema, returns True if the schema has a Tensor arg.\n    A Tensor arg is any arg with a type annotation that might involve Tensor.\n    \"\"\"\n    return any(\n        (is_tensor_like_type(a.type) or is_tensorlist_like_type(a.type))\n        for a in schema.arguments\n    )\n\n\ndef get_device_arg_index(schema: _C.FunctionSchema) -> Union[int, None]:\n    \"\"\"\n    Given a schema, returns the id of the `device: torch.device` argument.\n    If it does not exist, returns None.\n    \"\"\"\n    for index, arg in enumerate(schema.arguments):\n        if arg.type is _C.DeviceObjType.get() and arg.name == \"device\":\n            return index\n    return None\n\n\ndef iter_tensors(\n    args: tuple[Any], kwargs: dict[str, Any], allowed_nesting: int = 1\n) -> Iterator[torch.Tensor]:\n    def check(arg):\n        if isinstance(arg, torch.Tensor):\n            yield arg\n        elif allowed_nesting > 0 and isinstance(arg, (tuple, list)):\n            yield from iter_tensors(tuple(arg), {}, allowed_nesting - 1)\n\n    for arg in args:\n        yield from check(arg)\n    for kwarg in kwargs.values():\n        yield from check(kwarg)\n\n\ndef check_aliasing_constraint(name, prev, result, get_module=lambda: \"???\"):\n    \"\"\"\n    custom operators' outputs must not alias any inputs or other outputs.\n    \"\"\"\n    storages = {id(t.untyped_storage()) for t in prev if isinstance(t, torch.Tensor)}\n    tuple_result = result\n    if not isinstance(result, tuple):\n        tuple_result = (result,)\n    for tensor in iter_tensors(tuple_result, {}):\n        key = id(tensor.untyped_storage())\n        if id(tensor.untyped_storage()) in storages:\n            raise RuntimeError(\n                f\"{name} (with implementation in {get_module()}): \"\n                f\"The output of this custom operator (1) must not \"\n                f\"also be an input to this custom operator and \"\n                f\"(2) may not alias any inputs to this custom operator \"\n                f\"or other returns. \"\n                f\"The most common way to trigger this error is if \"\n                f\"we have y = custom_op(x) and y and x are the same Tensor. \"\n                f\"Please instead return a clone of the offending output \"\n                f\"tensor(s) (e.g. return x.clone()) or refactor the custom \"\n                f\"operator to not return y.\"\n            )\n        storages.add(key)\n\n\ndef _c_check_aliasing_constraint(name, args, kwargs, result, get_module=lambda: \"???\"):\n    \"\"\"\n    custom operators' outputs must not have any aliases\n    This version uses C++ implementation for perf.\n    Only List container is supported.\n    Tensors in Lists with not only Tensors are checked.\n    \"\"\"\n    tuple_result = result\n    if not isinstance(result, tuple):\n        tuple_result = (result,)\n    if _C._any_output_is_alias_to_input_or_output(args, kwargs, tuple_result):\n        raise RuntimeError(\n            f\"{name} (with implementation in {get_module()}): \"\n            f\"The output of this custom operator (1) must not \"\n            f\"also be an input to this custom operator and \"\n            f\"(2) may not alias any inputs to this custom operator \"\n            f\"or other returns. \"\n            f\"The most common way to trigger this error is if \"\n            f\"we have y = custom_op(x) and y and x are the same Tensor. \"\n            f\"Please instead return a clone of the offending output \"\n            f\"tensor(s) (e.g. return x.clone()) or refactor the custom \"\n            f\"operator to not return y.\"\n        )\n\n\nclass MutationChecker:\n    \"\"\"\n    Check if an operator mutated its arguments.\n    Usage:\n\n    checker = MutationChecker(op, flat_args, args_spec)\n    op(*args, **kwargs)\n    checker.check()\n    \"\"\"\n\n    def __init__(self, op, flat_args, args_spec):\n        self.op = op\n        self.args_spec = args_spec\n        self.flat_args = flat_args\n        self.real_pre_hashes = [\n            hash_tensor(a) if isinstance(a, torch.Tensor) else None for a in flat_args\n        ]\n\n    def check(self):\n        real_post_hashes = [\n            hash_tensor(a) if isinstance(a, torch.Tensor) else None\n            for a in self.flat_args\n        ]\n        was_mutated = [\n            not torch.equal(pre, post)\n            and not (pre.isnan().all() and post.isnan().all())\n            if isinstance(pre, torch.Tensor) and isinstance(post, torch.Tensor)\n            else None\n            for pre, post in zip(self.real_pre_hashes, real_post_hashes)\n        ]\n        was_mutated_args, was_mutated_kwargs = pytree.tree_unflatten(\n            was_mutated, self.args_spec\n        )\n        for info, was_mutated in zip_schema(\n            self.op._schema, was_mutated_args, was_mutated_kwargs\n        ):\n\n            def check_one(info, was_mutated):\n                if info.is_write == was_mutated:\n                    return\n                raise RuntimeError(\n                    f\"{self.op._name}: for argument '{info.name}': the operator's schema \"\n                    f\"{self.op._schema} specified that \"\n                    f\"the operator {'mutates' if info.is_write else 'does not mutate'} \"\n                    f\"the argument, but this seems to be empirically wrong. \"\n                    f\"Please make the schema and operator behavior consistent. \"\n                    f\"You can specify that an operator mutates a Tensor by \"\n                    f\"e.g. changing its schema type from 'Tensor name' to 'Tensor(a!) name'\"\n                    f\"(use different identifiers (a, b, c, ...) for different Tensors)\"\n                )\n\n            if is_tensor_like_type(info.type):\n                check_one(info, was_mutated)\n            elif is_tensorlist_like_type(info.type):\n                was_any_mutated = False if was_mutated is None else any(was_mutated)\n                check_one(info, was_any_mutated)\n\n\ndef hash_tensor(t: torch.Tensor) -> torch.Tensor:\n    \"\"\"Some inexpensive hash. Used as a quick and dirty indicator for tensor mutation\"\"\"\n    return t.detach().float().mean()\n\n\ndef has_fake_kernel(op: torch._ops.OpOverload) -> bool:\n    \"\"\"If an operator (that stays alive until FakeTensorMode) has a Fake kernel.\n    Don't use this if the operator decomposes before FakeTensorMode.\n    \"\"\"\n    if can_generate_trivial_fake_impl(op):\n        return True\n    name = op._name\n    if torch._C._dispatch_has_kernel_for_dispatch_key(\n        name, \"CompositeImplicitAutograd\"\n    ):\n        return True\n    opdef = torch._library.custom_ops._maybe_get_opdef(name)\n    if opdef is None:\n        # the non-torch.library.custom_op path\n        if torch._C._dispatch_has_kernel_for_dispatch_key(\n            name, \"CompositeExplicitAutograd\"\n        ):\n            return True\n        entry = torch._library.simple_registry.singleton.find(name)\n        if entry.fake_impl.kernel is not None:\n            return True\n        if torch._C._dispatch_has_kernel_for_dispatch_key(name, \"Meta\"):\n            return True\n    else:\n        # the torch.library.custom_op path\n        if opdef._abstract_fn is not None:\n            return True\n    return False\n\n\ndef mutated_args_kwargs(schema: _C.FunctionSchema) -> tuple[list[int], list[str]]:\n    idxs = []\n    keys = []\n    for i, info in enumerate(schema.arguments):\n        if info.alias_info is not None and info.alias_info.is_write:\n            if info.kwarg_only:\n                keys.append(info.name)\n            else:\n                idxs.append(i)\n    return idxs, keys\n\n\ntags_by_priority = [\n    _C.Tag.needs_exact_strides,\n    _C.Tag.needs_contiguous_strides,\n    _C.Tag.needs_fixed_stride_order,\n    _C.Tag.flexible_layout,\n]\n\n\ndef get_layout_constraint_tag(fn, *, with_default=True):\n    for tag in tags_by_priority:\n        if tag in fn.tags:\n            return tag\n    if with_default:\n        if is_builtin(fn):\n            return _C.Tag.flexible_layout\n        import torch._functorch\n        from torch._functorch import config\n\n        return getattr(torch._C.Tag, config.custom_op_default_layout_constraint)\n    return None\n", 515], "/data/wangyingqi/code/pytorch/torch/library.py": ["# mypy: allow-untyped-defs\nimport contextlib\nimport functools\nimport inspect\nimport re\nimport sys\nimport traceback\nimport weakref\nfrom collections.abc import Sequence\nfrom typing import (\n    Any,\n    Callable,\n    Literal,\n    Optional,\n    overload,\n    TYPE_CHECKING,\n    TypeVar,\n    Union,\n)\nfrom typing_extensions import deprecated, ParamSpec\n\nimport torch\nimport torch._library as _library\nfrom torch._library.custom_ops import (\n    _cast,\n    _maybe_get_opdef,\n    custom_op,\n    CustomOpDef,\n    device_types_t,\n)\nfrom torch._library.infer_schema import infer_schema  # noqa: F401\nfrom torch._library.triton import triton_op, wrap_triton\nfrom torch._ops import OpOverload\nfrom torch.types import _dtype\n\n\n__all__ = [\n    \"Library\",\n    \"impl\",\n    \"define\",\n    \"fallthrough_kernel\",\n    \"impl_abstract\",\n    \"register_autocast\",\n    \"register_fake\",\n    \"register_torch_dispatch\",\n    \"register_vmap\",\n    \"get_ctx\",\n    \"custom_op\",\n    \"triton_op\",\n    \"wrap_triton\",\n    \"infer_schema\",\n]\n\n_T = TypeVar(\"_T\")\n_P = ParamSpec(\"_P\")\n\n# Set containing the combination of (namespace, operator, DispatchKey) for which a new kernel has been registered\n# The keys in the set are of the form `namespace + \"/\" + op_name + \"/\" + dispatch_key`.\n# This set is maintained to ensure that two libraries don't try to override the exact same functionality to avoid\n# libraries calling into kernels not intended to be called.\n_impls: set[str] = set()\n_defs: set[str] = set()\n\n# prim is reserved by TorchScript interpreter\n_reserved_namespaces = [\"prim\"]\n\n\ndef fallthrough_kernel():\n    \"\"\"\n    A dummy function to pass to ``Library.impl`` in order to register a fallthrough.\n    \"\"\"\n    raise NotImplementedError(\"fallthrough_kernel() should never be called.\")\n\n\nclass Library:\n    \"\"\"\n    A class to create libraries that can be used to register new operators or\n    override operators in existing libraries from Python.\n    A user can optionally pass in a dispatch keyname if they only want to register\n    kernels corresponding to only one specific dispatch key.\n\n    To create a library to override operators in an existing library (with name ns), set the kind to \"IMPL\".\n    To create a new library (with name ns) to register new operators, set the kind to \"DEF\".\n    To create a fragment of a possibly existing library to register operators (and bypass\n    the limitation that there is only one library for a given namespace), set the kind to\n    \"FRAGMENT\".\n\n    Args:\n        ns: library name\n        kind: \"DEF\", \"IMPL\", \"FRAGMENT\"\n        dispatch_key: PyTorch dispatch key (default: \"\")\n    \"\"\"\n\n    def __init__(self, ns, kind, dispatch_key=\"\"):\n        from torch.fx.operator_schemas import _SCHEMA_TO_SIGNATURE_CACHE\n\n        if kind not in (\"IMPL\", \"DEF\", \"FRAGMENT\"):\n            raise ValueError(\"Unsupported kind: \", kind)\n\n        if ns in _reserved_namespaces and (kind == \"DEF\" or kind == \"FRAGMENT\"):\n            raise ValueError(\n                ns,\n                \" is a reserved namespace. Please try creating a library with another name.\",\n            )\n\n        frame = traceback.extract_stack(limit=3)[0]\n        filename, lineno = frame.filename, frame.lineno\n        self.m: Optional[Any] = torch._C._dispatch_library(\n            kind, ns, dispatch_key, filename, lineno\n        )\n        self.ns = ns\n        self._op_defs: set[str] = set()\n        self._op_impls: set[str] = set()\n        self._registration_handles: list[torch._library.utils.RegistrationHandle] = []\n        self.kind = kind\n        self.dispatch_key = dispatch_key\n        # Use a finalizer to setup the \"destructor\" instead of __del__.\n        # Python __del__ can lead to weird things (globals and locals may already\n        # be gone when __del__ actually gets called!). finalizers help the\n        # situation because it lets us capture references and keeps them alive\n        weakref.finalize(\n            self,\n            _del_library,\n            _impls,\n            self._op_impls,\n            _defs,\n            self._op_defs,\n            self._registration_handles,\n            self.m,\n            _SCHEMA_TO_SIGNATURE_CACHE,\n        )\n\n    def __repr__(self):\n        return f\"Library(kind={self.kind}, ns={self.ns}, dispatch_key={self.dispatch_key})>\"\n\n    def define(self, schema, alias_analysis=\"\", *, tags=()):\n        r\"\"\"Defines a new operator and its semantics in the ns namespace.\n\n        Args:\n            schema: function schema to define a new operator.\n            alias_analysis (optional): Indicates if the aliasing properties of the operator arguments can be\n                                       inferred from the schema (default behavior) or not (\"CONSERVATIVE\").\n            tags (Tag | Sequence[Tag]): one or more torch.Tag to apply to this\n                                       operator. Tagging an operator changes the operator's behavior\n                                       under various PyTorch subsystems; please read the docs for the\n                                       torch.Tag carefully before applying it.\n\n        Returns:\n            name of the operator as inferred from the schema.\n\n        Example::\n\n            >>> my_lib = Library(\"mylib\", \"DEF\")\n            >>> my_lib.define(\"sum(Tensor self) -> Tensor\")\n        \"\"\"\n\n        # This is added because we also want to disallow PURE_FUNCTION alias analysis which is a valid\n        # AliasAnalysis type in C++\n        if alias_analysis not in [\"\", \"FROM_SCHEMA\", \"CONSERVATIVE\"]:\n            raise RuntimeError(f\"Invalid alias_analysis type {alias_analysis}\")\n        assert self.m is not None\n        if isinstance(tags, torch.Tag):\n            tags = (tags,)\n\n        name = schema.split(\"(\")[0]\n        packet_name = name.split(\".\")[0] if \".\" in name else name\n        has_preexisting_packet = hasattr(torch.ops, self.ns) and hasattr(\n            getattr(torch.ops, self.ns), packet_name\n        )\n\n        result = self.m.define(schema, alias_analysis, tuple(tags))\n        name = schema.split(\"(\")[0]\n        qualname = self.ns + \"::\" + name\n\n        # If the OpOverloadPacket exists already, then this means we're adding a\n        # new OpOverload for it. Refresh the packet to include the new OpOverload.\n        if has_preexisting_packet:\n            ns = getattr(torch.ops, self.ns)\n            packet = getattr(ns, packet_name)\n            torch._ops._refresh_packet(packet)\n\n        self._op_defs.add(qualname)\n        _defs.add(qualname)\n        return result\n\n    def _register_fake(self, op_name, fn, _stacklevel=1, *, allow_override=False):\n        r\"\"\"Registers the fake impl for an operator defined in the library.\"\"\"\n\n        source = torch._library.utils.get_source(_stacklevel + 1)\n        frame = sys._getframe(_stacklevel)\n        caller_module = inspect.getmodule(frame)\n        # Can be none if you call register_fake from somewhere there isn't a module\n        # (e.g. __main__)\n        caller_module_name = None if caller_module is None else caller_module.__name__\n\n        # TODO(rzou): We're gonna need to stage this change with torchvision,\n        # since torchvision is github first.\n        if caller_module_name is not None and caller_module_name.startswith(\n            \"torchvision.\"\n        ):\n            caller_module_name = None\n\n        qualname = f\"{self.ns}::{op_name}\"\n        entry = torch._library.simple_registry.singleton.find(qualname)\n        if caller_module_name is not None:\n            func_to_register = _check_pystubs_once(fn, qualname, caller_module_name)\n        else:\n            func_to_register = fn\n\n        handle = entry.fake_impl.register(\n            func_to_register, source, lib=self, allow_override=allow_override\n        )\n        self._registration_handles.append(handle)\n\n    def _register_torch_dispatch_rule(self, op_name, torch_dispatch_class, fn):\n        r\"\"\"Registers a torch_dispatch rule for the given operator and torch_dispatch_class.\n\n        This allows for open registration to specify the behavior between the operator\n        and the torch_dispatch_class without needing to modify the torch_dispatch_class\n        or the operator directly.\n\n        The torch_dispatch_class is either a Tensor subclass with `__torch_dispatch__` or a\n        TorchDispatchMode.\n\n        If it is a Tensor subclass, we expect fn to have the following signature:\n        (cls, func: OpOverload, types: Tuple[type, ...], args, kwargs) -> Any\n\n        If it is a TorchDispatchMode, we expect fn to have the following signature:\n        (mode, func: OpOverload, types: Tuple[type, ...], args, kwargs) -> Any\n        \"\"\"\n\n        qualname = f\"{self.ns}::{op_name}\"\n        entry = torch._library.simple_registry.singleton.find(qualname)\n        handle = entry.torch_dispatch_rules.register(torch_dispatch_class, fn)\n        self._registration_handles.append(handle)\n\n    def _impl_with_aoti_compile(self, op_name, dispatch_key=\"\"):\n        r\"\"\"Register the operator to use the AOTI-compiled implementation.\n\n        Args:\n            op_name: operator name (along with the overload) or OpOverload object.\n            dispatch_key: dispatch key that the input function should be registered for. By default, it uses\n                          the dispatch key that the library was created with.\n\n        Example::\n\n            >>> my_lib = Library(\"aten\", \"IMPL\")\n            >>> my_lib._impl_with_aoti_compile(\"div.Tensor\", \"CPU\")\n        \"\"\"\n\n        if dispatch_key == \"\":\n            dispatch_key = self.dispatch_key\n        assert torch.DispatchKeySet(dispatch_key).has(torch._C.DispatchKey.Dense)\n\n        if isinstance(op_name, str):\n            name = op_name\n        elif isinstance(op_name, OpOverload):\n            name = op_name._schema.name\n            overload_name = op_name._schema.overload_name\n            if overload_name != \"\":\n                name = name + \".\" + overload_name\n        else:\n            raise RuntimeError(\n                \"_impl_with_aoti_compile should be passed either a name or an OpOverload object \"\n                \"as the first argument\"\n            )\n\n        key = self.ns + \"/\" + name.split(\"::\")[-1] + \"/\" + dispatch_key\n        if key in _impls:\n            # TODO: in future, add more info about where the existing function is registered (this info is\n            # today already returned by the C++ warning when _impl_with_aoti_compile is called but we error out before that)\n            raise RuntimeError(\n                \"This is not allowed since there's already a kernel registered from python overriding {}\"\n                \"'s behavior for {} dispatch key and {} namespace.\".format(\n                    name.split(\"::\")[-1], dispatch_key, self.ns\n                )\n            )\n\n        assert self.m is not None\n        impl_fn: Callable = self.m.impl_with_aoti_compile\n        impl_fn(self.ns, name.split(\"::\")[-1], dispatch_key)\n\n        _impls.add(key)\n        self._op_impls.add(key)\n\n    def impl(\n        self, op_name, fn, dispatch_key=\"\", *, with_keyset=False, allow_override=False\n    ):\n        r\"\"\"Registers the function implementation for an operator defined in the library.\n\n        Args:\n            op_name: operator name (along with the overload) or OpOverload object.\n            fn: function that's the operator implementation for the input dispatch key or :func:`~fallthrough_kernel`\n                to register a fallthrough.\n            dispatch_key: dispatch key that the input function should be registered for. By default, it uses\n                          the dispatch key that the library was created with.\n            with_keyset: flag controlling if the current dispatcher call keyset should be passed as the first argument\n                         to :attr:`fn` when calling. This should be used to create the appropriate keyset for redispatch calls.\n            allow_override: Flag controlling if we want to override an\n                         existing registered kernel implementation. This is by\n                         default off, and will error you're trying to register a\n                         kernel to a dispatch key with a kernel already\n                         registered.\n\n        Example::\n\n            >>> my_lib = Library(\"aten\", \"IMPL\")\n            >>> def div_cpu(self, other):\n            >>>     return self * (1 / other)\n            >>> my_lib.impl(\"div.Tensor\", div_cpu, \"CPU\")\n        \"\"\"\n\n        if not callable(fn):\n            raise TypeError(\n                f\"Input function is required to be a callable but found type {type(fn)}\"\n            )\n        if dispatch_key == \"\":\n            dispatch_key = self.dispatch_key\n\n        if isinstance(op_name, str):\n            name = op_name\n        elif isinstance(op_name, OpOverload):\n            name = op_name._schema.name\n            overload_name = op_name._schema.overload_name\n            if overload_name != \"\":\n                name = name + \".\" + overload_name\n        else:\n            raise RuntimeError(\n                \"impl should be passed either a name or an OpOverload object as the first argument\"\n            )\n\n        key = self.ns + \"/\" + name.split(\"::\")[-1] + \"/\" + dispatch_key\n        if (not allow_override) and key in _impls:\n            # TODO: in future, add more info about where the existing function is registered (this info is\n            # today already returned by the C++ warning when impl is called but we error out before that)\n            raise RuntimeError(\n                \"This is not allowed since there's already a kernel registered from python overriding {}\"\n                \"'s behavior for {} dispatch key and {} namespace.\".format(\n                    name.split(\"::\")[-1], dispatch_key, self.ns\n                )\n            )\n\n        if dispatch_key == \"Meta\":\n            dispatcher_op_name = name\n            if \"::\" not in dispatcher_op_name:\n                dispatcher_op_name = f\"{self.ns}::{dispatcher_op_name}\"\n\n            # Internally, we shouldn't be registering meta kernels for any operators that\n            # have CompositeImplicitAutograd kernels.\n            # Instead, we should be letting those decompositions run, and writing meta kernels\n            # only for the base operators.\n            if torch._C._dispatch_has_kernel_for_dispatch_key(\n                dispatcher_op_name, \"CompositeImplicitAutograd\"\n            ):\n                raise RuntimeError(\n                    f\"We should not register a meta kernel directly to the operator '{name}',\"\n                    \" because it has a CompositeImplicitAutograd kernel in core.\"\n                    \" Instead we should let the operator decompose, and ensure that we have meta kernels\"\n                    \" for the base ops that it decomposes into.\"\n                )\n\n        assert self.m is not None\n        self.m.impl(\n            name,\n            dispatch_key if dispatch_key != \"\" else \"CompositeImplicitAutograd\",\n            fn,\n            with_keyset,\n        )\n\n        _impls.add(key)\n        self._op_impls.add(key)\n\n    def fallback(self, fn, dispatch_key=\"\", *, with_keyset=False):\n        r\"\"\"Registers the function implementation as the fallback for the given key.\n\n        This function only works for a library with global namespace (\"_\").\n\n        Args:\n            fn: function used as fallback for the given dispatch key or :func:`~fallthrough_kernel`\n                to register a fallthrough.\n            dispatch_key: dispatch key that the input function should be registered for. By default, it uses\n                          the dispatch key that the library was created with.\n            with_keyset: flag controlling if the current dispatcher call keyset should be passed as the first argument\n                         to :attr:`fn` when calling. This should be used to create the appropriate keyset for redispatch calls.\n\n        Example::\n\n            >>> my_lib = Library(\"_\", \"IMPL\")\n            >>> def fallback_kernel(op, *args, **kwargs):\n            >>>     # Handle all autocast ops generically\n            >>>     # ...\n            >>> my_lib.fallback(fallback_kernel, \"Autocast\")\n        \"\"\"\n\n        if dispatch_key == \"\":\n            dispatch_key = self.dispatch_key\n\n        if self.ns != \"_\":\n            raise RuntimeError(\n                f\"\"\"Fallback can only be registered using library fragment on the global namespace \"_\" but it is {self.ns}\"\"\"\n            )\n\n        assert dispatch_key != \"\"\n        assert self.m is not None\n\n        self.m.fallback(dispatch_key, fn, with_keyset)\n\n    def _destroy(self):\n        if self.m is not None:\n            self.m.reset()\n        self.m = None\n        for handle in self._registration_handles:\n            handle.destroy()\n        self._registration_handles.clear()\n        global _impls\n        _impls -= self._op_impls\n        for name in self._op_defs:\n            # Delete the cached torch.ops.ns.foo if it was registered.\n            # Otherwise, accessing it leads to a segfault.\n            # It's possible that we only registered an overload in this Library\n            # and another library owns an alive overload.\n            # That's OK - the next time torch.ops.ns.foo gets called, it'll be\n            # recomputed to point at the right collection of overloads.\n            ns, name_with_overload = name.split(\"::\")\n            name = name_with_overload.split(\".\")[0]\n            if not hasattr(torch.ops, ns):\n                continue\n            namespace = getattr(torch.ops, ns)\n            if not hasattr(namespace, name):\n                continue\n            delattr(namespace, name)\n            namespace._dir.remove(name)\n\n\ndef _del_library(\n    captured_impls,\n    op_impls,\n    captured_defs,\n    op_defs,\n    registration_handles,\n    m,\n    schema_to_signature_cache,\n):\n    for op_def in op_defs:\n        name = op_def\n        overload_name = \"\"\n        if \".\" in op_def:\n            name, overload_name = op_def.split(\".\")\n        if (\n            name,\n            overload_name,\n        ) in schema_to_signature_cache:\n            del schema_to_signature_cache[(name, overload_name)]\n\n    captured_impls -= op_impls\n    captured_defs -= op_defs\n    for handle in registration_handles:\n        handle.destroy()\n\n    if m is not None:\n        m.reset()\n\n\n@contextlib.contextmanager\ndef _scoped_library(*args, **kwargs):\n    try:\n        lib = Library(*args, **kwargs)\n        yield lib\n    finally:\n        lib._destroy()\n\n\n_keep_alive: list[Library] = []\n\n\nNAMELESS_SCHEMA = re.compile(r\"\\(.*\\) -> .*\")\n\n\n@functools.singledispatch\ndef define(qualname, schema, *, lib=None, tags=()):\n    r\"\"\"Defines a new operator.\n\n    In PyTorch, defining an op (short for \"operator\") is a two step-process:\n    - we need to define the op (by providing an operator name and schema)\n    - we need to implement behavior for how the operator interacts with\n    various PyTorch subsystems, like CPU/CUDA Tensors, Autograd, etc.\n\n    This entrypoint defines the custom operator (the first step)\n    you must then perform the second step by calling various\n    ``impl_*`` APIs, like :func:`torch.library.impl` or\n    :func:`torch.library.register_fake`.\n\n    Args:\n        qualname (str): The qualified name for the operator. Should be\n            a string that looks like \"namespace::name\", e.g. \"aten::sin\".\n            Operators in PyTorch need a namespace to\n            avoid name collisions; a given operator may only be created once.\n            If you are writing a Python library, we recommend the namespace to\n            be the name of your top-level module.\n        schema (str): The schema of the operator. E.g. \"(Tensor x) -> Tensor\"\n            for an op that accepts one Tensor and returns one Tensor. It does\n            not contain the operator name (that is passed in ``qualname``).\n        lib (Optional[Library]): If provided, the lifetime of this operator\n            will be tied to the lifetime of the Library object.\n        tags (Tag | Sequence[Tag]): one or more torch.Tag to apply to this\n            operator. Tagging an operator changes the operator's behavior\n            under various PyTorch subsystems; please read the docs for the\n            torch.Tag carefully before applying it.\n\n    Example::\n        >>> import torch\n        >>> import numpy as np\n        >>>\n        >>> # Define the operator\n        >>> torch.library.define(\"mylib::sin\", \"(Tensor x) -> Tensor\")\n        >>>\n        >>> # Add implementations for the operator\n        >>> @torch.library.impl(\"mylib::sin\", \"cpu\")\n        >>> def f(x):\n        >>>     return torch.from_numpy(np.sin(x.numpy()))\n        >>>\n        >>> # Call the new operator from torch.ops.\n        >>> x = torch.randn(3)\n        >>> y = torch.ops.mylib.sin(x)\n        >>> assert torch.allclose(y, x.sin())\n\n    \"\"\"\n    if not isinstance(qualname, str):\n        raise ValueError(\n            f\"define(qualname, schema): expected qualname \"\n            f\"to be instance of str, got {type(qualname)}\"\n        )\n    namespace, name = torch._library.utils.parse_namespace(qualname)\n    if lib is None:\n        lib = Library(namespace, \"FRAGMENT\")\n        _keep_alive.append(lib)\n    if not NAMELESS_SCHEMA.fullmatch(schema):\n        raise ValueError(\n            f\"define(qualname, schema, ...): expected schema \"\n            f'to look like e.g. \"(Tensor x) -> Tensor\" but '\n            f'got \"{schema}\"'\n        )\n    lib.define(name + schema, alias_analysis=\"\", tags=tags)\n\n\n@define.register\ndef _(lib: Library, schema, alias_analysis=\"\"):\n    \"\"\"The old torch.library.define.\n    We're keeping this around for BC reasons\n    \"\"\"\n\n    def wrap(f):\n        name = lib.define(schema, alias_analysis)\n        lib.impl(name, f)\n        return f\n\n    return wrap\n\n\n@overload\ndef impl(\n    qualname: str,\n    types: Union[str, Sequence[str]],\n    func: Literal[None] = None,\n    *,\n    lib: Optional[Library] = None,\n) -> Callable[[Callable[..., object]], None]: ...\n\n\n@overload\ndef impl(\n    qualname: str,\n    types: Union[str, Sequence[str]],\n    func: Callable[..., object],\n    *,\n    lib: Optional[Library] = None,\n) -> None: ...\n\n\n# Deprecated BC API\n@overload\ndef impl(\n    lib: Library,\n    name: str,\n    dispatch_key: str = \"\",\n) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]: ...\n\n\n@functools.singledispatch\ndef impl(\n    qualname: str,\n    types: Union[str, Sequence[str]],\n    func: Optional[Callable[_P, _T]] = None,\n    *,\n    lib: Optional[Library] = None,\n) -> object:\n    \"\"\"Register an implementation for a device type for this operator.\n\n    You may pass \"default\" for ``types`` to register this implementation as the\n    default implementation for ALL device types.\n    Please only use this if the implementation truly supports all device types;\n    for example, this is true if it is a composition of built-in PyTorch operators.\n\n    This API may be used as a decorator. You can use nested decorators\n    with this API provided they return a function and are placed inside\n    this API (see Example 2).\n\n    Some valid types are: \"cpu\", \"cuda\", \"xla\", \"mps\", \"ipu\", \"xpu\".\n\n    Args:\n        qualname (str): Should be a string that looks like \"namespace::operator_name\".\n        types (str | Sequence[str]): The device types to register an impl to.\n        lib (Optional[Library]): If provided, the lifetime of this registration\n            will be tied to the lifetime of the Library object.\n\n    Examples:\n        >>> import torch\n        >>> import numpy as np\n        >>> # Example 1: Register function.\n        >>> # Define the operator\n        >>> torch.library.define(\"mylib::mysin\", \"(Tensor x) -> Tensor\")\n        >>>\n        >>> # Add implementations for the cpu device\n        >>> @torch.library.impl(\"mylib::mysin\", \"cpu\")\n        >>> def f(x):\n        >>>     return torch.from_numpy(np.sin(x.numpy()))\n        >>>\n        >>> x = torch.randn(3)\n        >>> y = torch.ops.mylib.mysin(x)\n        >>> assert torch.allclose(y, x.sin())\n        >>>\n        >>> # Example 2: Register function with decorator.\n        >>> def custom_decorator(func):\n        >>>     def wrapper(*args, **kwargs):\n        >>>         return func(*args, **kwargs) + 1\n        >>>     return wrapper\n        >>>\n        >>> # Define the operator\n        >>> torch.library.define(\"mylib::sin_plus_one\", \"(Tensor x) -> Tensor\")\n        >>>\n        >>> # Add implementations for the operator\n        >>> @torch.library.impl(\"mylib::sin_plus_one\", \"cpu\")\n        >>> @custom_decorator\n        >>> def f(x):\n        >>>     return torch.from_numpy(np.sin(x.numpy()))\n        >>>\n        >>> # Call the new operator from torch.ops.\n        >>> x = torch.randn(3)\n        >>>\n        >>> y1 = torch.ops.mylib.sin_plus_one(x)\n        >>> y2 = torch.sin(x) + 1\n        >>> assert torch.allclose(y1, y2)\n    \"\"\"\n    return _impl(qualname, types, func, lib=lib, disable_dynamo=False)\n\n\nif not TYPE_CHECKING:\n\n    @impl.register\n    def _(\n        lib: Library, name: str, dispatch_key: str = \"\"\n    ) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]:\n        \"\"\"Legacy torch.library.impl API. Kept around for BC\"\"\"\n\n        def wrap(f: Callable[_P, _T]) -> Callable[_P, _T]:\n            lib.impl(name, f, dispatch_key)\n            return f\n\n        return wrap\n\n\n@overload\ndef _impl(\n    qualname: str,\n    types: Union[str, Sequence[str]],\n    func: Literal[None] = None,\n    *,\n    lib: Optional[Library] = None,\n    disable_dynamo: bool = False,\n) -> Callable[[Callable[..., object]], None]: ...\n\n\n@overload\ndef _impl(\n    qualname: str,\n    types: Union[str, Sequence[str]],\n    func: Callable[..., object],\n    *,\n    lib: Optional[Library] = None,\n    disable_dynamo: bool = False,\n) -> None: ...\n\n\ndef _impl(\n    qualname: str,\n    types: Union[str, Sequence[str]],\n    func: Optional[Callable[..., object]] = None,\n    *,\n    lib: Optional[Library] = None,\n    disable_dynamo: bool = False,\n) -> Optional[Callable[[Callable[..., object]], None]]:\n    # See impl()\n    if isinstance(types, str):\n        types = (types,)\n    keys = set({})\n    for typ in types:\n        is_dispatch_key = torch._C._parse_dispatch_key(typ)\n        if is_dispatch_key:\n            # We also support passing a DispatchKey to impl. Please prefer using\n            # the higher-level torch.library APIs and only pass DispatchKey to\n            # torch.library.impl with caution (or even better, don't use this\n            # option and file an issue on GitHub for what you need).\n            # We don't advertise this to users because\n            # it is very easy to shoot yourself in the foot.\n            keys.add(typ)\n        else:\n            keys.add(_device_type_to_key(typ))\n\n    def register_(func: Callable[..., object]) -> None:\n        namespace, _ = torch._library.utils.parse_namespace(qualname)\n\n        if lib is None:\n            use_lib = Library(namespace, \"FRAGMENT\")\n            _keep_alive.append(use_lib)\n        else:\n            use_lib = lib\n        if disable_dynamo:\n\n            @torch._disable_dynamo\n            def func_no_dynamo(*args, **kwargs):\n                return func(*args, **kwargs)\n\n            for key in keys:\n                use_lib.impl(qualname, func_no_dynamo, key)\n        else:\n            for key in keys:\n                use_lib.impl(qualname, func, key)\n\n    if func is None:\n        return register_\n    else:\n        register_(func)\n        return None\n\n\ndef _device_type_to_key(device_type: str) -> str:\n    if device_type == \"default\":\n        # This is technically not correct, because although all device_type\n        # DispatchKeys are included in CompositeExplicitAutograd,\n        # not everything in CompositeExplicitAutograd is associated with a\n        # device_type. I don't really care that much about the difference.\n        return \"CompositeExplicitAutograd\"\n    return torch._C._dispatch_key_for_device(device_type)\n\n\n@deprecated(\n    \"`torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that \"\n    \"instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\",\n    category=FutureWarning,\n)\ndef impl_abstract(qualname, func=None, *, lib=None, _stacklevel=1):\n    r\"\"\"This API was renamed to :func:`torch.library.register_fake` in PyTorch 2.4.\n    Please use that instead.\n    \"\"\"\n    if func is not None:\n        _stacklevel = _stacklevel + 1\n    return register_fake(qualname, func, lib=lib, _stacklevel=_stacklevel)\n\n\n_op_identifier = Union[\n    str, \"torch._ops.OpOverload\", \"torch._library.custom_ops.CustomOpDef\"\n]\n\n\ndef register_kernel(\n    op: _op_identifier,\n    device_types: device_types_t,\n    func: Optional[Callable] = None,\n    /,\n    *,\n    lib: Optional[Library] = None,\n):\n    \"\"\"Register an implementation for a device type for this operator.\n\n    Some valid device_types are: \"cpu\", \"cuda\", \"xla\", \"mps\", \"ipu\", \"xpu\".\n    This API may be used as a decorator.\n\n    Args:\n        op (str | OpOverload): The operator to register an impl to.\n        device_types (None | str | Sequence[str]): The device_types to register an impl to.\n            If None, we will register to all device types -- please only use\n            this option if your implementation is truly device-type-agnostic.\n        func (Callable): The function to register as the implementation for\n            the given device types.\n        lib (Optional[Library]): If provided, the lifetime of this registration\n\n    Examples::\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n        >>> import torch\n        >>> from torch import Tensor\n        >>> from torch.library import custom_op\n        >>> import numpy as np\n        >>>\n        >>> # Create a custom op that works on cpu\n        >>> @custom_op(\"mylib::numpy_sin\", mutates_args=(), device_types=\"cpu\")\n        >>> def numpy_sin(x: Tensor) -> Tensor:\n        >>>     x_np = x.numpy()\n        >>>     y_np = np.sin(x_np)\n        >>>     return torch.from_numpy(y_np)\n        >>>\n        >>> # Add implementations for the cuda device\n        >>> @torch.library.register_kernel(\"mylib::numpy_sin\", \"cuda\")\n        >>> def _(x):\n        >>>     x_np = x.cpu().numpy()\n        >>>     y_np = np.sin(x_np)\n        >>>     return torch.from_numpy(y_np).to(device=x.device)\n        >>>\n        >>> x_cpu = torch.randn(3)\n        >>> x_cuda = x_cpu.cuda()\n        >>> assert torch.allclose(numpy_sin(x_cpu), x_cpu.sin())\n        >>> assert torch.allclose(numpy_sin(x_cuda), x_cuda.sin())\n\n    \"\"\"\n\n    if not isinstance(\n        op, (str, torch._ops.OpOverload, torch._library.custom_ops.CustomOpDef)\n    ):\n        raise ValueError(\n            f\"register_kernel({op}): got unexpected type for op: {type(op)}\"\n        )\n    if isinstance(op, torch._ops.OpOverload):\n        op = op._name\n    opdef = _maybe_get_opdef(op)\n    if opdef is not None:\n        return opdef.register_kernel(device_types, func)\n    assert isinstance(op, str)\n    if device_types is None:\n        device_types = \"CompositeExplicitAutograd\"\n\n    return _impl(op, device_types, func, lib=lib, disable_dynamo=True)\n\n\ndef register_autocast(\n    op: _op_identifier,\n    device_type: str,\n    cast_inputs: _dtype,\n    /,\n    *,\n    lib: Optional[Library] = None,\n):\n    r\"\"\"Register an autocast dispatch rule for this custom op.\n\n    Valid `device_type` include: \"cpu\" and \"cuda\".\n\n    Args:\n        op (str | OpOverload): The operator to register an autocast dispatch rule to.\n        device_type(str):  Device type to use. 'cuda' or 'cpu'.\n            The type is the same as the `type` attribute of a :class:`torch.device`.\n            Thus, you may obtain the device type of a tensor using `Tensor.device.type`.\n        cast_inputs (:class:`torch.dtype`): When custom op runs in an autocast-enabled region,\n            casts incoming floating-point Tensors to the target dtype (non-floating-point Tensors\n            are not affected), then executes custom op with autocast disabled.\n        lib (Optional[Library]): If provided, the lifetime of this registration\n\n    Examples::\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n        >>> import torch\n        >>> from torch import Tensor\n        >>> from torch.library import custom_op\n        >>>\n        >>> # Create a custom op that works on cuda\n        >>> @torch.library.custom_op(\"mylib::my_sin\", mutates_args=())\n        >>> def my_sin(x: Tensor) -> Tensor:\n        >>>     return torch.sin(x)\n        >>>\n        >>> # Register autocast dispatch rule for the cuda device\n        >>> torch.library.register_autocast(\"mylib::my_sin\", \"cuda\", torch.float16)\n        >>>\n        >>> x = torch.randn(3, dtype=torch.float32, device=\"cuda\")\n        >>> with torch.autocast(\"cuda\", dtype=torch.float16):\n        >>>     y = torch.ops.mylib.my_sin(x)\n        >>> assert y.dtype == torch.float16\n\n    \"\"\"\n    if not isinstance(\n        op, (str, torch._ops.OpOverload, torch._library.custom_ops.CustomOpDef)\n    ):\n        raise ValueError(\n            f\"register_autocast({op}): got unexpected type for op: {type(op)}\"\n        )\n    if device_type not in [\"cpu\", \"cuda\"]:\n        raise ValueError(f\"Unknown device type: {device_type}\")\n\n    if isinstance(op, torch._ops.OpOverload):\n        op = op._name\n    opdef = _maybe_get_opdef(op)\n    if opdef is not None:\n        return opdef.register_autocast(device_type, cast_inputs)\n\n    assert isinstance(op, str)\n    qualname = op\n    _op = torch._library.utils.lookup_op(qualname)\n\n    namespace, opname = torch._library.utils.parse_namespace(qualname)\n    if lib is None:\n        lib = Library(namespace, \"FRAGMENT\")\n        _keep_alive.append(lib)\n\n    def kernel(_, *args, **kwargs):\n        assert len(kwargs) == 0, \"Custom ops do not support kwargs yet.\"\n        autocast_keyset = torch._C.DispatchKeySet(\n            torch._C.DispatchKey.AutocastCPU\n        ) | torch._C.DispatchKeySet(torch._C.DispatchKey.AutocastCUDA)\n        with torch._C._ExcludeDispatchKeyGuard(autocast_keyset):\n            return _op(*_cast(args, device_type, cast_inputs))\n\n    if device_type == \"cuda\":\n        return lib.impl(opname, kernel, \"AutocastCUDA\", with_keyset=True)\n    else:\n        # device_type is \"cpu\"\n        return lib.impl(opname, kernel, \"AutocastCPU\", with_keyset=True)\n\n\ndef register_fake(\n    op: _op_identifier,\n    func: Optional[Callable] = None,\n    /,\n    *,\n    lib: Optional[Library] = None,\n    _stacklevel: int = 1,\n    allow_override: bool = False,\n):\n    r\"\"\"Register a FakeTensor implementation (\"fake impl\") for this operator.\n\n    Also sometimes known as a \"meta kernel\", \"abstract impl\".\n\n    An \"FakeTensor implementation\" specifies the behavior of this operator on\n    Tensors that carry no data (\"FakeTensor\"). Given some input Tensors with\n    certain properties (sizes/strides/storage_offset/device), it specifies\n    what the properties of the output Tensors are.\n\n    The FakeTensor implementation has the same signature as the operator.\n    It is run for both FakeTensors and meta tensors. To write a FakeTensor\n    implementation, assume that all Tensor inputs to the operator are\n    regular CPU/CUDA/Meta tensors, but they do not have storage, and\n    you are trying to return regular CPU/CUDA/Meta tensor(s) as output.\n    The FakeTensor implementation must consist of only PyTorch operations\n    (and may not directly access the storage or data of any input or\n    intermediate Tensors).\n\n    This API may be used as a decorator (see examples).\n\n    For a detailed guide on custom ops, please see\n    https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html\n\n    Args:\n        op_name: Operator name (along with the overload) or OpOverload object.\n        func: Fake tensor implementation.\n        lib (Optional[Library]): Library to register the fake tensor to.\n        allow_override: Flag controlling if we want to override an\n                        existing registered fake impl. This is by default off,\n                        and will error you're trying to register a fake impl to\n                        an operator that already has a fake impl. This also only\n                        applies if the custom operator was not created via\n                        torch.library.custom_op, as overriding and existing fake\n                        impl is already allowed.\n\n    Examples:\n        >>> import torch\n        >>> import numpy as np\n        >>> from torch import Tensor\n        >>>\n        >>> # Example 1: an operator without data-dependent output shape\n        >>> @torch.library.custom_op(\"mylib::custom_linear\", mutates_args=())\n        >>> def custom_linear(x: Tensor, weight: Tensor, bias: Tensor) -> Tensor:\n        >>>     raise NotImplementedError(\"Implementation goes here\")\n        >>>\n        >>> @torch.library.register_fake(\"mylib::custom_linear\")\n        >>> def _(x, weight, bias):\n        >>>     assert x.dim() == 2\n        >>>     assert weight.dim() == 2\n        >>>     assert bias.dim() == 1\n        >>>     assert x.shape[1] == weight.shape[1]\n        >>>     assert weight.shape[0] == bias.shape[0]\n        >>>     assert x.device == weight.device\n        >>>\n        >>>     return (x @ weight.t()) + bias\n        >>>\n        >>> with torch._subclasses.fake_tensor.FakeTensorMode():\n        >>>     x = torch.randn(2, 3)\n        >>>     w = torch.randn(3, 3)\n        >>>     b = torch.randn(3)\n        >>>     y = torch.ops.mylib.custom_linear(x, w, b)\n        >>>\n        >>> assert y.shape == (2, 3)\n        >>>\n        >>> # Example 2: an operator with data-dependent output shape\n        >>> @torch.library.custom_op(\"mylib::custom_nonzero\", mutates_args=())\n        >>> def custom_nonzero(x: Tensor) -> Tensor:\n        >>>     x_np = x.numpy(force=True)\n        >>>     res = np.stack(np.nonzero(x_np), axis=1)\n        >>>     return torch.tensor(res, device=x.device)\n        >>>\n        >>> @torch.library.register_fake(\"mylib::custom_nonzero\")\n        >>> def _(x):\n        >>> # Number of nonzero-elements is data-dependent.\n        >>> # Since we cannot peek at the data in an fake impl,\n        >>> # we use the ctx object to construct a new symint that\n        >>> # represents the data-dependent size.\n        >>>     ctx = torch.library.get_ctx()\n        >>>     nnz = ctx.new_dynamic_size()\n        >>>     shape = [nnz, x.dim()]\n        >>>     result = x.new_empty(shape, dtype=torch.int64)\n        >>>     return result\n        >>>\n        >>> from torch.fx.experimental.proxy_tensor import make_fx\n        >>>\n        >>> x = torch.tensor([0, 1, 2, 3, 4, 0])\n        >>> trace = make_fx(torch.ops.mylib.custom_nonzero, tracing_mode=\"symbolic\")(x)\n        >>> trace.print_readable()\n        >>>\n        >>> assert torch.allclose(trace(x), torch.ops.mylib.custom_nonzero(x))\n\n    \"\"\"\n    if not isinstance(\n        op, (str, torch._ops.OpOverload, torch._library.custom_ops.CustomOpDef)\n    ):\n        raise ValueError(f\"register_fake({op}): got unexpected type for op: {type(op)}\")\n    if isinstance(op, torch._ops.OpOverload):\n        op = op._name\n    opdef = _maybe_get_opdef(op)\n    if opdef is not None:\n        if func is None:\n            return opdef.register_fake\n        else:\n            return opdef.register_fake(func)\n    assert isinstance(op, str)\n\n    stacklevel = _stacklevel\n\n    def register(func):\n        namespace, op_name = torch._library.utils.parse_namespace(op)\n        if lib is None:\n            use_lib = Library(namespace, \"FRAGMENT\")\n            _keep_alive.append(use_lib)\n        else:\n            use_lib = lib\n        use_lib._register_fake(\n            op_name, func, _stacklevel=stacklevel + 1, allow_override=allow_override\n        )\n        return func\n\n    if func is None:\n        return register\n    else:\n        stacklevel += 1\n        return register(func)\n\n\ndef register_autograd(\n    op: _op_identifier,\n    backward: Callable,\n    /,\n    *,\n    setup_context: Optional[Callable] = None,\n    lib=None,\n) -> None:\n    r\"\"\"Register a backward formula for this custom op.\n\n    In order for an operator to work with autograd, you need to register\n    a backward formula:\n    1. You must tell us how to compute gradients during the backward pass\n    by providing us a \"backward\" function.\n    2. If you need any values from the forward to compute gradients, you can\n    use `setup_context` to save values for backward.\n\n    ``backward`` runs during the backward pass. It accepts ``(ctx, *grads)``:\n    - ``grads`` is one or more gradients. The number of gradients matches\n    the number of outputs of the operator.\n    The ``ctx`` object is `the same ctx object <context_method_mixins>`_ used by\n    :class:`torch.autograd.Function`. The semantics of ``backward_fn`` are the\n    same as :meth:`torch.autograd.Function.backward`.\n\n    ``setup_context(ctx, inputs, output)`` runs during the forward pass.\n    Please save quantities needed for backward onto the ``ctx`` object via\n    either :meth:`torch.autograd.function.FunctionCtx.save_for_backward`\n    or assigning them as attributes of ``ctx``. If your custom op has\n    kwarg-only arguments, we expect the signature of ``setup_context``\n    to be ``setup_context(ctx, inputs, keyword_only_inputs, output)``.\n\n    Both ``setup_context_fn`` and ``backward_fn`` must be traceable. That is,\n    they may not directly access :meth:`torch.Tensor.data_ptr` and they must\n    not depend on or mutate global state. If you need a non-traceable backward,\n    you can make it a separate custom_op that you call inside ``backward_fn``.\n\n    If you need different autograd behavior on different devices, then we\n    recommend creating two different custom operators, one for each device\n    that needs different behavior, and switching between them at runtime.\n\n    Examples:\n        >>> import torch\n        >>> import numpy as np\n        >>> from torch import Tensor\n        >>>\n        >>> @torch.library.custom_op(\"mylib::numpy_sin\", mutates_args=())\n        >>> def numpy_sin(x: Tensor) -> Tensor:\n        >>>     x_np = x.cpu().numpy()\n        >>>     y_np = np.sin(x_np)\n        >>>     return torch.from_numpy(y_np).to(device=x.device)\n        >>>\n        >>> def setup_context(ctx, inputs, output) -> Tensor:\n        >>>     x, = inputs\n        >>>     ctx.save_for_backward(x)\n        >>>\n        >>> def backward(ctx, grad):\n        >>>     x, = ctx.saved_tensors\n        >>>     return grad * x.cos()\n        >>>\n        >>> torch.library.register_autograd(\n        ...     \"mylib::numpy_sin\", backward, setup_context=setup_context\n        ... )\n        >>>\n        >>> x = torch.randn(3, requires_grad=True)\n        >>> y = numpy_sin(x)\n        >>> (grad_x,) = torch.autograd.grad(y, x, torch.ones_like(y))\n        >>> assert torch.allclose(grad_x, x.cos())\n        >>>\n        >>> # Example with a keyword-only arg\n        >>> @torch.library.custom_op(\"mylib::numpy_mul\", mutates_args=())\n        >>> def numpy_mul(x: Tensor, *, val: float) -> Tensor:\n        >>>     x_np = x.cpu().numpy()\n        >>>     y_np = x_np * val\n        >>>     return torch.from_numpy(y_np).to(device=x.device)\n        >>>\n        >>> def setup_context(ctx, inputs, keyword_only_inputs, output) -> Tensor:\n        >>>     ctx.val = keyword_only_inputs[\"val\"]\n        >>>\n        >>> def backward(ctx, grad):\n        >>>     return grad * ctx.val\n        >>>\n        >>> torch.library.register_autograd(\n        ...     \"mylib::numpy_mul\", backward, setup_context=setup_context\n        ... )\n        >>>\n        >>> x = torch.randn(3, requires_grad=True)\n        >>> y = numpy_mul(x, val=3.14)\n        >>> (grad_x,) = torch.autograd.grad(y, x, torch.ones_like(y))\n        >>> assert torch.allclose(grad_x, torch.full_like(x, 3.14))\n\n    \"\"\"\n    if not isinstance(\n        op, (str, torch._ops.OpOverload, torch._library.custom_ops.CustomOpDef)\n    ):\n        raise ValueError(\n            f\"register_autograd({op}): got unexpected type for op: {type(op)}\"\n        )\n    if isinstance(op, torch._ops.OpOverload):\n        op = op._name\n    opdef = _maybe_get_opdef(op)\n    if opdef is not None:\n        opdef.register_autograd(backward, setup_context=setup_context)\n        return\n\n    assert isinstance(op, str)\n    qualname = op\n    op = torch._library.utils.lookup_op(qualname)\n    schema = op._schema\n    if not _library.utils.is_functional_schema(schema):\n        raise RuntimeError(\n            f\"Cannot register autograd formula for non-functional operator \"\n            f\"{op} with schema {schema}. Please create \"\n            f\"a functional operator and register an autograd formula for that.\"\n        )\n    if _library.utils.has_kwarg_only_tensors(schema):\n        raise NotImplementedError(\n            f\"register_autograd with kwarg-only Tensor args. In the original \"\n            f\"definition of the op, please make your tensors not kwarg-only. \"\n            f\"Got: {schema}\"\n        )\n\n    info = _library.autograd.Info(backward, setup_context)\n    autograd_kernel = _library.autograd.make_autograd_impl(op, info)\n    namespace, opname = torch._library.utils.parse_namespace(qualname)\n    if lib is None:\n        lib = Library(namespace, \"FRAGMENT\")\n        _keep_alive.append(lib)\n    lib.impl(opname, autograd_kernel, \"Autograd\", with_keyset=True)\n\n\ndef register_torch_dispatch(\n    op: _op_identifier,\n    torch_dispatch_class: Any,\n    func: Optional[Callable] = None,\n    /,\n    *,\n    lib: Optional[Library] = None,\n):\n    r\"\"\"Registers a torch_dispatch rule for the given operator and ``torch_dispatch_class``.\n\n    This allows for open registration to specify the behavior between the operator\n    and the ``torch_dispatch_class`` without needing to modify the ``torch_dispatch_class``\n    or the operator directly.\n\n    The ``torch_dispatch_class`` is either a Tensor subclass with ``__torch_dispatch__`` or a\n    TorchDispatchMode.\n\n    If it is a Tensor subclass, we expect ``func`` to have the following signature:\n    ``(cls, func: OpOverload, types: Tuple[type, ...], args, kwargs) -> Any``\n\n    If it is a TorchDispatchMode, we expect ``func`` to have the following signature:\n    ``(mode, func: OpOverload, types: Tuple[type, ...], args, kwargs) -> Any``\n\n    ``args`` and ``kwargs`` will have been normalized the same way they are\n    in ``__torch_dispatch__`` (see :ref:`torch-dispatch-calling-convention`).\n\n    Examples:\n\n        >>> import torch\n        >>>\n        >>> @torch.library.custom_op(\"mylib::foo\", mutates_args={})\n        >>> def foo(x: torch.Tensor) -> torch.Tensor:\n        >>>     return x.clone()\n        >>>\n        >>> class MyMode(torch.utils._python_dispatch.TorchDispatchMode):\n        >>>     def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n        >>>         return func(*args, **kwargs)\n        >>>\n        >>> @torch.library.register_torch_dispatch(\"mylib::foo\", MyMode)\n        >>> def _(mode, func, types, args, kwargs):\n        >>>     x, = args\n        >>>     return x + 1\n        >>>\n        >>> x = torch.randn(3)\n        >>> y = foo(x)\n        >>> assert torch.allclose(y, x)\n        >>>\n        >>> with MyMode():\n        >>>     y = foo(x)\n        >>> assert torch.allclose(y, x + 1)\n\n    \"\"\"\n    if not isinstance(\n        op, (str, torch._ops.OpOverload, torch._library.custom_ops.CustomOpDef)\n    ):\n        raise ValueError(\n            f\"register_torch_dispatch({op}): got unexpected type for op: {type(op)}\"\n        )\n    if isinstance(op, torch._ops.OpOverload):\n        op = op._name\n    opdef = _maybe_get_opdef(op)\n    if opdef is not None:\n        return opdef.register_torch_dispatch(torch_dispatch_class, func)\n    assert isinstance(op, str)\n\n    def register(func):\n        namespace, op_name = torch._library.utils.parse_namespace(op)\n        if lib is None:\n            use_lib = Library(namespace, \"FRAGMENT\")\n            _keep_alive.append(use_lib)\n        else:\n            use_lib = lib\n        use_lib._register_torch_dispatch_rule(op_name, torch_dispatch_class, func)\n        return func\n\n    if func is None:\n        return register\n    else:\n        return register(func)\n\n\ndef register_vmap(\n    op: _op_identifier,\n    func: Optional[Callable] = None,\n    /,\n    *,\n    lib=None,\n):\n    r\"\"\"Register a vmap implementation to support :func:`torch.vmap` for this custom op.\n\n    This API may be used as a decorator (see examples).\n\n    In order for an operator to work with :func:`torch.vmap`, you may need to register a\n    vmap implementation in the following signature:\n\n        ``vmap_func(info, in_dims: Tuple[Optional[int]], *args, **kwargs)``,\n\n    where ``*args`` and ``**kwargs`` are the arguments and kwargs for ``op``.\n    We do not support kwarg-only Tensor args.\n\n    It specifies how do we compute the batched version of ``op`` given inputs with an additional\n    dimension (specified by ``in_dims``).\n\n    For each arg in ``args``, ``in_dims`` has a corresponding ``Optional[int]``. It is ``None``\n    if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer\n    specifying what dimension of the Tensor is being vmapped over.\n\n    ``info`` is a collection of additional metadata that may be helpful:\n    ``info.batch_size`` specifies the size of the dimension being vmapped over, while\n    ``info.randomness`` is the ``randomness`` option that was passed to :func:`torch.vmap`.\n\n    The return of the function ``func`` is a tuple of ``(output, out_dims)``. Similar to ``in_dims``,\n    ``out_dims`` should be of the same structure as ``output`` and contain one ``out_dim``\n    per output that specifies if the output has the vmapped dimension and what index it is in.\n\n    Examples:\n        >>> import torch\n        >>> import numpy as np\n        >>> from torch import Tensor\n        >>> from typing import Tuple\n        >>>\n        >>> def to_numpy(tensor):\n        >>>     return tensor.cpu().numpy()\n        >>>\n        >>> lib = torch.library.Library(\"mylib\", \"FRAGMENT\")\n        >>> @torch.library.custom_op(\"mylib::numpy_cube\", mutates_args=())\n        >>> def numpy_cube(x: Tensor) -> Tuple[Tensor, Tensor]:\n        >>>     x_np = to_numpy(x)\n        >>>     dx = torch.tensor(3 * x_np ** 2, device=x.device)\n        >>>     return torch.tensor(x_np ** 3, device=x.device), dx\n        >>>\n        >>> def numpy_cube_vmap(info, in_dims, x):\n        >>>     result = numpy_cube(x)\n        >>>     return result, (in_dims[0], in_dims[0])\n        >>>\n        >>> torch.library.register_vmap(numpy_cube, numpy_cube_vmap)\n        >>>\n        >>> x = torch.randn(3)\n        >>> torch.vmap(numpy_cube)(x)\n        >>>\n        >>> @torch.library.custom_op(\"mylib::numpy_mul\", mutates_args=())\n        >>> def numpy_mul(x: Tensor, y: Tensor) -> Tensor:\n        >>>     return torch.tensor(to_numpy(x) * to_numpy(y), device=x.device)\n        >>>\n        >>> @torch.library.register_vmap(\"mylib::numpy_mul\")\n        >>> def numpy_mul_vmap(info, in_dims, x, y):\n        >>>     x_bdim, y_bdim = in_dims\n        >>>     x = x.movedim(x_bdim, -1) if x_bdim is not None else x.unsqueeze(-1)\n        >>>     y = y.movedim(y_bdim, -1) if y_bdim is not None else y.unsqueeze(-1)\n        >>>     result = x * y\n        >>>     result = result.movedim(-1, 0)\n        >>>     return result, 0\n        >>>\n        >>>\n        >>> x = torch.randn(3)\n        >>> y = torch.randn(3)\n        >>> torch.vmap(numpy_mul)(x, y)\n\n    .. note::\n        The vmap function should aim to preserve the semantics of the entire custom operator.\n        That is, ``grad(vmap(op))`` should be replaceable with a ``grad(map(op))``.\n\n        If your custom operator has any custom behavior in the backward pass, please\n        keep this in mind.\n\n    \"\"\"\n    if not isinstance(\n        op, (str, torch._ops.OpOverload, torch._library.custom_ops.CustomOpDef)\n    ):\n        raise ValueError(f\"register_vmap({op}): got unexpected type for op: {type(op)}\")\n    if isinstance(op, torch._ops.OpOverload):\n        op = op._name\n    opdef = _maybe_get_opdef(op)\n    if opdef is not None:\n        return opdef.register_vmap(func)\n    assert isinstance(op, str)\n    qualname = op\n    op = torch._library.utils.lookup_op(qualname)\n    schema = op._schema\n    if _library.utils.has_kwarg_only_tensors(schema):\n        raise NotImplementedError(\n            f\"register_vmap with kwarg-only Tensor args. In the original \"\n            f\"definition of the op, please make your tensors not kwarg-only. \"\n            f\"Got: {schema}\"\n        )\n\n    def register(func):\n        nonlocal op, lib\n\n        namespace, opname = torch._library.utils.parse_namespace(qualname)\n        if lib is None:\n            lib = Library(namespace, \"FRAGMENT\")\n            _keep_alive.append(lib)\n\n        from torch._functorch.autograd_function import custom_function_call_vmap_helper\n        from torch._functorch.pyfunctorch import retrieve_current_functorch_interpreter\n\n        def wrapped_func(keyset, *args, **kwargs):\n            interpreter = retrieve_current_functorch_interpreter()\n            return custom_function_call_vmap_helper(\n                interpreter, func, op, *args, **kwargs\n            )\n\n        lib.impl(opname, wrapped_func, \"FuncTorchBatched\", with_keyset=True)\n\n    if func is None:\n        return register\n    else:\n        return register(func)\n\n\n# If the op was defined in C++, then we want to make sure there was an\n# m.set_python_module(module, ...) call and that the module is the\n# same as the module that called torch.library.register_fake.\ndef _check_pystubs_once(func, qualname, actual_module_name):\n    checked = False\n\n    def inner(*args, **kwargs):\n        nonlocal checked\n        if checked:\n            return func(*args, **kwargs)\n\n        op = torch._library.utils.lookup_op(qualname)\n        if op._defined_in_python:\n            checked = True\n            return func(*args, **kwargs)\n\n        maybe_pystub = torch._C._dispatch_pystub(\n            op._schema.name, op._schema.overload_name\n        )\n        if maybe_pystub is None:\n            if torch._library.utils.requires_set_python_module():\n                namespace = op.namespace\n                cpp_filename = op._handle.debug()\n                raise RuntimeError(\n                    f\"Operator '{qualname}' was defined in C++ and has a Python \"\n                    f\"fake impl. In this situation, we require there to also be a \"\n                    f'companion C++ `m.set_python_module(\"{actual_module_name}\")` '\n                    f\"call, but we could not find one. Please add that to \"\n                    f\"to the top of the C++ TORCH_LIBRARY({namespace}, ...) block the \"\n                    f\"operator was registered in ({cpp_filename})\"\n                )\n        else:\n            pystub_module = maybe_pystub[0]\n            if actual_module_name != pystub_module:\n                cpp_filename = op._handle.debug()\n                raise RuntimeError(\n                    f\"Operator '{qualname}' specified that its python fake impl \"\n                    f\"is in the Python module '{pystub_module}' but it was actually found \"\n                    f\"in '{actual_module_name}'. Please either move the fake impl \"\n                    f\"or correct the m.set_python_module call ({cpp_filename})\"\n                )\n        checked = True\n        return func(*args, **kwargs)\n\n    return inner\n\n\n# NOTE [ctx inside the fake implementation]\n# If a user has an operator with data-dependent output shape, then when writing\n# a fake implementation they must query the current ctx and use methods on the\n# ctx to construct a new unbacked symint.\n#\n# This is done via us setting the global_ctx_getter function every time a fake\n# implementation is invoked.\ndef get_ctx() -> \"torch._library.fake_impl.FakeImplCtx\":\n    \"\"\"get_ctx() returns the current AbstractImplCtx object.\n\n    Calling ``get_ctx()`` is only valid inside of an fake impl\n    (see :func:`torch.library.register_fake` for more usage details.\n    \"\"\"\n    return torch._library.fake_impl.global_ctx_getter()\n\n\n_OPCHECK_DEFAULT_UTILS = (\n    \"test_schema\",\n    \"test_autograd_registration\",\n    \"test_faketensor\",\n    \"test_aot_dispatch_dynamic\",\n)\n\n\ndef opcheck(\n    op: Union[torch._ops.OpOverload, torch._ops.OpOverloadPacket, CustomOpDef],\n    args: tuple[Any, ...],\n    kwargs: Optional[dict[str, Any]] = None,\n    *,\n    test_utils: Union[str, Sequence[str]] = _OPCHECK_DEFAULT_UTILS,\n    raise_exception: bool = True,\n    atol=None,\n    rtol=None,\n) -> dict[str, str]:\n    \"\"\"Given an operator and some sample arguments, tests if the operator is\n    registered correctly.\n\n    That is, when you use the torch.library/TORCH_LIBRARY APIs to create a\n    custom op, you specified metadata (e.g. mutability info) about the custom op\n    and these APIs require that the functions you pass them satisfy certain\n    properties (e.g. no data pointer access in the fake/meta/abstract kernel)\n    ``opcheck`` tests these metadata and properties.\n\n    Concretely, we test the following:\n\n    - test_schema: If the schema matches the implementation of\n      the operator. For example: if the schema specifies a Tensor is mutated,\n      then we check the implementation mutates the Tensor. If the schema\n      specifies that we return a new Tensor, then we check that the\n      implementation returns a new Tensor (instead of an existing one or\n      a view of an existing one).\n    - test_autograd_registration: If the operator supports training\n      (autograd): we check that its autograd formula is registered via\n      torch.library.register_autograd or a manual registration to one\n      or more DispatchKey::Autograd keys. Any other DispatchKey-based\n      registrations may lead to undefined behavior.\n    - test_faketensor: If the operator has a FakeTensor kernel\n      (and if it is correct). The FakeTensor kernel is necessary (\n      but not sufficient) for the operator to work with PyTorch compilation\n      APIs (torch.compile/export/FX). We check that a FakeTensor kernel\n      (also sometimes known as a meta kernel) was registered for the\n      operator and that it is correct. This test takes the result of\n      running the operator on real tensors and the result of running\n      the operator on FakeTensors and checks that they have the same\n      Tensor metadata (sizes/strides/dtype/device/etc).\n    - test_aot_dispatch_dynamic: If the operator has correct behavior\n      with PyTorch compilation APIs (torch.compile/export/FX).\n      This checks that the outputs (and gradients, if applicable) are the\n      same under eager-mode PyTorch and torch.compile.\n      This test is a superset of ``test_faketensor`` and is an e2e test;\n      other things it tests are that the operator supports\n      functionalization and that the backward pass (if it exists) also\n      supports FakeTensor and functionalization.\n\n    For best results, please call ``opcheck`` multiple times with a\n    representative set of inputs. If your operator supports\n    autograd, please use ``opcheck`` with inputs with ``requires_grad = True``;\n    if your operator supports multiple devices (e.g. CPU and CUDA), please\n    use ``opcheck`` with inputs on all supported devices.\n\n    Args:\n        op: The operator. Must either be a function decorated with\n            :func:`torch.library.custom_op` or an OpOverload/OpOverloadPacket\n            found in torch.ops.* (e.g. torch.ops.aten.sin, torch.ops.mylib.foo)\n        args: The args to the operator\n        kwargs: The kwargs to the operator\n        test_utils: Tests that we should run. Default: all of them.\n            Example: (\"test_schema\", \"test_faketensor\")\n        raise_exception: If we should raise an exception on the first\n            error. If False, we will return a dict with information\n            on if each test passed or not.\n        rtol (Optional[float]): Relative tolerance for floating point comparisons.\n            If specified ``atol`` must also be specified.\n            If omitted, default values based on the ``dtype`` are selected\n            (see the table in :func:`torch.testing.assert_close`).\n        atol (Optional[float]): Absolute tolerance for floating point comparisons.\n            If specified ``rtol`` must also be specified.\n            If omitted, default values based on the ``dtype`` are selected\n            (see the table in :func:`torch.testing.assert_close`).\n\n    .. warning::\n\n        opcheck and :func:`torch.autograd.gradcheck` test different things;\n        opcheck tests if your usage of torch.library APIs is correct while\n        :func:`torch.autograd.gradcheck` tests if your autograd formula is\n        mathematically correct. Use both to test custom ops that support\n        gradient computation.\n\n    Example:\n\n        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)\n        >>> @torch.library.custom_op(\"mylib::numpy_mul\", mutates_args=())\n        >>> def numpy_mul(x: Tensor, y: float) -> Tensor:\n        >>>     x_np = x.numpy(force=True)\n        >>>     z_np = x_np * y\n        >>>     return torch.from_numpy(z_np).to(x.device)\n        >>>\n        >>> @numpy_mul.register_fake\n        >>> def _(x, y):\n        >>>     return torch.empty_like(x)\n        >>>\n        >>> def setup_context(ctx, inputs, output):\n        >>>     y, = inputs\n        >>>     ctx.y = y\n        >>>\n        >>> def backward(ctx, grad):\n        >>>     return grad * ctx.y, None\n        >>>\n        >>> numpy_mul.register_autograd(backward, setup_context=setup_context)\n        >>>\n        >>> sample_inputs = [\n        >>>     (torch.randn(3), 3.14),\n        >>>     (torch.randn(2, 3, device='cuda'), 2.718),\n        >>>     (torch.randn(1, 10, requires_grad=True), 1.234),\n        >>>     (torch.randn(64, 64, device='cuda', requires_grad=True), 90.18),\n        >>> ]\n        >>>\n        >>> for args in sample_inputs:\n        >>>     torch.library.opcheck(numpy_mul, args)\n\n    \"\"\"\n    import torch.testing._internal.optests as optests\n\n    return optests.opcheck(\n        op,\n        args,\n        kwargs,\n        test_utils=test_utils,\n        raise_exception=raise_exception,\n        rtol=rtol,\n        atol=atol,\n    )\n", 1599], "/data/wangyingqi/code/pytorch/torch/_subclasses/fake_tensor.py": ["# mypy: allow-untyped-decorators\nfrom __future__ import annotations\n\nimport atexit\nimport contextlib\nimport dataclasses\nimport functools\nimport logging\nimport math\nimport os\nimport threading\nimport traceback\nimport types\nimport typing\nimport weakref\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, cast, Literal, Optional, TYPE_CHECKING, TypeVar, Union\nfrom typing_extensions import Self, TypeGuard\nfrom weakref import ReferenceType\n\nimport torch\nimport torch._library.utils as library_utils\nfrom torch import SymBool, SymFloat, SymInt, Tensor\nfrom torch._C._functorch import is_functorch_wrapped_tensor, is_legacy_batchedtensor\nfrom torch._library.fake_class_registry import FakeScriptObject\nfrom torch._library.fake_profile import MissingOpProfile\nfrom torch._logging import dtrace_structured\nfrom torch._prims_common import suggest_memory_format\nfrom torch._subclasses.meta_utils import (\n    assert_eq,\n    assert_metadata_eq,\n    is_sparse_any,\n    is_sparse_compressed,\n    MetaConverter,\n)\nfrom torch._utils import render_call\nfrom torch.fx.immutable_collections import immutable_dict\nfrom torch.fx.operator_schemas import normalize_function\nfrom torch.multiprocessing.reductions import StorageWeakRef\nfrom torch.overrides import TorchFunctionMode\nfrom torch.types import IntLikeType, py_sym_types\nfrom torch.utils._backport_slots import dataclass_slots\nfrom torch.utils._mode_utils import no_dispatch\nfrom torch.utils._python_dispatch import (\n    is_traceable_wrapper_subclass,\n    TorchDispatchMode,\n)\nfrom torch.utils._pytree import KeyPath, keystr, PyTree, tree_map, tree_map_, TreeSpec\nfrom torch.utils._stats import count\nfrom torch.utils._traceback import CapturedTraceback\n\nfrom ._fake_tensor_utils import _CacheKeyState, _PySymInputStub, _SymIntOutputStub\n\n\nif TYPE_CHECKING:\n    from collections.abc import Generator, Iterable, Mapping, Sequence\n    from types import TracebackType\n\n    from torch._guards import Source\n    from torch._ops import OpOverload\n    from torch.fx.experimental.symbolic_shapes import ShapeEnv, SymbolicContext\n\nlog = logging.getLogger(__name__)\nhc_log = torch._logging.getArtifactLogger(__name__, \"hierarchical_compile\")\n\n# TODO: Hack to unblock https://github.com/pytorch/pytorch/pull/108186\n# Proper fix tracked by https://github.com/pytorch/pytorch/issues/120105\ntry:\n    not_implemented_log = torch._logging.getArtifactLogger(__name__, \"not_implemented\")\nexcept ValueError as e:\n    if \"'not_implemented' not registered\" in str(e):\n        not_implemented_log = logging.getLogger(__name__ + \".not_implemented\")\n    else:\n        raise e\n\n\nDimList = list\n\npytree = torch.utils._pytree\nT = TypeVar(\"T\")\n\naten = torch._ops.ops.aten\n\nCONSTANT_NUMEL_LIMIT = 1\n\nRECURSION_COUNT = 0\n\n\n# Small helper that increments recursion count, and\n# resets it when the object goes out of scope.  Useful\n# if you don't want to increase indentation which is\n# what a context manager would do.\nclass IncrementRecursionCount:\n    def __init__(self) -> None:\n        global RECURSION_COUNT\n        RECURSION_COUNT += 1\n\n    def __del__(self) -> None:\n        global RECURSION_COUNT\n        RECURSION_COUNT -= 1\n\n\n@dataclass\nclass UnsupportedFakeTensorException(RuntimeError):\n    reason: str\n\n\n@dataclass\nclass DynamicOutputShapeException(RuntimeError):\n    func: OpOverload\n\n\n@dataclass\nclass DataDependentOutputException(RuntimeError):\n    func: OpOverload\n\n\n@dataclass\nclass UnsupportedOperatorException(RuntimeError):\n    func: OpOverload\n\n\n@dataclass\nclass UnsupportedMutationAliasingException(RuntimeError):\n    reason: str\n\n\n@dataclass\nclass MetadataMismatchError(RuntimeError):\n    reason: str\n\n\nclass FakeTensorTLS(threading.local):\n    # Default to None, otherwise it'll be used to override _all_\n    # `FakeTensorMode.allow_non_fake_inputs` in this thread.\n    allow_non_fake_inputs_override: Optional[bool]\n\n    def __init__(self) -> None:\n        self.allow_non_fake_inputs_override = None\n\n\nfake_tensor_tls = FakeTensorTLS()\n\n\ndef ordered_set(*items: T) -> dict[T, Literal[True]]:\n    return dict.fromkeys(items, True)\n\n\n@contextlib.contextmanager\ndef unset_fake_temporarily() -> Generator[Optional[TorchDispatchMode], None, None]:\n    old = torch._C._unset_dispatch_mode(torch._C._TorchDispatchModeKey.FAKE)\n    try:\n        yield old\n    finally:\n        if old is not None:\n            torch._C._set_dispatch_mode(old)\n\n\n@contextlib.contextmanager\ndef disable_fake_tensor_cache(fake_mode: FakeTensorMode) -> Generator[None, None, None]:\n    old_value: bool = fake_mode.cache_enabled\n    try:\n        fake_mode.cache_enabled = False\n        yield\n    finally:\n        fake_mode.cache_enabled = old_value\n\n\ndef get_plain_tensors(\n    subclass: Tensor, *, out: list[Union[Tensor, int, SymInt]]\n) -> list[Union[Tensor, int, SymInt]]:\n    # This function is used in Runtime, do not add redundant asserts\n    todo = [subclass]\n    while todo:\n        curr = todo.pop()\n        if not is_traceable_wrapper_subclass(curr):\n            out.append(curr)\n            continue\n\n        inner_keys, _ = curr.__tensor_flatten__()\n        todo.extend(getattr(curr, key) for key in reversed(inner_keys))\n\n    return out\n\n\ndef is_fake(x: object) -> TypeGuard[Tensor]:\n    from torch._subclasses.functional_tensor import FunctionalTensor\n\n    if isinstance(x, FakeTensor):\n        return True\n    if is_traceable_wrapper_subclass(x):\n        attrs, _ = type(x).__tensor_flatten__(x)\n        flattened_tensors = [getattr(x, attr) for attr in attrs]\n        all_fake = all(is_fake(x) for x in flattened_tensors)\n        any_fake = any(is_fake(x) for x in flattened_tensors)\n        assert all_fake == any_fake, \"got mixed fake and real tensors!\"\n        return all_fake\n    elif isinstance(x, FunctionalTensor):\n        return is_fake(x.elem)\n    elif isinstance(x, Tensor) and torch._is_functional_tensor(x):\n        reapply_views = torch._C._functionalization_reapply_views_tls()\n        unwrapped = torch._C._functorch._unwrap_functional_tensor(x, reapply_views)\n        return is_fake(unwrapped)\n    elif isinstance(x, Tensor) and is_functorch_wrapped_tensor(x):\n        unwrapped = torch._C._functorch.get_unwrapped(x)\n        return is_fake(unwrapped)\n    return False\n\n\ndef maybe_get_fake_mode(t: object) -> Optional[FakeTensorMode]:\n    from torch._subclasses.functional_tensor import FunctionalTensor\n\n    if isinstance(t, FakeTensor):\n        return t.fake_mode\n    if is_traceable_wrapper_subclass(t):\n        inner_tensor_names, _ = t.__tensor_flatten__()\n        modes = [\n            maybe_get_fake_mode(getattr(t, t_name)) for t_name in inner_tensor_names\n        ]\n        m = modes[0]\n        assert all(m is x for x in modes)\n        return m\n    elif isinstance(t, FunctionalTensor):\n        return maybe_get_fake_mode(t.elem)\n    elif isinstance(t, Tensor) and torch._is_functional_tensor(t):\n        reapply_views = torch._C._functionalization_reapply_views_tls()\n        unwrapped = torch._C._functorch._unwrap_functional_tensor(t, reapply_views)\n        return maybe_get_fake_mode(unwrapped)\n    elif isinstance(t, Tensor) and is_functorch_wrapped_tensor(t):\n        unwrapped = torch._C._functorch.get_unwrapped(t)\n        return maybe_get_fake_mode(unwrapped)\n    return None\n\n\n@functools.cache\ndef get_schema_info(func: OpOverload) -> torch._C._SchemaInfo:\n    return torch._C._SchemaInfo(func._schema)\n\n\n# many of the decompositions registered to torch/_prims do not at the moment model\n# aliasing or strides, so as an incremental step, just enable the decompositions in\n# torch/_decomp/decompositions.py.\n# decomps are used for aot autograd tracing so we would like to unify on their\n# implementation and add additional testing to them\n@functools.cache\ndef torch_decomp_decompositions(func: OpOverload) -> bool:\n    from torch._decomp import decomposition_table\n\n    decompositions = torch._decomp.decompositions\n    # Note that the function in the decomposition table might be\n    # different from the one in the module because of the difference\n    # in out handling in aten API and torch public API\n    return decomposition_table[func].__module__.startswith(\n        \"torch._decomp\"\n    ) and decomposition_table[func].__name__ in dir(decompositions)\n\n\ndef tree_flatten_only(ty: type[T], tree: PyTree) -> list[T]:\n    flat_vals = pytree.tree_leaves(tree)\n    return [elem for elem in flat_vals if isinstance(elem, ty)]\n\n\ndef _is_plain_tensor(t: object) -> bool:\n    return (\n        type(t) is Tensor\n        and t.layout == torch.strided\n        and not (\n            t.is_sparse\n            or t.is_nested\n            or is_functorch_wrapped_tensor(t)\n            or is_legacy_batchedtensor(t)\n            or torch._is_functional_tensor(t)\n        )\n    )\n\n\n# Similar to `MetaConverter`, this is a class for converting\n# multiple tensors into fake tensors which share the same view/storage\n# structure. Like `MetaConverter`, it uses `WeakIdRef` to\n# hold a weak reference for all memoized tensors.\nclass FakeTensorConverter:\n    @property\n    def tensor_memo(\n        self,\n    ) -> weakref.WeakValueDictionary:\n        # not valid until py3.10\n        # weakref.WeakValueDictionary[\"torch._subclasses.meta_utils.MetaTensorId\", Optional[\"FakeTensor\"]]\n        return self.meta_converter.tensor_memo\n\n    meta_converter: MetaConverter\n    constant_storage_mapping: dict[StorageWeakRef, list[ReferenceType]]\n    export: bool\n\n    def __init__(self, *, copy_data: bool = False, export: bool = False) -> None:\n        self.meta_converter = MetaConverter(copy_data=copy_data)\n        self.export = export\n\n        # map from to storage to corresponding constant tensors\n        self.constant_storage_mapping = {}\n\n    def add_constant_storage_mapping(self, fake_tensor: FakeTensor) -> None:\n        # when you have a constant, aliased tensor:\n        # const_tensor.add_(torch.rand([1]))\n        # all aliases of it must become no longer const\n        assert isinstance(fake_tensor, FakeTensor) and fake_tensor.constant is not None\n        weak_st = StorageWeakRef(fake_tensor.constant._typed_storage())\n\n        # we need a map from a weak storage to all of its corresponding\n        # constant tensors. python doesn't have the weak value equivalent\n        # of defaultdict(list), so we are using a WeakValueDictionary as one\n        if weak_st not in self.constant_storage_mapping:\n            self.constant_storage_mapping[weak_st] = []\n        self.constant_storage_mapping[weak_st].append(weakref.ref(fake_tensor))\n\n    def invalidate_constant_aliases(self, tensor: Tensor) -> None:\n        assert not isinstance(tensor, FakeTensor)\n\n        weak_st = StorageWeakRef(tensor._typed_storage())\n        if weak_st not in self.constant_storage_mapping:\n            return\n\n        for weak_tensor_ref in self.constant_storage_mapping[weak_st]:\n            ten = weak_tensor_ref()\n            if ten is not None:\n                ten._fix_weakref()\n                ten.constant = None\n\n        del self.constant_storage_mapping[weak_st]\n\n    def _get_memo(self, t: Tensor) -> Optional[FakeTensor]:\n        tid = self.meta_converter.describer.lookup_tensor.get(t)\n        if tid is None:\n            return None\n        return self.tensor_memo.get(tid)\n\n    def set_tensor_memo(self, t: Tensor, v: FakeTensor) -> None:\n        tid = self.meta_converter.describer.get_tensor_id(t)\n        self.meta_converter.tensor_memo[tid] = v\n\n    # You can have a real tensor that you need to convert into a fake tensor.\n    # If you have a meta tensor already, call from_meta_and_device.\n    #\n    # You're allowed to pass a meta tensor to be turned into a fake\n    # tensor; although an odd thing to do, this can occur if you're doing\n    # cross ref testing and the inner test is already operating on meta tensors.\n    def from_real_tensor(\n        self,\n        fake_mode: FakeTensorMode,\n        t: Tensor,\n        make_constant: bool = False,\n        shape_env: Optional[ShapeEnv] = None,\n        *,\n        source: Optional[Source] = None,\n        symbolic_context: Optional[SymbolicContext] = None,\n        trace: bool = True,\n    ) -> FakeTensor:\n        # see note [Tensor Fakification and Symbol Caching]\n        if not symbolic_context and not source and shape_env:\n            if tracing_context := torch._guards.TracingContext.try_get():\n                if t in tracing_context.tensor_to_context:\n                    symbolic_context = tracing_context.tensor_to_context[t]\n                    from torch.fx.experimental.symbolic_shapes import (\n                        StatefulSymbolicContext,\n                    )\n\n                    assert isinstance(symbolic_context, StatefulSymbolicContext)\n                    source = symbolic_context.tensor_source\n\n        maybe_memo = self._get_memo(t)\n        if maybe_memo is not None:\n            return maybe_memo\n        # not yet supported in metatensors\n        if t.is_quantized:\n            raise UnsupportedFakeTensorException(\"quantized nyi in meta tensors\")\n        if type(t) is torch.nn.Parameter:\n            assert not make_constant\n\n        constant = t if make_constant else None\n\n        # This callback is used by both subclass and inner tensors. Require the\n        # caller to explicitly specify the device in case outer and inner tensors\n        # have different devices.\n        def mk_fake_tensor(\n            make_meta_t: Callable[[], object], device: Union[torch.device, str]\n        ) -> FakeTensor:\n            # NB: don't use in_kernel_invocation_manager. to\n            # ensure FakeTensor can internally do constant computation\n            # as necessary.  Invocation manager is \"more correct\" as\n            # it works for more operators in make_meta_t, but\n            # invariant is that make_meta_t only calls factories\n            # for which it is not strictly necessary to use the\n            # invocation manager (I think!)\n            with no_dispatch():\n                return FakeTensor(\n                    fake_mode,\n                    make_meta_t(),\n                    device,\n                    # TODO: callback might be used in recursive contexts, in\n                    # which case using t is wrong!  BUG!\n                    constant=constant,\n                )\n\n        out = self.meta_converter(\n            t,\n            shape_env=shape_env,\n            callback=mk_fake_tensor,\n            source=source,\n            symbolic_context=symbolic_context,\n            trace=trace,\n        )\n        if out is NotImplemented:\n            raise UnsupportedFakeTensorException(\"meta converter nyi\")\n\n        from torch._dynamo.source import RandomValueSource\n\n        value = None\n        if (\n            not self.export\n            and _is_plain_tensor(t)  # mostly, we want to know if item() works\n            and t.dim() == 0\n            and t.device.type == \"cpu\"\n            # All integer types are fair game, because signed overflow is UB\n            # (and even int64 can overflow, since integers in Python are\n            # arbitrary precision). But only float64 is OK for float, because\n            # switching between float32 and float64 changes semantics in an\n            # observable way without hitting UB.\n            and t.dtype\n            in [torch.int64, torch.int32, torch.int16, torch.int8, torch.float64]\n            and source is not None\n            # Impede setting up item() on things coming from random.  These\n            # are not \"real\" item() calls, instead UnspecializedPythonVariable\n            # is unsafely pretending an int is a tensor, which can sometimes\n            # implicitly cause an item call.  The problem is this is pretty\n            # unsound: there's no reason substituting an int with a Tensor is\n            # going to give the same results.  Today, you mostly get around\n            # this by typically not having capture_scalar_outputs on and graph\n            # breaking when someone tries to use the unspec variable in an\n            # int-y context.  But allowing it through here would break that.\n            # So don't.\n            #\n            # Once random values are setup to be represented as\n            # SymNodeVariable, this condition can be removed.  To check if\n            # you've done it right, this is a good test:\n            #\n            #   PYTORCH_TEST_WITH_DYNAMO=1 python test/test_reductions.py -k\n            #   TestReductionsCPU.test_dim_reduction_fns_fn_name_amax_cpu_bfloat16\n            and not isinstance(source, RandomValueSource)\n            # In Dynamo, shape_env is never none (even with static shapes).\n            # However, FakeTensorMode can be used by hand and in some cases\n            # ShapeEnv is not allocated.\n            and shape_env is not None\n        ):\n            from torch._dynamo.source import CallMethodItemSource, FloatTensorSource\n            from torch.fx.experimental.symbolic_shapes import DimDynamic\n\n            with no_dispatch():\n                value = t.item()\n            if not math.isnan(value) and not math.isinf(value):\n                # Peephole strip out unnecessary torch.as_tensor(x).item()\n                if isinstance(source, FloatTensorSource):\n                    item_source = source.base\n                else:\n                    item_source = CallMethodItemSource(source)\n                symbol = shape_env.create_unspecified_symbol(\n                    value,\n                    source=item_source,\n                    dynamic_dim=DimDynamic.DYNAMIC,\n                    symbolic_context=symbolic_context,\n                )\n                # NB: reusing item_memo here ensures that we invalidate on\n                # mutation\n                if t.dtype == torch.int64:\n                    out.item_memo = shape_env.create_symintnode(\n                        symbol,\n                        hint=value,\n                        source=item_source,\n                    )\n                elif t.dtype == torch.float64:\n                    out.item_memo = shape_env.create_symfloatnode(\n                        symbol,\n                        hint=value,\n                        source=item_source,\n                    )\n        if make_constant:\n            self.add_constant_storage_mapping(out)\n        # NB: meta_converter set the memo\n        return out\n\n    # If you specify the device, it MUST be a meta tensor.\n    def from_meta_and_device(\n        self,\n        fake_mode: FakeTensorMode,\n        t: Tensor,\n        device: torch.device,\n        pytype: Optional[type[torch.Tensor]] = None,\n        dispatch_keys: Optional[torch.DispatchKeySet] = None,\n    ) -> FakeTensor:\n        assert t.device.type == \"meta\", (\n            f\"tensor's device must be `meta`, got {t.device.type} instead\"\n        )\n        # This is a bit abusive (this is not the \"real\" tensor) but whatever,\n        # the meta tensor should be fresh so there's no way to get it wrong\n        maybe_memo = self._get_memo(t)\n        if maybe_memo is not None:\n            return maybe_memo\n        out = FakeTensor(\n            fake_mode, t, device, pytype=pytype, dispatch_keys=dispatch_keys\n        )\n        self.set_tensor_memo(t, out)\n        return out\n\n\n@functools.cache\ndef init_gpu_context(device: torch.device) -> None:\n    # Backward will error with cuda Fake Tensors if no cuda tensors have been initialized first\n    if torch.cuda.is_available() or torch.xpu.is_available():\n        (\n            torch.empty(1, device=device)\n            if torch.version.hip is None\n            else torch.zeros(1, device=device)\n        )\n\n\n@contextlib.contextmanager\ndef in_kernel_invocation_manager(\n    fake_mode: FakeTensorMode,\n) -> Generator[None, None, None]:\n    # See: note [Fake Tensor Dispatch Keys]\n    prev_in_kernel = fake_mode.in_kernel_invocation\n    meta_in_tls = torch._C._meta_in_tls_dispatch_include()\n    assert meta_in_tls == prev_in_kernel, f\"{meta_in_tls}, {prev_in_kernel}\"\n\n    with torch._C._DisableTorchDispatch():\n        fake_mode.in_kernel_invocation = True\n        # Unfortunately _set_meta_in_tls_dispatch_include(False) can leave\n        # `Dense` turned on (because it's implied by `Meta`)\n        with torch._C._PreserveDispatchKeyGuard():\n            torch._C._set_meta_in_tls_dispatch_include(True)\n            try:\n                yield\n            finally:\n                fake_mode.in_kernel_invocation = prev_in_kernel\n                # torch._C._set_meta_in_tls_dispatch_include(prev_in_kernel)\n\n\n# Return if the function allows Python numbers to bind to Tensors\ndef should_allow_numbers_as_tensors(func: OpOverload) -> bool:\n    return torch._C._should_allow_numbers_as_tensors(\n        func.name().split(\"::\")[-1].split(\".\")[0]\n    )\n\n\nclass FakeTensorConfig:\n    debug = os.environ.get(\"TORCH_FAKE_TENSOR_DEBUG\", \"0\") == \"1\"\n\n\n# This memorizes unbacked SymInt or SymFloats representing quantities like the\n# number of nonzero elements in this tensor or learning rate. There is one\n# instance of the descriptor per particular quantity to memoize.\n#\n# Memoization is helpful if you do something like x[mask] and y[mask];\n# mask.nonzero() gets repeatedly called and should give a consistent unbacked\n# SymInt. It needs to be invalidated in the same way constant is.\n#\n# Making this a descriptor may seem overly fancy, but actually it's the most\n# convenient way to ensure access to FakeTensor during access, which is\n# required for testing version counter and epoch validity.\nclass SymNumberMemoDescriptor:\n    _name: str\n\n    # By default, SymInts in this memo are invalidated across versions/epochs.\n    # nested_ints however are preserved across epochs and across versions.\n    # Preserving across versions is okay for nested int since the association\n    # of a nested int is agnostic to the underlying data and nested ints are not\n    # shared across multiple distinct tensors.\n    _is_nested_int: bool\n\n    def __init__(self, *, is_nested_int: bool = False) -> None:\n        self._is_nested_int = is_nested_int\n\n    def __set_name__(self, owner: str, name: str) -> None:\n        self._name = name\n\n    def _memo(self, obj: FakeTensor) -> str:\n        return f\"_{self._name}\"\n\n    def _memo_vc(self, obj: FakeTensor) -> str:\n        return f\"_{self._name}_vc\"\n\n    # When we retrace, we need to invalidate all the memos so that we can\n    # accurately identify the first time unbacked SymInts are allocated.\n    # This is only relevant for inputs; for intermediates, they will get fresh\n    # fake tensors so you won't have a memo anyway\n    def _memo_epoch(self, obj: FakeTensor) -> str:\n        return f\"_{self._name}_epoch\"\n\n    def __get__(\n        self, obj: FakeTensor, objtype: Optional[type[FakeTensor]] = None\n    ) -> Optional[Union[torch.SymInt, torch.SymFloat]]:\n        if (r := getattr(obj, self._memo(obj))) is None:\n            return None\n\n        # If backed, it's ok to preserve memo since we know it won't renumber.\n        if isinstance(r, torch.SymFloat) and r.node.hint is not None:\n            return r\n\n        # Version counter based tracking isn't 100% sound but it's close\n        # enough\n        if (\n            not self._is_nested_int and getattr(obj, self._memo_vc(obj)) != obj._version\n        ) or (\n            not self._is_nested_int\n            and getattr(obj, self._memo_epoch(obj)) != obj.fake_mode.epoch\n        ):\n            setattr(obj, self._memo(obj), None)\n            return None\n        return r\n\n    def __set__(\n        self, obj: FakeTensor, value: Optional[Union[torch.SymInt, torch.SymFloat]]\n    ) -> None:\n        if value is None:\n            setattr(obj, self._memo(obj), None)\n            setattr(obj, self._memo_vc(obj), None)\n            setattr(obj, self._memo_epoch(obj), None)\n        elif not obj.is_inference() or self._is_nested_int:\n            setattr(obj, self._memo(obj), value)\n            if not self._is_nested_int:\n                setattr(obj, self._memo_vc(obj), obj._version)\n            setattr(obj, self._memo_epoch(obj), obj.fake_mode.epoch)\n\n\nclass FakeTensor(Tensor):\n    \"\"\"\n    Meta tensors give you the ability to run PyTorch code without having to\n    actually do computation through tensors allocated on a `meta` device.\n    Because the device is `meta`, meta tensors do not model device propagation.\n    FakeTensor extends MetaTensors to also carry an additional `fake_device`\n    which tracks devices that would have been used.\n    \"\"\"\n\n    fake_device: torch.device\n    fake_mode: FakeTensorMode\n    constant: Optional[Tensor]\n    real_tensor: Optional[Tensor]\n\n    # TODO: Generalize this as needed, e.g., into a trie of memos, if\n    # you do something like x[0].item()  (x[0] is fresh each time, so\n    # memo mechanism here won't work)\n    nonzero_memo = SymNumberMemoDescriptor()\n    item_memo = SymNumberMemoDescriptor()\n    unique_memo = SymNumberMemoDescriptor()\n    unique_consecutive_memo = SymNumberMemoDescriptor()\n\n    # We expect nested_int_memo to be None when an offsets is a graph\n    # intermediate, or an input that has never been associated with a\n    # nested int.\n    nested_int_memo = SymNumberMemoDescriptor(is_nested_int=True)\n\n    # FakeTensor doesn't fully emulate the original tensor's Python type\n    # and dispatch key set, therefore sometimes we want to track them\n    # separately.\n    pytype: Optional[type[Tensor]]\n    dispatch_keys: Optional[torch.DispatchKeySet]\n\n    # Indicates to our torch_dispatch dispatching infra that\n    # this is an \"infra\" mode with lower dispatching precedence.\n    _mode_key = torch._C._TorchDispatchModeKey.FAKE\n\n    @property\n    def device(self) -> torch.device:\n        if self.fake_mode.in_kernel_invocation:\n            return torch.device(\"meta\")\n        else:\n            return self.fake_device\n\n    @device.setter\n    def device(self, _: torch.device) -> None:\n        raise NotImplementedError\n\n    # Note: [Fake Tensor Dispatch Keys]\n    # In order to model the behavior of device-specific autocast\n    # and autograd logic, we update the dispatch keys of FakeTensors\n    # to reflect their fake device. This includes the BackendComponent\n    # (DispatchKey::Meta -> DispatchKey::CUDA), and also the BackendComponent\n    # related Autocast and Autograd keys. __torch_dispatch__ sits below\n    # Autocast and Autograd, and is only invoked when we are at the\n    # kernel for the BackendComponent. Then, we add Meta to the\n    # thread-local dispatch include set to hit the meta kernel\n    # instead of the kernel of the BackendComponent for the fake device.\n    # The `device_for_backend_keys` does that below\n    # NOTE: this probably will not do the right thing for backends\n    # that have dispatch keys which are higher than the \"meta\" key:\n    # https://github.com/pytorch/pytorch/blob/main/c10/core/DispatchKey.h#L189\n\n    # We don't support named tensors; graph break\n    @property\n    def names(self) -> list[str]:\n        raise UnsupportedFakeTensorException(\n            \"torch.compile doesn't support named tensors\"\n        )\n\n    @names.setter\n    def names(self, _: list[str]) -> None:\n        raise NotImplementedError\n\n    @staticmethod\n    def __new__(\n        cls,\n        fake_mode: FakeTensorMode,\n        elem: Tensor,\n        device: torch.device,\n        constant: Optional[Tensor] = None,\n        real_tensor: Optional[Tensor] = None,\n        pytype: Optional[type[Tensor]] = None,\n        dispatch_keys: Optional[torch.DispatchKeySet] = None,\n    ) -> Self:\n        self = Tensor._make_subclass(\n            cls,\n            elem,\n            elem.requires_grad,\n            dispatch_device=True,\n            device_for_backend_keys=device,\n        )\n        if not fake_mode._allow_unsafe_data_ptr_access:\n            torch._C._set_throw_on_mutable_data_ptr(self)\n        else:\n            torch._C._set_warn_deprecated_on_mutable_data_ptr(self)\n\n        assert elem.device.type == \"meta\", elem.device.type\n        device = device if isinstance(device, torch.device) else torch.device(device)\n        # NB: it is fine, if a little confusing, for device to be meta\n        # (we are faking a meta tensor in that case).  However, it often\n        # indicates some sort of confusion (e.g., you accidentally passed\n        # in a meta tensor when you should have passed in the real tensor).\n        # So by default we disallow meta, and if you are working in a situation\n        # where it is helpful (e.g., crossref testing) you can turn it back\n        # on\n        if not fake_mode.allow_meta:\n            assert device.type != \"meta\"\n        # normalize device.\n        if device.type in [\"cuda\", \"xpu\"]:\n            init_gpu_context(device)\n\n        if (\n            device.type\n            in [\"cuda\", \"hpu\", \"xpu\", \"mps\", torch._C._get_privateuse1_backend_name()]\n            and device.index is None\n        ):\n            if device.type != \"mps\" and getattr(torch, device.type).is_initialized():\n                device = torch.device(\n                    f\"{device.type}:{getattr(torch, device.type).current_device()}\"\n                )\n            else:\n                device = torch.device(f\"{device.type}:0\")\n        self.fake_device = device\n        self.fake_mode = fake_mode\n        self.constant = constant\n        self.pytype = pytype\n        self.dispatch_keys = dispatch_keys\n        assert not isinstance(real_tensor, FakeTensor)\n        self.real_tensor = real_tensor\n        self.nonzero_memo = None\n        self.item_memo = None\n        self.unique_memo = None\n        self.unique_consecutive_memo = None\n        self.nested_int_memo = None\n\n        if FakeTensorConfig.debug:\n            self._debug_trace = CapturedTraceback.extract()  # type: ignore[attr-defined]\n        return self\n\n    # In some circumstances, a conventional Tensor constructor\n    # will get rewritten to call into FakeTensor.  We must provide an\n    # __init__ method that can accept the Python interpreters initialization\n    # in such a situation; we must also be able to handle direct fake\n    # tensor construction via FakeTensor().\n    #\n    # In particular, the __init__ call will look funny in the following case:\n    #\n    #   with FakeTensorMode():\n    #       x = Tensor([1, 2, 3])\n    #\n    # this desugars into:\n    #\n    #   with FakeTensorMode():\n    #       x = Tensor.__new__([1, 2, 3])\n    #       # NB: x is a fake tensor, because of the mode!\n    #       x.__init__([1, 2, 3])  # not the normal fake tensor args!\n    #\n    def __init__(self, *args: object, **kwargs: object) -> None:\n        super().__init__()\n\n    @staticmethod\n    def from_tensor(t: Tensor, fake_mode: FakeTensorMode) -> FakeTensor:\n        return fake_mode.from_tensor(t)\n\n    @classmethod\n    @count\n    def __torch_dispatch__(  # type: ignore[override] # TODO\n        cls,\n        func: OpOverload,\n        types: Sequence[type],\n        args: Sequence[object] = (),\n        kwargs: Mapping[str, object] = immutable_dict(),\n    ) -> object:\n        # need to handle here to avoid infinite recursion\n        # see [in_kernel_invocation]\n        if func == torch.ops.prim.device.default:\n            assert len(args) == 1 and isinstance(args[0], FakeTensor)\n            if args[0].fake_mode.in_kernel_invocation:\n                return torch.device(\"meta\")\n            else:\n                return args[0].fake_device\n\n        # this handler must be done inside FakeTensor subclass, not mode, because\n        # we can end up dispatching here when we have a fake tensor with\n        # symbolic sizes running under in_kernel_invocation_manager.\n        # The subclass is asked to handle this query because size (not\n        # sym_size) was called, but we are unable to serve it directly because\n        # there are symbolic sizes in the class.  The use of\n        # in_kernel_invocation_manager means it's incorrect to activate a\n        # mode to actually handle this (this caused\n        # https://github.com/pytorch/pytorch/issues/122772).\n        if handler := _DISPATCH_META_HANDLERS.get(func):\n            return handler(args)\n\n        # Because fake mode can return NotImplemented (if it sees a subclass\n        # it doesn't know how to deal with), this test here is important\n        # because the next dispatch after a fake mode will attempt to use\n        # subclasses of tensors to dispatch, and any FakeTensor arguments\n        # will be considered eligible.\n        unrecognized_types = [\n            t for t in types if not issubclass(t, FakeTensor) and t is not Tensor\n        ]\n        if unrecognized_types:\n            not_implemented_log.debug(\n                \"FakeTensor unrecognized subclass(es): %s\", unrecognized_types\n            )\n            return NotImplemented\n\n        fake_mode = None\n        for arg in pytree.arg_tree_leaves(*args, **kwargs):\n            if isinstance(arg, FakeTensor):\n                fake_mode = arg.fake_mode\n                break\n\n        assert fake_mode is not None\n\n        # If the fake mode is already active, don't try to reapply it!\n        # NotImplemented is the right thing to return here, because the\n        # typical situation this can occur is if ProxyTensorMode returned a\n        # NotImplemented because of a not implemented subclass; we may have\n        # unluckily attempted to hit FakeTensor's dispatch first,\n        # NotImplemented lets us keep chaining until we find the actual\n        # subclass\n        maybe_cur_fake_mode = torch._C._get_dispatch_mode(\n            torch._C._TorchDispatchModeKey.FAKE\n        )\n        if maybe_cur_fake_mode:\n            not_implemented_log.debug(\n                \"FakeTensor mode already active: %s in %s\",\n                fake_mode,\n                maybe_cur_fake_mode,\n            )\n            return NotImplemented\n\n        assert not fake_mode.in_kernel_invocation\n\n        with fake_mode:\n            return func(*args, **kwargs)\n\n    @staticmethod\n    def _find_common_device(\n        func: OpOverload, flat_args: Sequence[object]\n    ) -> tuple[torch.device, bool]:\n        # Returns: (common_device, has_scalar_only_inputs)\n\n        # cpu - zero-dim tensors can be called in cuda kernels,\n        # so overwrite the common_device if it the only existing\n        # device comes from a cpu zero-dim tensor\n        common_device = None\n        has_scalar_only_inputs = False\n        is_cpu_zero_dim = None\n\n        # list of ops which can have args(tensor/tensorList) in mixed device\n        mixed_device_fns = ordered_set(\n            aten._foreach_copy.default,\n        )\n\n        def check_cpu_device(device: torch.device) -> bool:\n            return device.type == \"cpu\"\n\n        def cpu_zero_dim(t: Tensor) -> bool:\n            return check_cpu_device(t.device) and t.dim() == 0\n\n        def merge_devices(t: object) -> None:\n            nonlocal common_device\n            nonlocal is_cpu_zero_dim\n            if not isinstance(t, FakeTensor):\n                return\n\n            if common_device is None:\n                common_device = t.device\n                is_cpu_zero_dim = cpu_zero_dim(t)\n                return\n\n            t_is_cpu_zero_dim = cpu_zero_dim(t)\n            if t.device == common_device:\n                if is_cpu_zero_dim:\n                    is_cpu_zero_dim = t_is_cpu_zero_dim\n                return\n\n            # mismatching devices !\n            # if current tensor is cpu 0 dim, defer to existing device\n            if t_is_cpu_zero_dim:\n                return\n\n            # current device is from cpu 0 dim tensor, overwrite\n            if is_cpu_zero_dim:\n                common_device = t.device\n                is_cpu_zero_dim = t_is_cpu_zero_dim\n                return\n\n            # if still device mismatches we will check ops which can work\n            # on different devices for ex. _foreach_copy, and one of the\n            # device must be cpu in this case we will return from here without\n            # throwing an error\n            if func in mixed_device_fns:\n                if any(map(check_cpu_device, (common_device, t.device))):\n                    return\n\n            # mismatching devices of non-zero dim tensors, throw\n            # This might be valid behavior and need to be explicitly modeled, e.g. reshape_as\n            raise RuntimeError(\n                f\"Unhandled FakeTensor Device Propagation for {func}, found two different devices {common_device}, {t.device}\"\n            )\n\n        for arg in flat_args:\n            merge_devices(arg)\n\n        # some functions that allow Python numbers to bind to Tensors\n        # if we have failed to find a device, and we're running one of these operators,\n        # we must have scalar only inputs\n        if should_allow_numbers_as_tensors(func) and common_device is None:\n            # ops with scalar only inputs always have result on cpu\n            has_scalar_only_inputs = True\n            common_device = torch.device(\"cpu\")\n\n        assert common_device is not None, f\"Could not find common device for {func}\"\n\n        return common_device, has_scalar_only_inputs\n\n    def get_nested_int(\n        self,\n        *,\n        coeff: Union[int, torch.SymInt] = 1,\n    ) -> torch.SymInt:\n        if self.nested_int_memo is None:\n            self.nested_int_memo = self.fake_mode.create_symbolic_nested_int(\n                nt_tensor_id=None\n            )\n        assert isinstance(self.nested_int_memo, torch.SymInt)\n        return self.nested_int_memo * coeff\n\n    # Similar to FunctionalTensor.tolist\n    def tolist(self) -> Any:\n        if self.dim() == 0:\n            return self.item()\n        elif self.dim() == 1:\n            return [elem.item() for elem in self]\n        else:\n            return [elem.tolist() for elem in self]\n\n\n_MetadataIntLike = Union[IntLikeType, \"_PySymInputStub\", \"_SymIntOutputStub\"]\n\n\n@dataclass_slots\n@dataclass\nclass TensorMetadata:\n    \"\"\"\n    The Tensor metadata relevant to hashing FakeTensors when caching.\n    \"\"\"\n\n    dtype: torch.dtype\n    shape: tuple[_MetadataIntLike, ...]\n    stride: tuple[_MetadataIntLike, ...]\n    device: torch.device\n    layout: torch.layout\n    memory_format: Optional[torch.memory_format]\n    storage_offset: _MetadataIntLike\n    storage_bytes: Optional[_MetadataIntLike]\n    requires_grad: bool\n    is_quantized: bool\n    is_conj: bool\n    is_neg: bool\n    is_inference: bool\n    is_sparse: bool  # read: is sparse COO\n    is_coalesced: Optional[bool]\n    dense_dim: Optional[int]\n    sparse_dim: Optional[int]\n\n    def _flatten_into(\n        self,\n        result: list[object],\n        mode: FakeTensorMode,\n        state: _CacheKeyState,\n    ) -> None:\n        # Flatten the TensorMetadata out into `result`.  Make sure to call\n        # state.convert_sym_int() on any SymInts.\n        for field in dataclasses.fields(self):\n            value = getattr(self, field.name)\n            if isinstance(value, (tuple, list, torch.Size)):\n                # This will recursively flatten the iterable, calling\n                # convert_sym_int() as necessary.\n                id_hashed_objects: list[object] = []\n                mode._prep_args_for_hash(result, value, state, id_hashed_objects)\n                id_hashed_objects.clear()\n            elif isinstance(value, SymInt):\n                state.convert_sym_int(result, value)\n            else:\n                result.append(value)\n\n\ndef extract_tensor_metadata(t: Tensor) -> TensorMetadata:\n    \"\"\"\n    Extract the TensorMetadata of a tensor.\n    \"\"\"\n    memory_format = suggest_memory_format(t)\n    # Don't call is_contiguous() on a Tensor which has symbolic sizes or things\n    # will go badly (guards will be messed up?)\n    if (\n        t._has_symbolic_sizes_strides\n        or is_sparse_any(t)\n        or not t.is_contiguous(memory_format=memory_format)\n    ):\n        memory_format = None  # type: ignore[assignment]\n\n    storage_offset = t.storage_offset()\n\n    return TensorMetadata(\n        t.dtype,\n        t.shape,\n        t.stride() if t.layout == torch.strided else (),\n        t.device,\n        t.layout,\n        memory_format,\n        storage_offset,\n        # Only set storage_bytes for tensors that have storage (not sparse)\n        t.untyped_storage().nbytes() if not is_sparse_any(t) else None,\n        t.requires_grad,\n        t.is_quantized,\n        t.is_conj(),\n        t.is_neg(),\n        t.is_inference(),\n        t.is_sparse,\n        t.is_coalesced() if t.is_sparse else None,\n        t.dense_dim() if is_sparse_any(t) else None,\n        t.sparse_dim() if is_sparse_any(t) else None,\n    )\n\n\n@dataclass_slots\n@dataclass\nclass _DispatchCacheKey:\n    \"\"\"\n    Key for the FakeTensor dispatch cache.\n    \"\"\"\n\n    key: tuple[object, ...]\n    hashvalue: int\n\n    def __init__(self, tup: tuple[object, ...]) -> None:\n        self.key = tup\n        self.hashvalue = hash(tup)\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, _DispatchCacheKey) and self.key == other.key\n\n    def __hash__(self) -> int:\n        return self.hashvalue\n\n    def strip_shape_env(self) -> None:\n        # We need to strip the ShapeEnv from any values before we store in the\n        # cache so the cache doesn't keep our ShapeEnvs alive.\n        for v in self.key:\n            if isinstance(v, _PySymInputStub):\n                v.strip_shape_env()\n\n\n# Default value for constant_value in _DispatchCacheEntryOutputInfo. This is\n# only for checking and differentiates from None.\nclass SingletonConstant:\n    pass\n\n\n@dataclass_slots\n@dataclass(frozen=True)\nclass _DispatchCacheEntryOutputInfo:\n    \"\"\"\n    Entry type for the FakeTensor dispatch cache for an output. Accounts for three\n    possibilities:\n    1) The op is inplace, and a hit means we need to alias the argument at a\n       given index.\n    2) We need to synthesize a new FakeTensor given tensor metadata. For view\n       ops, we further capture the index of the arg to alias.\n    3) if the tensor related fields are None, then it is a constant value (e.g.\n    None or integer)\n    \"\"\"\n\n    inplace_idx: Optional[int]\n    metadata: Optional[TensorMetadata]\n    view_idx: Optional[int]\n    constant_value: Optional[Any] = SingletonConstant\n\n\n@dataclass_slots\n@dataclass(frozen=True)\nclass _DispatchCacheValidEntry:\n    \"\"\"\n    Entry type for the FakeTensor dispatch cache. It supports two types of outputs\n    1) tensor\n    2) tuple of tensors\n\n    is_output_tuple flag helps in differentiating the return type\n    \"\"\"\n\n    output_infos: tuple[_DispatchCacheEntryOutputInfo]\n    is_output_tuple: bool = False\n\n\n@dataclass_slots\n@dataclass(frozen=True)\nclass _DispatchCacheBypassEntry:\n    \"\"\"\n    Entry type for a negative cache entry.\n    \"\"\"\n\n    reason: str\n\n\nif TYPE_CHECKING:\n    _DispatchCacheEntry = Union[_DispatchCacheValidEntry, _DispatchCacheBypassEntry]\n\n\n@dataclass_slots\n@dataclass(frozen=True)\nclass _BypassDispatchCache(Exception):\n    \"\"\"\n    Signals cases that should skip FakeTensor caching.\n    \"\"\"\n\n    reason: str\n\n\n@dataclass_slots\n@dataclass(frozen=True)\nclass DispatchCacheInfo:\n    \"\"\"\n    Information about the state of the FakeTensor dispatch cache.\n    \"\"\"\n\n    hits: int\n    misses: int\n    bypasses: dict[str, int]\n    size: int\n\n\n# We keep one instantiation of `fake_tensor_converter` active\n# for the duration of `with FakeTensorMode()`.\n# This allows accurate storage aliasing across invocation of\n# different operators. While this will keep all freshly allocated\n# tensors alive during `FakeTensorMode`, there will be no\n# new allocations of Tensors which have non-meta storage so\n# memory should not significantly increase.\n\n\nclass FakeTensorMode(TorchDispatchMode):\n    cache: dict[_DispatchCacheKey, _DispatchCacheEntry] = {}\n    cache_hits: int = 0\n    cache_misses: int = 0\n    cache_bypasses: dict[str, int] = defaultdict(int)\n    # Every time you retrace using the same fake tensor mode, you should\n    # advance the epoch so we don't reuse unbacked memos\n    epoch: int = 0\n    in_kernel_invocation: bool = False\n    static_shapes: bool\n    shape_env: Optional[ShapeEnv]\n    _stack: Optional[str]\n    allow_meta: bool\n\n    # NestedTensor uses a tensor_id_counter to uniquely identify offsets.\n    # This counter is incremented when an offsets is used to create an NJT\n    # for the first time. To avoid mutating eager state if we construct NJT\n    # during tracing, we maintain a separate counter on the FakeTensorMode.\n    # The initial count is set to the current eager tensor_id_counter value\n    # upon initialization, and every time you retrace using the same fake tensor\n    # mode, you should reset the counter to the initial count.\n    nt_tensor_id_counter: int = -1\n    nt_tensor_id_initial_count: int = -1\n\n    def __init__(\n        self,\n        *,\n        allow_fallback_kernels: bool = True,\n        allow_non_fake_inputs: bool = False,\n        shape_env: Optional[ShapeEnv] = None,\n        static_shapes: Optional[bool] = None,\n        # TODO: This is a temporary measure, see\n        # https://github.com/pytorch/pytorch/pull/126245#discussion_r1604185748\n        # We're currently solely using this to impede population of\n        # item_memo for 0d scalar tensor inputs when export, because this\n        # causes things that used to be deferred runtime asserts to turn into\n        # guards, and then the guards are just lost.  We can potentially fix\n        # this by ensuring guards also get put in the graph, but this is\n        # pending a rework of how deferred runtime asserts in export.  Once\n        # that's done, we can remove this.\n        export: bool = False,\n    ) -> None:\n        log.debug(\"create_mode 0x%x\", id(self))\n        super().__init__()\n        self.allow_fallback_kernels = allow_fallback_kernels\n\n        import torch._dynamo.config\n        import torch._functorch.config\n\n        self.propagate_real_tensors = (\n            torch._functorch.config.fake_tensor_propagate_real_tensors\n        )\n        self.fake_tensor_converter = FakeTensorConverter(\n            copy_data=self.propagate_real_tensors,\n            export=export,\n        )\n\n        if static_shapes is not None:\n            self.static_shapes = static_shapes\n        else:\n            self.static_shapes = shape_env is None\n\n        # This is temporarily patched to True in Dynamo to grandfather in some\n        # places where we unconditionally allow scalar outputs, TO BE REMOVED\n        self.allow_scalar_outputs = False\n\n        self._allow_unsafe_data_ptr_access = (\n            torch._functorch.config.fake_tensor_allow_unsafe_data_ptr_access\n        )\n        self.allow_meta = torch._functorch.config.fake_tensor_allow_meta\n        self.cache_enabled: bool = (\n            torch._dynamo.config.fake_tensor_cache_enabled\n            and not self.propagate_real_tensors\n        )\n        self.cache_crosscheck_enabled = (\n            torch._dynamo.config.fake_tensor_cache_crosscheck_enabled\n        )\n\n        # A flag that controls, whether we want to invoke ops on mix of\n        # real weights/global variables and fake inputs\n        self.allow_non_fake_inputs = allow_non_fake_inputs\n\n        # [in_kernel_invocation]\n        # when FakeTensor is invoked in user code, .device should return\n        # the fake_device of the tensor so that code such as as `if x.is_cuda`\n        # or torch.zeros([10, 10], device=x.device) continues to execute as if\n        # the FakeTensor were real. However, within kernel execution, we return\n        # the `Meta` device because all computation within the kernels should\n        # behave as if the Tensors are on meta devices. Kernels should allocate\n        # new tensors on meta devices, and checks like `is_meta` should return true.\n        # within python refs, we always return the real device by defining\n        # the device property\n        self.in_kernel_invocation = False\n\n        # True if we enter'ed and actually enabled fake tensor mode,\n        # false if it was a no-op.  Not thread safe but neither is\n        # in_kernel_invocation\n        # If another fake mode was already active when we enter, we also stash it here.\n        # That way when we exit, we know to re-enable the previous fake mode.\n        self.enter_stack: list[\n            tuple[bool, Optional[TorchDispatchMode], Optional[bool]]\n        ] = []\n\n        self.shape_env = shape_env\n\n        self._stack_trace = traceback.extract_stack()\n        self._stack = None\n\n        # Indicates to our torch_dispatch dispatching infra that\n        # this is an \"infra\" mode with lower dispatching precedence.\n        self._mode_key = torch._C._TorchDispatchModeKey.FAKE\n\n        import torch.nested._internal.nested_tensor\n\n        self.nt_tensor_id_initial_count = (\n            torch.nested._internal.nested_tensor._tensor_id_counter\n        )\n        self.nt_tensor_id_counter = self.nt_tensor_id_initial_count\n\n    def reset_nt_tensor_id_counter(self) -> None:\n        self.nt_tensor_id_counter = self.nt_tensor_id_initial_count\n\n    # Typically, there is only one fake tensor mode and you test for it by\n    # doing an isinstance test.  However, in some situations, there might be\n    # TWO fake tensor modes.  The canonical example of this is exporting\n    # a fake model: there is an outer fake mode created by the user, and\n    # an inner fake mode created by Dynamo.  The two phase process is required\n    # because the outer fake mode typically won't have a ShapeEnv, even if\n    # the user is interested in exporting with dynamic shapes (so the inner\n    # fake mode will actually have a ShapeEnv and swap in symbolic sizes.)\n    #\n    # In this case, it's insufficient to test only one FakeTensor: you need\n    # to distinguish between our fake tensor and other fake tensors.  That's\n    # what this function does.\n    def is_our_fake(self, t: object) -> TypeGuard[FakeTensor]:\n        return isinstance(t, FakeTensor) and t.fake_mode is self\n\n    # If we should avoid device init. This changes the behavior of various APIs:\n    # - We avoid constant-prop on Tensors with ops that move them to another device\n    # - We change the torch.tensor ctor contract to never materialize\n    #   tensors on device\n    #   (see NOTE: [torch.tensor, lift_fresh, and device movement])\n    @property\n    def avoid_device_init(self) -> bool:\n        if torch.xpu._is_compiled():\n            assert not torch.cuda._is_compiled()\n            return not torch.xpu.is_available()\n\n        return not (\n            torch.cuda.is_available()\n            or (hasattr(torch, \"hpu\") and torch.hpu.is_available())\n        )\n\n    @property\n    def stack(self) -> str:\n        if self._stack is None:\n            self._stack = \"\".join(traceback.format_list(self._stack_trace))\n        return self._stack\n\n    @count\n    def __torch_dispatch__(\n        self,\n        func: OpOverload,\n        types: Sequence[type],\n        args: Sequence[object] = (),\n        kwargs: Mapping[str, object] = immutable_dict(),\n    ) -> object:\n        # FakeTensorMode should not be set when we're inside of it.\n        assert (\n            torch._C._get_dispatch_mode(torch._C._TorchDispatchModeKey.FAKE) is None\n        ), func\n        try:\n            return self.dispatch(func, types, args, kwargs)\n        except TypeError:\n            log.exception(\"fake tensor raised TypeError\")\n            raise\n\n    # No-op if FakeTensorMode is already in use\n    def __enter__(self) -> Self:\n        import torch.nested._internal.nested_tensor\n\n        prev_only_lift_cpu_tensors = None\n        if self.avoid_device_init:\n            # See NOTE: [torch.tensor, lift_fresh, and device movement]\n            prev_only_lift_cpu_tensors = torch._C._only_lift_cpu_tensors()\n            torch._C._set_only_lift_cpu_tensors(True)\n        maybe_prev_fake_mode = torch._C._unset_dispatch_mode(self._mode_key)\n        if self is not maybe_prev_fake_mode:\n            self.enter_stack.append(\n                (True, maybe_prev_fake_mode, prev_only_lift_cpu_tensors)\n            )\n            return super().__enter__()\n        else:\n            # no-op (still need to re-set the fake mode though since we unset it)\n            torch._C._set_dispatch_mode(self)\n            self.enter_stack.append((False, None, prev_only_lift_cpu_tensors))\n        return self\n\n    def __exit__(\n        self,\n        a: Optional[type[BaseException]],\n        b: Optional[BaseException],\n        c: Optional[TracebackType],\n    ) -> None:\n        (\n            live,\n            maybe_prev_fake_mode,\n            maybe_prev_only_lift_cpu_tensors,\n        ) = self.enter_stack.pop()\n        if live:\n            super().__exit__(a, b, c)\n\n            # Re-enable the previous fake mode, if there was one.\n            if maybe_prev_fake_mode is not None:\n                torch._C._set_dispatch_mode(maybe_prev_fake_mode)\n            if maybe_prev_only_lift_cpu_tensors is not None:\n                torch._C._set_only_lift_cpu_tensors(maybe_prev_only_lift_cpu_tensors)\n\n    @classmethod\n    def is_infra_mode(cls) -> bool:\n        return True\n\n    @classmethod\n    def cache_info(cls) -> DispatchCacheInfo:\n        \"\"\"\n        Query the state of the dispatch cache.\n        \"\"\"\n        return DispatchCacheInfo(\n            FakeTensorMode.cache_hits,\n            FakeTensorMode.cache_misses,\n            dict(FakeTensorMode.cache_bypasses),\n            len(FakeTensorMode.cache),\n        )\n\n    @classmethod\n    def cache_clear(cls) -> None:\n        \"\"\"\n        Clear the dispatch cache.\n        \"\"\"\n        cls.cache_hits = 0\n        cls.cache_misses = 0\n        cls.cache_bypasses.clear()\n        cls.cache.clear()\n\n    def _cached_dispatch_impl(\n        self,\n        func: OpOverload,\n        types: Sequence[type],\n        args: Sequence[object],\n        kwargs: Mapping[str, object],\n    ) -> object:\n        \"\"\"\n        Lookup a cache entry for the given arguments. If none exists, dispatch\n        and cache the result (if the result is eligible for caching).\n        \"\"\"\n        state = None\n        key = None\n        try:\n            state = _CacheKeyState(self.shape_env)\n            key = self._cache_key(state, func, args, kwargs)\n        except _BypassDispatchCache as e:\n            # We couldn't create the cache key at all\n            if (\n                isinstance(func, torch._ops.HigherOrderOperator)\n                and func.name() == \"invoke_subgraph\"\n            ):\n                hc_log.debug(\n                    \"Fake tensor cache failed: identifier = %s, reason = %s\",\n                    args[1],\n                    e.reason,\n                )\n            FakeTensorMode.cache_bypasses[e.reason] += 1\n\n        if key is None:\n            # Do this dispatch outside the above except handler so if it\n            # generates its own exception there won't be a __context__ caused by\n            # the caching mechanism.\n            return self._dispatch_impl(func, types, args, kwargs)\n\n        assert state is not None\n        if state.cache_on_shape_env():\n            assert state.shape_env is not None\n            cache = state.shape_env.fake_tensor_cache\n            set_cache_key = _set_cache_key_for_shape_env\n        else:\n            cache = FakeTensorMode.cache\n            set_cache_key = _set_cache_key\n        entry = cache.get(key, None)\n\n        if entry is not None:\n            if isinstance(entry, _DispatchCacheBypassEntry):\n                # This represents a negative cache entry - we already saw that the\n                # output is uncachable. Compute it from first principals.\n                FakeTensorMode.cache_bypasses[entry.reason] += 1\n                return self._dispatch_impl(func, types, args, kwargs)\n\n            # We have a cache entry.\n            output = self._output_from_cache_entry(state, entry, key, func, args)\n            FakeTensorMode.cache_hits += 1\n            if self.cache_crosscheck_enabled:\n                # For debugging / testing: Validate that the output synthesized\n                # from the cache matches the output created by normal dispatch.\n                with disable_fake_tensor_cache(self):\n                    self._crosscheck_cache_output(output, func, types, args, kwargs)\n            return output\n\n        # We don't have a cache entry.\n        output = self._dispatch_impl(func, types, args, kwargs)\n\n        try:\n            self._validate_cache_key(func, args, kwargs)\n        except _BypassDispatchCache as e:\n            # We ran \"extra\" checks on the cache key and determined that it's no\n            # good. Record the reason and mark it so we don't bother validating\n            # again.\n            if (\n                isinstance(func, torch._ops.HigherOrderOperator)\n                and func.name() == \"invoke_subgraph\"\n            ):\n                hc_log.debug(\n                    \"Fake tensor cache failed: identifier = %s, reason = %s\",\n                    args[1],\n                    e.reason,\n                )\n            FakeTensorMode.cache_bypasses[e.reason] += 1\n            set_cache_key(cache, key, _DispatchCacheBypassEntry(e.reason))\n            return output\n\n        try:\n            entry = self._make_cache_entry(state, key, func, args, kwargs, output)\n        except _BypassDispatchCache as e:\n            # We had trouble making the cache entry. Record the reason and mark\n            # it.\n            FakeTensorMode.cache_bypasses[e.reason] += 1\n            set_cache_key(cache, key, _DispatchCacheBypassEntry(e.reason))\n            return output\n\n        set_cache_key(cache, key, entry)\n        FakeTensorMode.cache_misses += 1\n        return output\n\n    def _cache_key(\n        self,\n        state: _CacheKeyState,\n        func: OpOverload,\n        args: Sequence[object],\n        kwargs: Mapping[str, object],\n    ) -> _DispatchCacheKey:\n        \"\"\"\n        Create a cache key given the dispatch args. Raises _BypassDispatchCache\n        for any situation that precludes caching.\n        \"\"\"\n        key_values = [\n            func,\n            # Capture the default_dtype mode since that can affect the output tensor,\n            # e.g., when operating on constant float values.\n            torch.get_default_dtype(),\n            # Capture the current device to support, e.g., cache tensor creation,\n            # where there isn't necessarily a tensor to take the device from.\n            torch._C._get_default_device(),\n            # We want to create tensors from cached metadata only when the inference\n            # mode is the same.\n            torch.is_inference_mode_enabled(),\n            # Shape env settings could affect behavior. One example seen in the wild:\n            # Disallowing dynamic shapes can introduce a DynamicOutputShapeException\n            # where it wasn't seen on a previous instance of the same op.\n            self.shape_env.settings if self.shape_env else None,\n        ]\n        if state.known_symbols:\n            # If there are symbols then include the epoch - this is really more\n            # of a Shape env var which lives on the FakeTensorMode.\n            key_values.append(self.epoch)\n        # Collect the id_hashed objects to attach a weakref finalize later\n        id_hashed_objects: list[object] = []\n        # Translate any FakeTensor args to metadata.\n        if args:\n            self._prep_args_for_hash(key_values, args, state, id_hashed_objects)\n        if kwargs:\n            self._prep_args_for_hash(key_values, kwargs, state, id_hashed_objects)\n        key = _DispatchCacheKey(tuple(key_values))\n\n        for id_hashed_obj in id_hashed_objects:\n            weakref.finalize(\n                id_hashed_obj, functools.partial(evict_fake_tensor_cache_key, key=key)\n            )\n        id_hashed_objects.clear()\n        return key\n\n    def _validate_cache_key(\n        self,\n        func: OpOverload,\n        args: Sequence[object],\n        kwargs: Mapping[str, object],\n    ) -> None:\n        \"\"\"\n        Validate that the cache key generated by _cache_key will be\n        reasonable.\n        \"\"\"\n        from torch._higher_order_ops.utils import registered_hop_fake_fns\n\n        # For hops, we perform the validity check in _make_cache_entry  because we\n        # need to have the output tensor.\n        if (\n            isinstance(func, torch._ops.HigherOrderOperator)\n            and func in registered_hop_fake_fns\n        ):\n            return\n\n        # Avoid caching for any ops that would require a more sophisticated\n        # caching implementation, e.g., data dependent ops or ops that modify\n        # the inputs.\n        if torch.Tag.data_dependent_output in func.tags:\n            raise _BypassDispatchCache(\"data dependent output\")\n\n        if torch.Tag.dynamic_output_shape in func.tags:\n            if func is aten.index.Tensor:\n                _, new_kwargs = normalize_function(  # type: ignore[misc]\n                    func,\n                    args=args,  # type: ignore[arg-type]\n                    kwargs=kwargs,  # type: ignore[arg-type]\n                    normalize_to_only_use_kwargs=True,\n                )\n                for index in new_kwargs[\"indices\"]:\n                    # index calls nonzero for bool or int8 tensors, and\n                    # therefore has a dynamic shape output. For other dtypes,\n                    # the output shape depends on the input shape (and not data)\n                    if isinstance(index, torch.Tensor) and index.dtype in (\n                        torch.bool,\n                        torch.int8,\n                    ):\n                        raise _BypassDispatchCache(\"dynamic output shape\")\n                return\n\n            raise _BypassDispatchCache(\"dynamic output shape\")\n\n        if torch.Tag.inplace_view in func.tags:\n            raise _BypassDispatchCache(\"inplace view\")\n\n        if func == aten._unsafe_view.default:\n            raise _BypassDispatchCache(\"unsafe view\")\n\n        if func in self.lift_fns:\n            raise _BypassDispatchCache(\"lift\")\n\n        if func.name() == \"inductor::resize_storage_bytes_\":\n            raise _BypassDispatchCache(\"inductor::resize_storage_bytes_\")\n\n        if not torch._library.utils.is_builtin(func):\n            raise _BypassDispatchCache(\"non-builtin\")\n\n        # In order to handle storage aliasing, we need to establish the alias\n        # for any view op on a cache hit. But CompositeImplicitAutograd ops may\n        # or may not alias the input, so just punt on caching these.\n        if func.is_view and torch._C._dispatch_has_kernel_for_dispatch_key(\n            func.name(), torch._C.DispatchKey.CompositeImplicitAutograd\n        ):\n            raise _BypassDispatchCache(\"CompositeImplicitAutograd\")\n\n    def _prep_args_for_hash(\n        self,\n        result: list[object],\n        args: Union[Mapping[str, object], Sequence[object], Iterable[object]],\n        state: _CacheKeyState,\n        id_hashed_objects: list[object],\n    ) -> None:\n        \"\"\"\n        Translate the provided args into a form suitable for caching at FakeTensor\n        dispatch, i.e., convert unhashable types like lists & dicts into tuples and\n        convert FakeTensors into metadata. Raises _BypassDispatchCache to signal\n        unsupported cases that should bypass caching.\n        \"\"\"\n        from torch._higher_order_ops.auto_functionalize import (\n            FunctionalCallableWithEpilogue,\n        )\n        from torch._higher_order_ops.utils import FunctionalizeCtxWrapper\n\n        if isinstance(args, dict):\n            self._prep_args_for_hash(result, args.keys(), state, id_hashed_objects)\n            self._prep_args_for_hash(result, args.values(), state, id_hashed_objects)\n            return\n\n        for arg in args:\n            if isinstance(arg, FakeTensor):\n                if not self.is_our_fake(arg):\n                    raise _BypassDispatchCache(\"not our fake\")\n                if arg.constant is not None:\n                    raise _BypassDispatchCache(\"constant attribute\")\n                if is_sparse_any(arg):\n                    raise _BypassDispatchCache(f\"{arg.layout} tensor\")\n                metadata = extract_tensor_metadata(arg)\n                metadata._flatten_into(result, self, state)\n            elif isinstance(arg, Tensor):\n                raise _BypassDispatchCache(\"non-fake tensor\")\n            elif isinstance(arg, SymInt):\n                state.convert_sym_int(result, arg)\n            elif isinstance(arg, (SymBool, SymFloat)):\n                raise _BypassDispatchCache(\"symbolic shape\")\n            elif isinstance(arg, (list, tuple, dict)):\n                self._prep_args_for_hash(result, arg, state, id_hashed_objects)\n            elif isinstance(arg, types.FunctionType):\n                raise _BypassDispatchCache(\"function argument\")\n            elif isinstance(arg, torch.fx.GraphModule):\n                # This is used for invoke_subgraph where id(graph_module) allows\n                # us to cache fake outputs\n                result.append(type(arg))\n                result.append(id(arg))\n                id_hashed_objects.append(arg)\n            elif isinstance(arg, FunctionalizeCtxWrapper):\n                # Special case for AOT Dispatcher first pass, where the fake\n                # tensor is called on the functional wrapper of the subgraph.\n                result.append(hash(arg))\n                # functional wrapper is destroyed after fake tensor prop. We\n                # need to put the finalizer on the subgraph.\n                id_hashed_objects.append(arg.subgraph)\n            elif isinstance(arg, FunctionalCallableWithEpilogue):\n                result.append(type(arg))\n                result.append(hash(arg))\n                id_hashed_objects.append(arg.orig_callable)\n            else:\n                # It's important to capture the type of the arg since, e.g., 1 and 1.0\n                # hash to the same value, but can produce different dtypes for the\n                # output tensor.\n                result.append(type(arg))\n                result.append(arg)\n\n    def _validate_output_for_cache_entry(\n        self,\n        state: _CacheKeyState,\n        key: _DispatchCacheKey,\n        func: OpOverload,\n        args: Sequence[object],\n        kwargs: Mapping[str, object],\n        output: Optional[FakeTensor],\n    ) -> None:\n        # Is this even possible? According to the signature this can be None but\n        # not `int`. So either the signature is a lie or (part of) this line is\n        # unnecessary...\n        if isinstance(output, (int, type(None))):\n            return\n\n        if _has_unrepresented_symbols(state, output):\n            # Unbacked symbols are fine - but only if they're also represented\n            # in the input. If there are any new unbacked symbols then we can't\n            # cache this output.\n            raise _BypassDispatchCache(\"unrepresented symbol in output\")\n\n        # Some ops return tuples of Tensors, but it's rare, so avoid\n        # the complexity of caching other types.\n        if not isinstance(output, FakeTensor):\n            raise _BypassDispatchCache(\"non-FakeTensor output\")\n\n        # Avoid caching FakeTensors with constants attached since those\n        # can be invalidated.\n        if output.constant is not None:\n            raise _BypassDispatchCache(\"constant attribute\")\n\n        # TODO: support caching sparse outputs?\n        if output.is_sparse:\n            raise _BypassDispatchCache(\"sparse output\")\n\n        if is_sparse_compressed(output):\n            raise _BypassDispatchCache(\"sparse compressed output\")\n\n        # Can an in-place op really reference a kwarg? If so, then we need\n        # to extend the implementation to handle it.\n        for kval in kwargs.values():\n            if id(kval) == id(output):\n                raise _BypassDispatchCache(\"kwarg aliases output\")\n\n    def _get_output_info_for_cache_entry(\n        self,\n        state: _CacheKeyState,\n        key: _DispatchCacheKey,\n        func: OpOverload,\n        args: Sequence[object],\n        kwargs: Mapping[str, object],\n        output: FakeTensor,\n    ) -> _DispatchCacheEntryOutputInfo:\n        if isinstance(output, (int, torch.SymInt, type(None))):\n            return _DispatchCacheEntryOutputInfo(\n                inplace_idx=None, metadata=None, view_idx=None, constant_value=output\n            )\n\n        # If this is an in-place op, the entry records which input arg is aliased.\n        for idx in range(len(args)):\n            if id(args[idx]) == id(output):\n                return _DispatchCacheEntryOutputInfo(\n                    inplace_idx=idx, metadata=None, view_idx=None\n                )\n\n        # Otherwise, create an entry that records the output tensor's metadata.\n        view_idx = None\n        if isinstance(func, torch._ops.OpOverload) and func.is_view:\n            idxs = [i for i, t in enumerate(args) if isinstance(t, Tensor)]\n            assert len(idxs) == 1\n            view_idx = idxs[0]\n\n        metadata = extract_tensor_metadata(output)\n        metadata.shape = tuple(state.convert_output(v) for v in metadata.shape)\n        metadata.stride = tuple(state.convert_output(v) for v in metadata.stride)\n        metadata.storage_offset = state.convert_output(metadata.storage_offset)\n        metadata.storage_bytes = (\n            None\n            if metadata.storage_bytes is None\n            else state.convert_output(metadata.storage_bytes)\n        )\n\n        entry = _DispatchCacheEntryOutputInfo(\n            inplace_idx=None,\n            metadata=metadata,\n            view_idx=view_idx,\n        )\n\n        # N.B.: Some checks for bypassing the cache would be performed on the\n        # output tensor synthesized from the cached metadata. As an optimization,\n        # we can synthesize a tensor here and do the checks on that instance.\n        # This approach keeps the (more frequent) cache-hit path as lightweight\n        # as possible.\n        entry_for_synth_output = _DispatchCacheValidEntry(\n            output_infos=(entry,), is_output_tuple=False\n        )\n        from torch.fx.experimental.symbolic_shapes import GuardOnDataDependentSymNode\n\n        try:\n            synth_output = self._output_from_cache_entry(\n                state, entry_for_synth_output, key, func, args\n            )\n        except GuardOnDataDependentSymNode:\n            # This should probably never really happen. If it does it means that\n            # although the original call didn't get a data-dependent error when\n            # we tried to reconstruct the output we did - that's almost\n            # certainly a bug.\n            raise _BypassDispatchCache(\"data dependent symnode\") from None\n\n        # Make sure the dispatch_key_set from the synthesized output tensor will\n        # be the same.\n        synth_key_set = torch._C._dispatch_key_set(synth_output)\n        key_set = torch._C._dispatch_key_set(output)\n        if synth_key_set != key_set:\n            raise _BypassDispatchCache(\"dispatch_key_set mismatch\")\n\n        return entry\n\n    def _make_cache_entry(\n        self,\n        state: _CacheKeyState,\n        key: _DispatchCacheKey,\n        func: OpOverload,\n        args: Sequence[object],\n        kwargs: Mapping[str, object],\n        output: Optional[FakeTensor],\n    ) -> _DispatchCacheValidEntry:\n        \"\"\"\n        Make a cache entry object for the given 'output' Tensor. Raises\n        _BypassDispatchCache if the output tensor has characteristics that\n        prevent caching it.\n        \"\"\"\n        from torch._higher_order_ops.utils import registered_hop_fake_fns\n        from torch.fx.experimental.symbolic_shapes import has_free_unbacked_symbols\n\n        # For hops, lets look at the output tensor to find any unbacked symints.\n        # If there are none, then we rely on the existing checks to validate\n        # caching.\n        # NB: Note that the HOPs that sta alive till FakeTensor are functional,\n        # once they support mutations, we will have to revisit this logic.\n        if (\n            isinstance(func, torch._ops.HigherOrderOperator)\n            and func in registered_hop_fake_fns\n        ):\n            assert isinstance(output, tuple)\n            non_cacheable = any(\n                isinstance(o, (torch.Tensor, torch.SymInt))\n                and has_free_unbacked_symbols(o)\n                for o in output\n            )\n            if non_cacheable:\n                raise _BypassDispatchCache(f\"unbacked symbol in HOP {func} output\")\n\n        if isinstance(output, (int, torch.SymInt, type(None))):\n            output_info = _DispatchCacheEntryOutputInfo(\n                inplace_idx=None, metadata=None, view_idx=None, constant_value=output\n            )\n            return _DispatchCacheValidEntry(\n                output_infos=(output_info,), is_output_tuple=False\n            )\n\n        if isinstance(output, tuple):\n            for out_element in output:\n                self._validate_output_for_cache_entry(\n                    state, key, func, args, kwargs, out_element\n                )\n        else:\n            self._validate_output_for_cache_entry(\n                state, key, func, args, kwargs, output\n            )\n\n        if isinstance(output, tuple):\n            output_infos = [\n                self._get_output_info_for_cache_entry(\n                    state, key, func, args, kwargs, out_elem\n                )\n                for out_elem in output\n            ]\n            return _DispatchCacheValidEntry(\n                output_infos=tuple(output_infos), is_output_tuple=True\n            )\n\n        else:\n            output_info = self._get_output_info_for_cache_entry(\n                state, key, func, args, kwargs, output\n            )\n            return _DispatchCacheValidEntry(\n                output_infos=(output_info,), is_output_tuple=False\n            )\n\n    def _get_output_tensor_from_cache_entry(\n        self,\n        state: _CacheKeyState,\n        entry: _DispatchCacheEntryOutputInfo,\n        key: _DispatchCacheKey,\n        func: OpOverload,\n        args: Sequence[object],\n    ) -> Optional[FakeTensor]:\n        if (\n            entry.inplace_idx is None\n            and entry.metadata is None\n            and entry.view_idx is None\n        ):\n            assert entry.constant_value is not SingletonConstant\n            return entry.constant_value\n        if entry.inplace_idx is not None:\n            # This is an in-place op; return the aliased arg.\n            inplace_arg = args[entry.inplace_idx]\n            assert isinstance(inplace_arg, FakeTensor)\n            return inplace_arg\n\n        # Synthesize a new FakeTensor with the cached metadata.\n        metadata = entry.metadata\n        if metadata is None:\n            return None\n\n        assert not is_sparse_any(metadata)\n\n        def check_value(\n            value: _MetadataIntLike, state: _CacheKeyState\n        ) -> Union[IntLikeType]:\n            if isinstance(value, _SymIntOutputStub):\n                assert state.shape_env is not None\n                return value.extract(key, state.shape_env)\n            else:\n                assert not isinstance(value, _PySymInputStub)\n                return value\n\n        shape = tuple(check_value(v, state) for v in metadata.shape)\n        stride = tuple(check_value(v, state) for v in metadata.stride)\n        storage_offset = check_value(metadata.storage_offset, state)\n        if metadata.storage_bytes is not None:\n            check_value(metadata.storage_bytes, state)\n\n        maybe_suppress: Callable[[], typing.ContextManager] = contextlib.nullcontext\n        if self.shape_env is not None:\n            maybe_suppress = self.shape_env.suppress_guards\n\n        with in_kernel_invocation_manager(self), maybe_suppress():\n            empty = torch.empty_strided(\n                shape,\n                stride,\n                dtype=metadata.dtype,\n                layout=metadata.layout,\n                device=\"meta\",\n                requires_grad=metadata.requires_grad,\n            )\n\n        if metadata.is_conj:\n            torch._C._set_conj(empty, True)\n        if metadata.is_neg:\n            torch._C._set_neg(empty, True)\n\n        if isinstance(func, torch._ops.OpOverload) and func.is_view:\n            # For view ops, the storage should be the same as the tensor input.\n            view_arg = args[cast(int, entry.view_idx)]\n            assert isinstance(view_arg, FakeTensor)\n            storage = view_arg.untyped_storage()\n            with in_kernel_invocation_manager(self), maybe_suppress():\n                empty.set_(storage, storage_offset, shape, stride)\n\n        return FakeTensor(self, empty, metadata.device)\n\n    def _output_from_cache_entry(\n        self,\n        state: _CacheKeyState,\n        entry: _DispatchCacheValidEntry,\n        key: _DispatchCacheKey,\n        func: OpOverload,\n        args: Sequence[object],\n    ) -> Union[Optional[FakeTensor], tuple[Optional[FakeTensor], ...]]:\n        \"\"\"\n        Create a new FakeTensor from the cache entry.\n        \"\"\"\n\n        if entry.is_output_tuple:\n            outputs = [\n                self._get_output_tensor_from_cache_entry(\n                    state, output_info, key, func, args\n                )\n                for output_info in entry.output_infos\n            ]\n            return tuple(outputs)\n        else:\n            return self._get_output_tensor_from_cache_entry(\n                state, entry.output_infos[0], key, func, args\n            )\n\n    def _crosscheck_cache_output(\n        self,\n        output: Union[Optional[FakeTensor], tuple[Optional[FakeTensor], ...]],\n        func: OpOverload,\n        types: Sequence[type],\n        args: Sequence[object],\n        kwargs: Mapping[str, object],\n    ) -> None:\n        \"\"\"\n        Helper to validate that the output synthesized from the cache matches\n        the output created by normal dispatch.\n        \"\"\"\n\n        def assert_helper(a: Any, b: Any) -> None:\n            if isinstance(a, tuple):\n                assert isinstance(b, tuple)\n                assert len(a) == len(b)\n                for l, r in zip(a, b):\n                    assert_helper(l, r)\n            elif isinstance(a, int):\n                assert isinstance(b, int) and a == b\n            elif a is None:\n                assert b is None\n            elif isinstance(a, py_sym_types):\n                assert type(a) == type(b) and a.node is b.node\n            elif isinstance(a, torch.Tensor):\n                assert isinstance(b, torch.Tensor)\n                assert_metadata_eq(assert_eq, a, b)\n            else:\n                raise RuntimeError(f\"Unsupported type {type(a)}\")\n\n        try:\n            true_output = self._dispatch_impl(func, types, args, kwargs)\n        except Exception as e:\n            raise RuntimeError(\n                f\"FakeTensor cache crosscheck failure: func={func}, \"\n                f\"args={args}, kwargs={kwargs}: Dispatch raised={e}\"\n            ) from e\n        try:\n            assert_helper(true_output, output)\n        except Exception as e:\n            raise RuntimeError(\n                f\"FakeTensor cache crosscheck failure: func={func}, \"\n                f\"args={args}, kwargs={kwargs}\"\n            ) from e\n\n    def dispatch(\n        self,\n        func: OpOverload,\n        types: Sequence[type],\n        args: Sequence[object] = (),\n        kwargs: Mapping[str, object] = immutable_dict(),\n    ) -> object:\n        kwargs = kwargs or {}\n        with no_dispatch():\n            log.debug(\"%s %s %s\", func, args, kwargs)\n\n        if func in _DISPATCH_META_HANDLERS:\n            return _DISPATCH_META_HANDLERS[func](args)\n\n        if log.getEffectiveLevel() <= logging.DEBUG:\n            log.debug(\n                \"%sFakeTensorMode.__torch_dispatch__: %s\", \" \" * RECURSION_COUNT, func\n            )\n            # NOTE: incr is intentionally unused for a RAII pattern\n            incr = IncrementRecursionCount()  # noqa: F841\n\n        # Some attribute queries that can be serviced directly\n        # See Note [is_coalesced is dispatched]\n        if func in _DISPATCH_HANDLE_DIRECTLY:\n            # NB: no_dispatch is ok here too, this func is very simple\n            with in_kernel_invocation_manager(self):\n                return func(*args, **kwargs)\n\n        if self.cache_enabled:\n            return self._cached_dispatch_impl(func, types, args, kwargs)\n        else:\n            return self._dispatch_impl(func, types, args, kwargs)\n\n    def _maybe_infer_fake(\n        self, func: OpOverload, path: KeyPath, fake: object, real: object\n    ) -> tuple[Optional[object], bool]:\n        \"\"\"\n        Helper to cross-check fake/real output properties & values,\n        and create new fake vals if mismatched.\n        Returns tuple of object & boolean, for whether or not it was overwrriten\n        \"\"\"\n        import sympy\n\n        from torch._subclasses.fake_utils import _check_fake_real_tensors\n\n        def _check_fake_real_vals(fake: Any, real: Any) -> None:\n            # use real values + ShapeEnv to check mismatches between potentially symbolic values\n            if isinstance(fake, (SymInt, SymFloat)):\n                # symbolic expression, ask ShapeEnv to substitute known backed/unbacked values\n                assert self.shape_env is not None\n                if (\n                    not fake.node.expr.free_symbols\n                    - self.shape_env.var_to_val.keys()\n                    - self.shape_env.unbacked_var_to_val.keys()\n                ):\n                    if (\n                        self.shape_env._maybe_evaluate_static(\n                            sympy.Eq(fake.node.expr, real), compute_hint=True\n                        )\n                        is not sympy.S.true\n                    ):\n                        raise MetadataMismatchError(\n                            f\"mismatch between fake value {fake} and real value {real} \"\n                        )\n            elif isinstance(\n                fake, (int, float, bool)\n            ):  # concrete value, check direct equality\n                if fake != real:\n                    raise MetadataMismatchError(\n                        f\"mismatch between fake value {fake} and real value {real} \"\n                    )\n\n        if isinstance(fake, torch.Tensor):\n            try:\n                _check_fake_real_tensors(\n                    real,  # type: ignore[arg-type]\n                    fake,  # type: ignore[arg-type]\n                    context=\"Real tensor propagation found\",\n                    sizes=False,  # manual check below\n                    strides=False,  # skip strides\n                    storage_offset=True,\n                    requires_grad=False,  # issues with FakeTensorConverter preserving requires_grad\n                )\n            except MetadataMismatchError as exc:\n                if torch._functorch.config.generate_fake_kernels_from_real_mismatches:\n                    dtrace_structured(\n                        \"mismatched_fake_kernel\",\n                        metadata_fn=lambda: {\n                            \"op\": str(func),\n                            \"reason\": exc.reason,  # noqa: F821\n                        },\n                    )\n                    return _infer_fake_from_real_tensor(self, func, real), True  # type: ignore[arg-type]\n                raise MetadataMismatchError(\n                    f\"Real tensor propagation found a metadata mismatch between \"\n                    f\"fake tensor {fake} and real tensor {real}, \"\n                    f\" at output{keystr(path)}, for func: {func}\"\n                ) from exc\n\n            for j, (s_fake, s_real) in enumerate(zip(fake.size(), real.size())):  # type: ignore[attr-defined]\n                try:\n                    _check_fake_real_vals(s_fake, s_real)\n                except MetadataMismatchError as exc:\n                    if torch._functorch.config.generate_fake_kernels_from_real_mismatches:\n                        dtrace_structured(\n                            \"mismatched_fake_kernel\",\n                            metadata_fn=lambda: {\n                                \"op\": str(func),\n                                \"reason\": exc.reason,  # noqa: F821\n                            },\n                        )\n                        return _infer_fake_from_real_tensor(self, func, real), True  # type: ignore[arg-type]\n                    raise MetadataMismatchError(\n                        f\"Real tensor propagation found an output size mismatch between \"\n                        f\"fake shape {s_fake} and real shape {s_real}, \"\n                        f\"at output{keystr(path)}.size({j}), for func: {func}\"\n                    ) from exc\n        elif fake is None and real is not None:\n            if torch._functorch.config.generate_fake_kernels_from_real_mismatches:\n                dtrace_structured(\n                    \"mismatched_fake_kernel\",\n                    metadata_fn=lambda: {\n                        \"op\": str(func),\n                        \"reason\": f\"mismatch between fake value {fake} and real value {real}\",  # noqa: F821\n                    },\n                )\n                return _infer_fake_from_real_tensor(self, func, real), True  # type: ignore[arg-type]\n            raise MetadataMismatchError(\n                f\"Real tensor propagation found a metadata mismatch between \"\n                f\"fake tensor {fake} and real tensor {real}, \"\n                f\" at output{keystr(path)}, for func: {func}\"\n            )\n        else:\n            try:\n                _check_fake_real_vals(fake, real)\n            except MetadataMismatchError as exc:\n                raise MetadataMismatchError(\n                    f\"Real tensor propagation found an output value mismatch between \"\n                    f\"fake output value {fake} and real output value {real}, \"\n                    f\"at output{keystr(path)}, for func: {func}\"\n                ) from exc\n        return fake, False\n\n    def _maybe_infer_fake_kernel_from_pytree_out(\n        self,\n        func: OpOverload,\n        fake_in: object,\n        real_in: object,\n        fake_out: object,\n        real_out: object,\n    ) -> Optional[object]:\n        \"\"\"\n        Helper to cross-check fake/real output properties & values,\n        and create new fake vals if mismatched, but at the kernel level.\n        Means this handles pytree outputs & checks aliasing.\n        \"\"\"\n        from torch._subclasses.fake_utils import _check_alias_info\n\n        # we might have to clear pending unbacked symbols, if we override the kernel\n        pending_unbacked = None\n        if self.shape_env:\n            pending_unbacked = list(self.shape_env.pending_fresh_unbacked_symbols)\n\n        def _clear_pending_unbacked() -> None:\n            self.shape_env.pending_fresh_unbacked_symbols = list(  # type: ignore[union-attr]\n                set(self.shape_env.pending_fresh_unbacked_symbols).difference(  # type: ignore[union-attr]\n                    pending_unbacked  # type: ignore[arg-type]\n                )\n            )\n\n        fake_paths_leaves, fake_spec = pytree.tree_flatten_with_path(fake_out)\n        real_leaves, _ = pytree.tree_flatten(real_out)\n        try:\n            # catch aliasing mismatches between fake/real tensors\n            _check_alias_info(\n                \"Real tensor propagation found\", real_out, real_in, fake_out, fake_in\n            )\n        except MetadataMismatchError as exc:\n            # if mismatch found, optionally infer fake kernel\n            if torch._functorch.config.generate_fake_kernels_from_real_mismatches:\n                dtrace_structured(\n                    \"mismatched_fake_kernel\",\n                    metadata_fn=lambda: {\n                        \"op\": str(func),\n                        \"reason\": (\n                            f\"Mismatched aliasing spec between fake kernel and real kernel: {exc.reason}\"  # noqa: F821\n                        ),\n                    },\n                )\n                # if aliasing mismatches are found, it's likely that the fake tensor impl\n                # is incorrectly aliasing, since we don't support aliasing custom ops.\n                # in this case we can default to inferring non-aliasing fake kernels from the real outputs.\n                _clear_pending_unbacked()\n                return tree_map(\n                    lambda x: _infer_fake_from_real_tensor(self, func, x), real_out\n                )\n            else:\n                raise MetadataMismatchError(\n                    f\"Real tensor propagation found an aliasing mismatch between \"\n                    f\"fake output {fake_out} and real output {real_out}, \"\n                    f\" for func: {func}\"\n                ) from exc\n\n        # if no errors raised, run cross checks on fake/real tensors,\n        # optionally overriding individual fake tensors, if individual meta kernel output is incorrect.\n        fake_leaves, overrides = zip(\n            *[\n                self._maybe_infer_fake(func, _fake_path, _fake_out, _real_out)\n                for (_fake_path, _fake_out), _real_out in zip(\n                    fake_paths_leaves, real_leaves\n                )\n            ]\n        )\n        if (\n            any(overrides) and pending_unbacked\n        ):  # only keep new pending unbacked symbols\n            _clear_pending_unbacked()\n        return pytree.tree_unflatten(fake_leaves, fake_spec)\n\n    def _dispatch_impl(\n        self,\n        func: OpOverload,\n        types: Sequence[type],\n        args: Sequence[object],\n        kwargs: Mapping[str, object],\n    ) -> Optional[FakeTensor]:\n        from torch._higher_order_ops.utils import registered_hop_fake_fns\n\n        flat_args, args_spec = pytree.tree_flatten((args, kwargs))\n\n        # DO NOT PUT LOGIC BEFORE UNRECOGNIZED TYPE CHECKING\n        # We must throw NotImplemented in case of unrecognized types to handle subclasses.\n        # Throwing the exception will pass the control to the next __torch_dispatch__.\n        # See [subclass inputs] below\n        # NB: If you're seeing a mysterious infinite loop involving fake\n        # tensor, it might be related to this line.  Though I'm not sure\n        # how you'll know to read this comment, as this line won't show up\n        # in the stack trace.\n        has_unrecognized_types = _check_for_subclass(flat_args)\n        if has_unrecognized_types:\n            unrecognized_types = [\n                type(x) for x in flat_args if _check_for_subclass_arg(x)\n            ]\n            not_implemented_log.debug(\n                \"FakeTensorMode unrecognized subclass(es): %s\", unrecognized_types\n            )\n            return NotImplemented\n\n        flat_arg_fake_tensors = [t for t in flat_args if self.is_our_fake(t)]\n        has_symbolic_sizes = any(\n            i._has_symbolic_sizes_strides for i in flat_arg_fake_tensors\n        ) or any(isinstance(a, SymInt) for a in flat_args)\n\n        converter = self.fake_tensor_converter\n\n        is_lift_func = func in self.lift_fns\n        device_conversion_skip_const_prop = (\n            func is torch.ops.aten._to_copy.default\n            and isinstance(args[0], torch.Tensor)\n            and args[0].device.type == \"meta\"\n        )\n\n        # To constant propagate through these functions:\n        # 1, If this is a lift due to a torch.tensor call,\n        #    the input tensor is guaranteed to be a\n        #    constant, so we keep a copy of the original argument along so\n        #    we can query it if we're asked to item() it at some later point.\n        #    (Note that you can always call a lift fn manually, so we do\n        #    have to check if there are any fake tensors!)\n        # 2, Some functions that allow Python numbers to bind to Tensors, e.g, torch.div\n        if (is_lift_func and not flat_arg_fake_tensors) or (\n            should_allow_numbers_as_tensors(func)\n            and not has_symbolic_sizes\n            and not flat_arg_fake_tensors\n            and not device_conversion_skip_const_prop\n        ):\n            assert all(t.constant is not None for t in flat_arg_fake_tensors), (\n                f\"{func} should not have fake inputs without constants\"\n            )\n            const_flat_args = [\n                a.constant if self.is_our_fake(a) else a for a in flat_args\n            ]\n            const_args, const_kwargs = pytree.tree_unflatten(const_flat_args, args_spec)\n            out = func(*const_args, **const_kwargs)\n            if type(out) is Tensor and self.may_turn_const(out):\n                # NB: not in_kernel_invocation_manager because we're doing real\n                # compute here\n                # NB: no_dispatch() here is VERY DANGEROUS (like, segfault\n                # dangerous) if this is actually a wrapper subclass tensor,\n                # therefore the exact type test above\n                with no_dispatch():\n                    out = out.clone()\n                return converter.from_real_tensor(self, out, make_constant=True)\n\n        # if we are in the dispatch mode, we will enter this function even if the inputs\n        # are not FakeTensors. For now, throw if any non-Fake Tensor inputs\n        # and just support constructors.\n\n        # this is generated from torch.tensor(), which does not use the\n        # dispatcher, to allow wrapper subclasses to wrap the new tensor\n        if is_lift_func:\n            assert len(kwargs) == 0 and len(args) == 1, f\"{args} {kwargs}\"\n\n            if type(args[0]) is Tensor:\n                return converter.from_real_tensor(self, args[0])\n\n        # If we are trying to avoid device init, then we need to avoid constant\n        # prop on constant tensors for ops that change devices.\n        avoiding_device_init = False\n        if self.avoid_device_init:\n            if (\n                func == torch.ops.aten._to_copy.default\n                and \"device\" in kwargs\n                and kwargs[\"device\"] != \"cpu\"\n            ):\n                avoiding_device_init = True\n            if func == torch.ops.prims.device_put.default:\n                avoiding_device_init = True\n\n        # Recompute flat_arg_fake_tensors here again in case some of the inputs\n        # were real tensors and fakified in validate_and_convert_non_fake_tensors\n        (flat_args, flat_arg_fake_tensors) = self.validate_and_convert_non_fake_tensors(\n            func, converter, flat_args, args_spec\n        )\n        del args, kwargs  # Invalidated\n\n        # The current constant handling only support tracing systems\n        # (aot autograd, torchdynamo) where each operation is run consecutively.\n        # Because each operation is run in order, we can trace out and support\n        # sequences like: x = torch.tensor(0.); y = x.add_(1)\n        # Whenever a constant is written to but with inputs that cannot be evaluated\n        # statically, such as random_(), we invalidate all constants that alias the input\n        # We will rely on functionalization for use of fake tensors constants as persistent\n        # objects on an FX Graph.\n\n        # We dispatch size/stride/numel on the FakeTensor not its constant, so bail on inplace_view\n        all_constant = all(e.constant is not None for e in flat_arg_fake_tensors)\n        if (\n            isinstance(func, torch._ops.OpOverload)\n            and torch.Tag.nondeterministic_seeded not in func.tags\n            and torch.Tag.inplace_view not in func.tags\n            and all_constant\n            and len(flat_arg_fake_tensors) != 0\n            and not has_symbolic_sizes\n            and not avoiding_device_init\n            and func is not aten._nested_tensor_from_tensor_list.default\n        ):\n            const_flat_args = [\n                a.constant if self.is_our_fake(a) else a for a in flat_args\n            ]\n            const_args, const_kwargs = pytree.tree_unflatten(const_flat_args, args_spec)\n\n            # NB: not in_kernel_invocation_manager(self) as we want to do REAL\n            # compute\n            with no_dispatch():\n                out = func(*const_args, **const_kwargs)\n\n            flat_out = pytree.tree_leaves(out)\n            flat_out_tensors = [t for t in flat_out if isinstance(t, Tensor)]\n            all_constant = all(self.may_turn_const(t) for t in flat_out_tensors)\n\n            if all_constant:\n                return pytree.tree_map_only(\n                    Tensor,\n                    lambda t: converter.from_real_tensor(self, t, make_constant=True),\n                    out,\n                )\n\n            # we weren't able to turn outputs to constants,\n            # so invalidate all constants that might be aliases of the outputs\n            for ten in flat_out_tensors:\n                converter.invalidate_constant_aliases(ten)\n\n        # we are falling through to running non constant tensors, any input constant that\n        # is written to must be invalidated\n        args, kwargs = pytree.tree_unflatten(flat_args, args_spec)\n\n        if (\n            isinstance(func, torch._ops.HigherOrderOperator)\n            and func in registered_hop_fake_fns\n        ):\n            # Reenable the fake tensor mode for the registered fake function\n            maybe_ignore_fresh_unbacked_symbols = (\n                contextlib.nullcontext\n                if self.shape_env is None\n                else self.shape_env.ignore_fresh_unbacked_symbols\n            )\n\n            with self, maybe_ignore_fresh_unbacked_symbols():\n                return registered_hop_fake_fns[func](*args, **kwargs)\n\n        self.invalidate_written_to_constants(func, flat_arg_fake_tensors, args, kwargs)\n\n        def maybe_to_real_tensor(\n            t: T,\n        ) -> Optional[Union[T, Tensor, torch._C.ScriptObject]]:\n            if isinstance(t, FakeTensor):\n                return t.real_tensor\n            elif isinstance(t, py_sym_types):\n                assert self.shape_env is not None\n                return t.node.pytype(\n                    t.node.expr.xreplace(self.shape_env.var_to_val).xreplace(\n                        self.shape_env.unbacked_var_to_val\n                    )\n                )\n            elif isinstance(t, FakeScriptObject):\n                return t.real_obj\n            else:\n                return t\n\n        from torch.fx.experimental.symbolic_shapes import (\n            compute_unbacked_bindings,\n            free_unbacked_symbols,\n        )\n\n        nil = object()\n\n        real_out = nil\n        if (\n            self.propagate_real_tensors\n            and all(e.real_tensor is not None for e in flat_arg_fake_tensors)\n            and not any(\n                (\n                    isinstance(a, py_sym_types)\n                    and (syms := free_unbacked_symbols(a))\n                    and self.shape_env is not None\n                    and any(s not in self.shape_env.unbacked_var_to_val for s in syms)\n                )\n                for a in flat_args\n            )\n        ):\n            log.debug(\"propagate_real_tensors %s\", func)\n            real_flat_args = [maybe_to_real_tensor(a) for a in flat_args]\n            real_args, real_kwargs = pytree.tree_unflatten(real_flat_args, args_spec)\n\n            is_builtin = library_utils.is_builtin(func)\n            if not is_builtin:\n                mutation_checker = library_utils.MutationChecker(\n                    func, real_flat_args, args_spec\n                )\n\n            try:\n                real_out = func(*real_args, **real_kwargs)\n            except ZeroDivisionError as exc:\n                # we shouldn't broadly catch all errors here;\n                # some come from real-kernel mutation/aliasing checks we want to run.\n                # add more exception types as needed.\n                log.debug(\n                    \"real-tensor fallback failed for %s: %s; silently ignoring\",\n                    func,\n                    exc,\n                )\n\n            if not is_builtin:\n                mutation_checker.check()  # type: ignore[possibly-undefined]\n                library_utils.check_aliasing_constraint(func._name, flat_args, real_out)\n\n        elif self.propagate_real_tensors:\n            # This can happen occasionally legitimately, specifically when you\n            # are inside the meta of a data dependent operation and you create\n            # a tensor on an unbacked SymInt; at this point in time we don't\n            # know what the unbacked SymInt is, but we will know later.\n            # However, if there's a bug in the condition above, this condition\n            # will also trigger.\n            log.debug(\n                \"SKIPPED propagate_real_tensors %s(%s, %s) %s\",\n                func,\n                flat_arg_fake_tensors,\n                flat_args,\n                self.shape_env.unbacked_var_to_val if self.shape_env else None,\n            )\n\n        def maybe_propagate_real_tensors(fake_out: T) -> T:\n            import sympy\n\n            log.debug(\"maybe_propagate_real_tensors %s\", func)\n\n            def go(t: object, real_t: Tensor) -> None:\n                if isinstance(t, FakeTensor):\n                    # NB: unconditionally overwrite\n                    log.debug(\n                        \"maybe_propagate_real_tensors %s -> %s\", id(t), id(real_t)\n                    )\n                    t.real_tensor = real_t\n                    for s, real_s in zip(t.size(), real_t.size()):\n                        go(s, real_s)  # type: ignore[arg-type]\n                    for s, real_s in zip(t.stride(), real_t.stride()):\n                        go(s, real_s)  # type: ignore[arg-type]\n                    go(t.storage_offset(), real_t.storage_offset())  # type: ignore[arg-type]\n                elif isinstance(t, py_sym_types) and free_unbacked_symbols(t):\n                    if isinstance(t.node.expr, sympy.Symbol):\n                        assert self.shape_env is not None\n                        self.shape_env.set_unbacked_var_to_val(t.node.expr, real_t)\n                    elif (\n                        isinstance(s := t.node.expr, sympy.Eq)\n                        and isinstance(s.lhs, sympy.Symbol)\n                        and s.rhs == 1\n                    ):\n                        assert self.shape_env is not None\n                        self.shape_env.set_unbacked_var_to_val(s, int(real_t))\n\n            if real_out is not nil:\n                # cross check fake/real outputs, and optionally override fake kernel mismatches\n                if not torch._functorch.config.generate_fake_kernels_from_real_mismatches:\n                    self._maybe_infer_fake_kernel_from_pytree_out(\n                        func,\n                        (args, kwargs),\n                        (real_args, real_kwargs),\n                        fake_out,\n                        real_out,\n                    )\n                else:\n                    # this can override the output only when the flag is True\n                    fake_out = self._maybe_infer_fake_kernel_from_pytree_out(  # type: ignore[assignment]\n                        func,\n                        (args, kwargs),\n                        (real_args, real_kwargs),\n                        fake_out,\n                        real_out,\n                    )\n\n                # populate unbacked_var_to_val\n                if (\n                    not isinstance(fake_out, Tensor)\n                    and not isinstance(real_out, Tensor)\n                    and type(fake_out) != type(real_out)\n                ):\n                    # This can happen when decompositions have different return types,\n                    # e.g. namedtuple vs. tuple vs. list.\n                    tree_map_(\n                        go,\n                        tuple(pytree.tree_flatten(fake_out)),\n                        tuple(pytree.tree_flatten(real_out)),\n                    )\n                else:\n                    tree_map_(go, fake_out, real_out)\n\n                # If a data-dependent op is used in a decomposition, we\n                # may need to get the unbacked settings \"early\"\n                # TODO: Is this really needed?\n                compute_unbacked_bindings(self.shape_env, fake_out, peek=True)\n\n            return fake_out\n\n        # Try for fastpath\n        if has_symbolic_sizes:\n            fast_impl = get_fast_op_impls().get(func)\n            if fast_impl is not None:\n                return maybe_propagate_real_tensors(fast_impl(self, *args, **kwargs))\n\n        # If there's a Python meta, prefer that over the decomposition\n        from torch._decomp import meta_table as meta_table\n\n        if func not in meta_table and not self.cpp_meta_supports_symint(func):\n            from torch._decomp import decomposition_table\n\n            # Prefer Python decompositions over C++ ones\n            if func in decomposition_table and (\n                has_symbolic_sizes\n                or (\n                    # TODO: Remove these exclusions, so that we can remove\n                    # this leg entirely\n                    torch_decomp_decompositions(func)\n                    and all(not is_sparse_any(e) for e in flat_arg_fake_tensors)\n                )\n            ):\n                with self:\n                    return maybe_propagate_real_tensors(\n                        decomposition_table[func](*args, **kwargs)\n                    )\n\n            with self:\n                # Decomposes CompositeImplicitAutograd ops\n                r = func.decompose(*args, **kwargs)\n                if r is not NotImplemented:\n                    return maybe_propagate_real_tensors(r)\n\n        # prims already wrap FakeTensor inputs to FakeTensor outputs\n        # and do device logic, we dont need do anything but run them\n        # and ensure that Meta kernels are dispatched to (see)\n        # Fake Tensor Dispatch Keys\n        # TODO - we should be use the prim aten impl\n        # TODO - fix prims complex ops\n        if (\n            \"prims::\" in func._schema.name\n            and hasattr(func, \"prim_meta_impl\")\n            and not stride_incorrect_op(func)\n        ):\n            with self:\n                return maybe_propagate_real_tensors(\n                    func.prim_meta_impl(*args, **kwargs)\n                )\n\n        profiles = torch._dynamo.config._custom_ops_profile\n        if profiles is not None:\n            if func in profiles.data:\n                return profiles.generic_fake_kernel(func, self, *args, **kwargs)\n\n        if (\n            self.propagate_real_tensors\n            and real_out is not nil\n            and not library_utils.is_builtin(func)\n            and self.shape_env is not None\n        ):\n            # Automatically infer a Fake kernel if there isn't one.\n            if not library_utils.has_fake_kernel(func):\n                result = inferred_fake_kernel_from_real_out(self, func, real_out)\n\n                dtrace_structured(\n                    \"missing_fake_kernel\",\n                    metadata_fn=lambda: {\n                        \"op\": str(func),\n                    },\n                )\n                return maybe_propagate_real_tensors(result)\n\n        # Users can register FakeTensor rules for custom operators\n        # Call them if they exist.\n        maybe_fake_impl = torch._library.simple_registry.singleton.find(\n            func.name()\n        ).fake_impl.kernel\n        if maybe_fake_impl:\n            try:\n                ctx = torch._library.fake_impl.FakeImplCtx(self, func)\n                with torch._library.fake_impl.set_ctx_getter(lambda: ctx), self:\n                    result = maybe_fake_impl(*args, **kwargs)\n                    return maybe_propagate_real_tensors(result)\n\n            except MissingOpProfile as e:\n                # If we have a fake kernel registered generated from OpProfiles\n                # but there doesn't exist a profile for the existing inputs, and we are in\n                if (\n                    self.propagate_real_tensors\n                    and real_out is not nil\n                    and not library_utils.is_builtin(func)\n                    and self.shape_env is not None\n                ):\n                    result = inferred_fake_kernel_from_real_out(self, func, real_out)\n\n                    dtrace_structured(\n                        \"missing_fake_kernel\",\n                        metadata_fn=lambda: {\n                            \"op\": str(func),\n                        },\n                    )\n                    return maybe_propagate_real_tensors(result)\n                else:\n                    raise e\n\n        # special handling for funcs registered through `register_op_impl`,\n        # e.g., manipulating args on constructor calls to construct meta tensors\n        # and then afterwards wrapping them to a FakeTensor\n        for run_impl_check, op_impl in op_implementations_checks:\n            if run_impl_check(func):\n                op_impl_out = op_impl(self, func, *args, **kwargs)\n                if op_impl_out is not NotImplemented:\n                    return maybe_propagate_real_tensors(op_impl_out)\n\n        def maybe_run_unsafe_fallback(\n            error: Optional[RuntimeError] = None,\n        ) -> Optional[FakeTensor]:\n            # We infer the meta of a custom ops that return None to just\n            # return None. custom ops are not allowed to mutate metadata\n            # of their inputs, so this is safe.\n            if torch._library.utils.can_generate_trivial_fake_impl(func):\n                return None\n            # no meta kernel registered, fallback to kernel for the device\n            if has_symbolic_sizes or not self.can_run_unsafe_fallback(func):\n                raise UnsupportedOperatorException(func)\n            if error is None:\n                error = UnsupportedOperatorException(func)\n            return run_fallback_kernel(self, func, flat_args, args_spec, error)\n\n        # Optimization: If there is no Meta kernel, it takes a surprisingly long\n        # amount of time to catch the NotImplementedError, so we check it here.\n        if not has_meta(func):\n            fallback = maybe_run_unsafe_fallback()\n            return maybe_propagate_real_tensors(fallback)\n\n        # run kernel registered to meta for func, which include\n        # python meta registrations, prims, decomps, and c++ meta fns (structured kernels)\n        # It's possible that the kernel will return NotImplementedError\n        try:\n            with in_kernel_invocation_manager(self):\n                r = func(*args, **kwargs)\n        except NotImplementedError as not_implemented_error:\n            return maybe_run_unsafe_fallback(not_implemented_error)\n        except Exception:\n            log.exception(\"failed while attempting to run meta for %s\", func)\n            raise\n\n        return maybe_propagate_real_tensors(\n            self.wrap_meta_outputs_with_default_device_logic(\n                r, func, flat_args, device=kwargs.get(\"device\")\n            )\n        )\n\n    # WARNING: DO NOT add any additional namespaces/operators here if they refer to operators\n    # outside of the pytorch/pytorch library! Any pre-existing things here\n    # are either in the pytorch/pytorch library or have been grandfathered in.\n    # The fallback does not always work and MAY CRASH and emit unreadable error messages\n    # so it should not be allowed by default.\n    _can_run_unsafe_fallback_allowed_namespaces = ordered_set(\n        \"debugprims\",\n        \"prims\",\n        \"aten\",\n        \"xla\",\n        \"vision\",\n        \"torchtext\",\n        \"torchaudio\",\n        \"quantized\",\n    )\n\n    def can_run_unsafe_fallback(self, func: OpOverload) -> bool:\n        if not self.allow_fallback_kernels:\n            return False\n        # It's OK to try the fallback for built-in ops (e.g. aten, prims)\n        # because we control and test these but the fallback leads to unexpected behavior\n        # in user-defined custom ops\n        return (\n            func.namespace in self._can_run_unsafe_fallback_allowed_namespaces\n            or func.name() == \"fbgemm::gmm\"\n        )\n\n    def validate_and_convert_non_fake_tensors(\n        self,\n        func: OpOverload,\n        converter: FakeTensorConverter,\n        flat_args: Sequence[object],\n        args_spec: TreeSpec,\n    ) -> tuple[list[object], list[FakeTensor]]:\n        \"\"\"\n        Checks if the list of tensors are fake tensors.\n        If not, try to convert them to fake tensors.\n        Returns the original args, kwargs, and a flattened list of (args, kwargs) that are fake tensors.\n        \"\"\"\n        flat_arg_fake_tensors: list[FakeTensor] = []\n\n        def validate(x: T) -> Union[T, FakeTensor]:\n            if not isinstance(x, Tensor):\n                return x\n\n            nonlocal flat_arg_fake_tensors\n            if not self.is_our_fake(x):\n                if hasattr(func, \"tags\") and torch.Tag.inplace_view in func.tags:\n                    args, kwargs = pytree.tree_unflatten(flat_args, args_spec)\n                    raise AssertionError(\n                        f\"Can't call metadata mutating ops on non-Fake Tensor inputs. Found in {render_call(func, args, kwargs)}\"\n                    )\n                allow_non_fake_inputs = (\n                    self.allow_non_fake_inputs\n                    if fake_tensor_tls.allow_non_fake_inputs_override is None\n                    else fake_tensor_tls.allow_non_fake_inputs_override\n                )\n                if not allow_non_fake_inputs:\n                    if isinstance(x, FakeTensor) and x.fake_mode is not self:\n                        raise AssertionError(\"Mixing fake modes NYI\")\n                    args, kwargs = pytree.tree_unflatten(flat_args, args_spec)\n                    raise AssertionError(\n                        f\"Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode \"\n                        f\"with 'allow_non_fake_inputs'. Found in {render_call(func, args, kwargs)}\"\n                    )\n\n                out = converter.from_real_tensor(self, x)\n            else:\n                out = x\n\n            flat_arg_fake_tensors.append(out)\n            return out\n\n        validated_args = [validate(a) for a in flat_args]\n        return validated_args, flat_arg_fake_tensors\n\n    def wrap_meta_outputs_with_default_device_logic(\n        self,\n        r: object,\n        func: OpOverload,\n        flat_args: Sequence[object],\n        device: torch.device,\n    ) -> PyTree:\n        converter = self.fake_tensor_converter\n\n        # Lazily initialized, in case there are no tensor returns\n        common_device = None\n        has_scalar_only_inputs = False\n\n        def wrap(e: T) -> Union[T, FakeTensor]:\n            nonlocal common_device\n            nonlocal has_scalar_only_inputs\n\n            if not isinstance(e, Tensor):\n                return e\n\n            if common_device is None:\n                (\n                    common_device,\n                    has_scalar_only_inputs,\n                ) = FakeTensor._find_common_device(func, flat_args)\n\n            is_our_fake = self.is_our_fake(e)\n            if is_our_fake:\n                torch._check(\n                    e.device == common_device,\n                    lambda: f\"FakeTensor is wrapped to wrong device, found {e.device}, expected {common_device}\",\n                )\n                return cast(T, e)\n            elif converter is not None:\n                if has_scalar_only_inputs:\n                    # Under FakeTensorMode, op accepts scalar only inputs, such as aten.add/sub/mul/div,\n                    # returns a real scalar tensor on CPU. See TensorMeta() in _prims/__init__.py for details.\n                    # We thus directly convert real tensor to fake tensor.\n                    return converter.from_real_tensor(self, e)\n                else:\n                    return converter.from_meta_and_device(\n                        self, e, device or common_device\n                    )\n            else:\n                return e\n\n        return tree_map(wrap, r)\n\n    def create_symbolic_nested_int(\n        self, *, nt_tensor_id: Optional[int] = None\n    ) -> torch.SymInt:\n        # See Note: [Creating symbolic nested int]\n        # Returned nested int always has coeff=1; multiply the result by coeff if needed\n        import torch.nested._internal.nested_tensor\n        from torch.nested._internal.nested_int import NestedIntNode\n\n        if nt_tensor_id is None:\n            nt_tensor_id = self.nt_tensor_id_counter\n            assert self.enter_stack, \"should only called while FakeTensorMode is active\"\n            self.nt_tensor_id_counter += 1\n        hint = torch.SymInt(NestedIntNode(nt_tensor_id, 1))\n\n        src = torch._dynamo.source.EphemeralSource(\"intermediate_offsets_or_lengths\")\n        assert self.shape_env is not None\n        ret = self.shape_env.create_symintnode(\n            sym=self.shape_env.create_symbol(\n                val=hint,\n                source=src,\n            ),\n            hint=hint,\n            source=src,\n        )\n        return ret\n\n    _cpp_meta_supports_symint = ordered_set(\n        aten.empty.memory_format,\n        aten.empty_strided.default,\n        aten.as_strided_scatter.default,\n        aten.as_strided.default,\n        aten.as_strided_.default,\n        aten.zeros.default,\n        aten.detach.default,\n        aten.view_as_real.default,\n        aten.view_as_complex.default,\n        aten.set_.source_Storage_storage_offset,\n        aten._sparse_coo_tensor_with_dims_and_tensors.default,\n    )\n\n    def cpp_meta_supports_symint(self, func: OpOverload) -> bool:\n        if torch.Tag.view_copy in func.tags:\n            return True\n        return func in self._cpp_meta_supports_symint\n\n    lift_fns = ordered_set(aten.lift_fresh.default, aten.lift_fresh_copy.default)\n\n    def may_turn_const(self, t: Tensor) -> bool:\n        return (\n            t.numel() <= CONSTANT_NUMEL_LIMIT\n            and not is_sparse_any(t)\n            and not self.is_our_fake(t)\n            and not t.device.type == \"meta\"\n        )\n\n    def invalidate_written_to_constants(\n        self,\n        func: OpOverload,\n        flat_arg_fake_tensors: Sequence[FakeTensor],\n        args: Sequence[object],\n        kwargs: Mapping[str, object],\n    ) -> None:\n        any_constant = any(e.constant is not None for e in flat_arg_fake_tensors)\n        schema_info = get_schema_info(func)\n        if any_constant and schema_info.is_mutable():\n            _, new_kwargs = normalize_function(  # type: ignore[misc]\n                func,\n                args=args,  # type: ignore[arg-type]\n                kwargs=kwargs,  # type: ignore[arg-type]\n                normalize_to_only_use_kwargs=True,\n            )\n            for k, v in new_kwargs.items():\n                k = k if (k != \"input\" or schema_info.has_argument(k)) else \"self\"\n                if (\n                    self.is_our_fake(v)\n                    and schema_info.is_mutable(k)\n                    and v.constant is not None\n                ):\n                    self.fake_tensor_converter.invalidate_constant_aliases(v.constant)\n\n    def from_tensor(\n        self,\n        tensor: Tensor,\n        *,\n        static_shapes: Optional[bool] = None,\n        source: Optional[Source] = None,\n        symbolic_context: Optional[SymbolicContext] = None,\n        trace: bool = True,\n    ) -> FakeTensor:\n        shape_env: Optional[ShapeEnv] = self.shape_env\n        if static_shapes is None:\n            static_shapes = self.static_shapes\n        if static_shapes:\n            assert symbolic_context is None, (\n                \"cannot set both static_shapes and symbolic_context\"\n            )\n            shape_env = None\n        return self.fake_tensor_converter.from_real_tensor(\n            self,\n            tensor,\n            shape_env=shape_env,\n            source=source,\n            symbolic_context=symbolic_context,\n            trace=trace,\n        )\n\n\n_StoragePointer = object\n\n\ndef _has_unrepresented_symbols(\n    state: _CacheKeyState, output: Optional[FakeTensor]\n) -> bool:\n    from torch.fx.experimental.symbolic_shapes import _iterate_exprs\n\n    for s in _iterate_exprs(output):\n        for symbol in s.free_symbols:\n            if symbol not in state.known_symbols:\n                return True\n\n    return False\n\n\n# NB: returns fake tensors\ndef run_fallback_kernel(\n    fake_mode: FakeTensorMode,\n    func: OpOverload,\n    flat_args: Sequence[object],\n    args_spec: PyTree,\n    orig_not_implemented_exception: RuntimeError,\n) -> FakeTensor:\n    # these should all be supported, just to be safe\n    # avoid fallback for operators which inplace modify metadata\n    # because the input fake tensors would be umodified\n    if torch.Tag.inplace_view in func.tags:\n        raise orig_not_implemented_exception\n\n    inp_impls = {}\n\n    # Don't use in_kernel_invocation_manager(fake_mode) as we want to do\n    # REAL compute (not with meta device)\n    with no_dispatch():\n\n        def to_real_tensor(e: T) -> Union[T, Tensor]:\n            if fake_mode.is_our_fake(e):\n                out = torch.zeros_like(e, device=e.fake_device)\n                if e.is_sparse:\n                    out._coalesced_(e.is_coalesced())\n                inp_impls[id(out)] = e\n                return out\n            return e\n\n        flat_args = [to_real_tensor(a) for a in flat_args]\n        args, kwargs = pytree.tree_unflatten(flat_args, args_spec)\n\n        r = func(*args, **kwargs)\n\n    storages: set[_StoragePointer] = set()\n\n    for e in flat_args:\n        if isinstance(e, Tensor):\n            if not is_sparse_any(e):\n                storages.add(e._typed_storage()._cdata)\n\n    # TODO: also check metadata change on inputs\n    # proper aliasing/metadata relationship between outputs and inputs will\n    # not be set up, bc of conversion to device, unless we can reuse an\n    # input impl\n\n    def map_out(e: T) -> Union[T, FakeTensor]:\n        if id(e) not in inp_impls and (\n            isinstance(e, Tensor)\n            and not is_sparse_any(e)\n            and e._typed_storage()._cdata in storages\n        ):\n            raise orig_not_implemented_exception\n\n        if isinstance(e, Tensor):\n            if id(e) in inp_impls:\n                return inp_impls[id(e)]\n            else:\n                return fake_mode.fake_tensor_converter.from_real_tensor(fake_mode, e)\n        else:\n            return e\n\n    return pytree.tree_map(map_out, r)\n\n\ndef _set_cache_key_for_shape_env(\n    cache: dict[_DispatchCacheKey, _DispatchCacheEntry],\n    key: _DispatchCacheKey,\n    entry: _DispatchCacheEntry,\n) -> None:\n    key.strip_shape_env()\n    cache[key] = entry\n\n\ndef _set_cache_key(\n    cache: dict[_DispatchCacheKey, _DispatchCacheEntry],\n    key: _DispatchCacheKey,\n    entry: _DispatchCacheEntry,\n) -> None:\n    cache[key] = entry\n\n\n# Just for use to allow copying a module to fake tensors,\n# does not apply elsewhere\nclass FakeCopyMode(TorchFunctionMode):\n    def __init__(self, fake_mode: FakeTensorMode) -> None:\n        self.fake_mode = fake_mode\n\n    def __torch_function__(\n        self,\n        func: OpOverload,\n        types: Sequence[type],\n        args: Sequence[object] = (),\n        kwargs: Optional[Mapping[str, object]] = None,\n    ) -> FakeTensor:\n        kwargs = kwargs if kwargs else {}\n\n        # clone will get called in Parameter deepcopy\n        if func == torch._C.TensorBase.clone:\n            assert isinstance(args[0], Tensor)\n            return func(\n                self.fake_mode.from_tensor(args[0], static_shapes=True), **kwargs\n            )\n        elif func == Tensor.__deepcopy__:\n            assert len(args) == 2 and len(kwargs) == 0\n            tensor = cast(Tensor, args[0])\n            memo = cast(dict[int, FakeTensor], args[1])\n\n            if id(tensor) in memo:\n                return memo[id(tensor)]\n\n            out = self.fake_mode.from_tensor(tensor, static_shapes=True)\n            memo[id(tensor)] = out\n            return out\n        else:\n            with torch._C.DisableTorchFunctionSubclass():\n                return func(*args, **kwargs)\n\n\ndef _device_handler(args: Sequence[object]) -> torch.device:\n    # NB: Don't use is_our_fake, just serve the fake information\n    # as is.  Notice we don't use 'self'; we use args[0].fake_mode\n    # because they may not be the same.  It would also be possible\n    # to return NotImplemented here, in which case the FakeTensor\n    # handler on args[0] would handle it, but we're being nice and\n    # short-circuiting quickly.\n    assert len(args) == 1 and isinstance(args[0], FakeTensor)\n    if args[0].fake_mode.in_kernel_invocation:\n        return torch.device(\"meta\")\n    else:\n        return args[0].fake_device\n\n\n# [subclass inputs]\n# Suppose we enable fake tensor mode.  This means that fake tensor\n# mode will run first.  But what if we do an operation that\n# involves a tensor subclass that will desugar into normal tensor\n# operations?  Without returning NotImplemented, fake tensor mode will run first,\n# decide that a conversion was made (since there was a non fake\n# tensor argument), and report an error that converting non\n# fake tensor is not supported.  What we actually wanted to happen\n# was to give the subclass a chance to figure out what it wants to\n# before erroring out. Returning NotImplemented here allows this.\ndef _check_for_subclass(flat_args: Sequence[object]) -> bool:\n    return any(_check_for_subclass_arg(x) for x in flat_args)\n\n\ndef _check_for_subclass_arg(x: object) -> bool:\n    return (\n        not isinstance(x, FakeTensor)\n        and isinstance(x, Tensor)\n        and type(x) is not Tensor\n        and type(x) is not torch.nn.Parameter\n    )\n\n\n_DISPATCH_META_HANDLERS = {\n    torch.ops.prim.device.default: _device_handler,\n    torch.ops.aten.size.default: lambda args: tuple(\n        int(s) for s in cast(Tensor, args[0]).size()\n    ),\n    torch.ops.aten.stride.default: lambda args: tuple(\n        int(s) for s in cast(Tensor, args[0]).stride()\n    ),\n    torch.ops.aten.storage_offset.default: lambda args: int(\n        cast(Tensor, args[0]).storage_offset()\n    ),\n}\n\n_DISPATCH_HANDLE_DIRECTLY = ordered_set(\n    torch.ops.aten.is_coalesced.default,\n    torch.ops.aten.dense_dim.default,\n    torch.ops.aten.sparse_dim.default,\n    # _RecordFunction doesn't support __eq__ so make sure not to attempt to\n    # cache it.\n    torch.ops.profiler._record_function_exit._RecordFunction,\n)\n\nfrom torch._subclasses.fake_impls import (  # noqa: F401\n    _device_not_kwarg_ops,\n    _is_tensor_constructor,\n    _like_tensor_constructors,\n    contains_tensor_types,\n    get_fast_op_impls,\n    has_meta,\n    op_implementations_checks,\n    stride_incorrect_op,\n)\n\n\ndef evict_fake_tensor_cache_key(key: _DispatchCacheKey) -> None:\n    if key in FakeTensorMode.cache:\n        FakeTensorMode.cache.pop(key)\n\n\n@atexit.register\ndef dump_cache_stats() -> None:\n    log.info(\"FakeTensor cache stats:\")\n    log.info(\"  cache_hits: %s\", FakeTensorMode.cache_hits)\n    log.info(\"  cache_misses: %s\", FakeTensorMode.cache_misses)\n    bypasses = FakeTensorMode.cache_bypasses\n    if bypasses:\n        log.info(\"  cache_bypasses:\")\n        width = max(len(k) for k in bypasses)\n        for k, v in sorted(bypasses.items(), key=lambda i: -i[1]):\n            log.info(\"    %-*s %s\", width + 1, f\"{k}:\", v)\n\n\ndef _infer_fake_from_real_tensor(\n    mode: FakeTensorMode, op: torch._ops.OpOverload, real_out: torch.Tensor\n) -> torch.Tensor:\n    def unsupported(reason: str) -> None:\n        raise RuntimeError(\n            f\"propagate_real_tensors: we cannot infer a Fake kernel \"\n            f\"(meta kernel) for operator {op._name} because {reason}. \"\n            f\"Please use torch.library.register_fake to add a Fake kernel.\"\n        )\n\n    if real_out.storage_offset() != 0:\n        unsupported(\n            f\"a return has a non-zero storage offset {real_out.storage_offset()}\"\n        )\n\n    # Since PT2 is rank specialized, there's no such thing as a symbolic\n    # output rank. So we can assume the fake tensor has the same number of\n    # dimensions as the real tensor output.\n    #\n    # We shouldn't assume the Fake sizes/strides are exactly what we see on\n    # the real tensor output (perhaps we should give users a lever to toggle\n    # this). This is because there's a good amount of operators that return\n    # outputs with data-dependent output shape.\n    # So we infer the output sizes to all be unbacked symints\n    fake_shape = [\n        torch._library.fake_impl.allocate_size(mode.shape_env)\n        for _ in range(real_out.dim())\n    ]\n\n    # We infer what the strides are. We had a couple of options for this:\n    # - assume the strides are computable from the sizes\n    # - use new fresh unbacked symints in the strides\n    #   This doesn't work that well (PT2 doesn't support unbacked symint strides well)\n    # - use the real strides\n    #   This can only be used if we assume the strides are static.\n    # We went with the first option.\n    fake_strides = [-1] * real_out.dim()\n    strides = [(s, idx) for idx, s in enumerate(real_out.stride())]\n    strides.sort(key=lambda x: (x[0], -x[1]))\n    expected = 1\n    fake_stride = expected\n    for s, idx in strides:\n        if s != expected:\n            unsupported(\n                f\"a return was not dense in memory (sizes {real_out.shape} strides {real_out.stride()})\"\n            )\n        fake_strides[idx] = fake_stride\n        expected = expected * real_out.shape[idx]\n        fake_stride = fake_stride * fake_shape[idx]\n\n    with mode:\n        return torch.empty_strided(\n            fake_shape,\n            fake_strides,\n            device=real_out.device,\n            dtype=real_out.dtype,\n            layout=real_out.layout,\n        )\n\n\ndef inferred_fake_kernel_from_real_out(\n    mode: FakeTensorMode, op: torch._ops.OpOverload, real_out: Any\n) -> Any:\n    assert mode.shape_env is not None\n\n    # Only support operators that have all Tensor outputs\n    # This is a general limitation on custom ops that we impose for PT2\n    # to avoid baking non-symbolic float/int outputs into the graph.\n    real_flat_out, spec = pytree.tree_flatten(real_out)\n    if not all(isinstance(t, torch.Tensor) for t in real_flat_out):\n        raise RuntimeError(\n            f\"propagate_real_tensors: we don't support operators that return \"\n            f\"non-Tensors. Got {op._schema}\"\n        )\n\n    fake_flat_out = [_infer_fake_from_real_tensor(mode, op, t) for t in real_flat_out]\n    return pytree.tree_unflatten(fake_flat_out, spec)\n", 3267], "/data/wangyingqi/code/pytorch/torch/utils/data/_utils/__init__.py": ["r\"\"\"Utility classes & functions for data loading. Code in this folder is mostly used by ../dataloder.py.\n\nA lot of multiprocessing is used in data loading, which only supports running\nfunctions defined in global environment (py2 can't serialize static methods).\nTherefore, for code tidiness we put these functions into different files in this\nfolder.\n\"\"\"\n\nimport atexit\nimport sys\n\n# old private location of the ExceptionWrapper that some users rely on:\nfrom torch._utils import ExceptionWrapper\n\n\nIS_WINDOWS = sys.platform == \"win32\"\n\n\nMP_STATUS_CHECK_INTERVAL = 5.0\nr\"\"\"Interval (in seconds) to check status of processes to avoid hanging in\n    multiprocessing data loading. This is mainly used in getting data from\n    another process, in which case we need to periodically check whether the\n    sender is alive to prevent hanging.\"\"\"\n\n\npython_exit_status = False\nr\"\"\"Whether Python is shutting down. This flag is guaranteed to be set before\nthe Python core library resources are freed, but Python may already be exiting\nfor some time when this is set.\n\nHook to set this flag is `_set_python_exit_flag`, and is inspired by a similar\nhook in Python 3.7 multiprocessing library:\nhttps://github.com/python/cpython/blob/d4d60134b29290049e28df54f23493de4f1824b6/Lib/multiprocessing/util.py#L277-L327\n\"\"\"\n\n\ntry:\n    import numpy\n\n    HAS_NUMPY = True\nexcept ModuleNotFoundError:\n    HAS_NUMPY = False\n\n\ndef _set_python_exit_flag() -> None:\n    global python_exit_status\n    python_exit_status = True\n\n\natexit.register(_set_python_exit_flag)\n\n\nfrom . import collate, fetch, pin_memory, signal_handling, worker\n", 53], "/data/wangyingqi/code/pytorch/torch/_logging/_internal.py": ["# mypy: allow-untyped-defs\nimport contextlib\nimport functools\nimport hashlib\nimport importlib.util\nimport itertools\nimport json\nimport logging\nimport os\nimport os.path\nimport pathlib\nimport re\nimport sys\nimport tempfile\nimport time\nimport warnings\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom typing import Any, Callable, Generic, Optional, Union\nfrom typing_extensions import ParamSpec\nfrom weakref import WeakSet\n\nimport torch._logging.structured\nfrom torch._guards import CompileId\nfrom torch._utils_internal import log_trace_structured_event\nfrom torch.utils._traceback import CapturedTraceback\n\n\n_P = ParamSpec(\"_P\")\n\nlog = logging.getLogger(__name__)\n\n# This is a synthetic logger which doesn't correspond to an actual logger,\n# but handles all of our \"tracing\" logging, which is structured and doesn't go\n# to stderr but always goes to a dedicated log file.  We don't put these\n# loggers in the classic module hierarchy, because we don't want a suppression\n# of logs to also cause a trace to get suppressed (traces typically are not\n# collected, unless we are in prod, in which case they always are collected.)\n#\n# TODO: Maybe we should allow for some sub-hierarchy so you can control which\n# traces you want to collect, for performance reasons.\n#\n# See https://docs.google.com/document/d/1CX_hJ0PNy9f3R1y8TJrfkSeLkvGjjjLU84BSXgS2AZ8/edit\ntrace_log = logging.getLogger(\"torch.__trace\")\n\nDEFAULT_LOG_LEVEL = logging.WARNING\nLOG_ENV_VAR = \"TORCH_LOGS\"\nLOG_OUT_ENV_VAR = \"TORCH_LOGS_OUT\"\nLOG_FORMAT_ENV_VAR = \"TORCH_LOGS_FORMAT\"\nLOG_TRACE_ID_FILTER = \"TORCH_LOGS_TRACE_ID_FILTER\"\nTRACE_ENV_VAR = \"TORCH_TRACE\"\nDTRACE_ENV_VAR = \"TORCH_DTRACE\"\n\nLOG_TRACE_HANDLER: Optional[\"LazyTraceHandler\"] = None\n\nGET_DTRACE_STRUCTURED = False\n\n\n@dataclass\nclass LogRegistry:\n    # shorthand name to log qualified name\n    # Note: this only contains loggers registered\n    # from register_log\n    # e.g. \"dynamo\" -> \"torch._dynamo\"\n    log_alias_to_log_qnames: dict[str, list[str]] = field(default_factory=dict)\n\n    # artifact logger qualified names,\n    # this is populated lazily, as calls to getArtifactLogger\n    # currently formatted as <module>.__<artifact_name>\n    # e.g. \"torch._dynamo.convert_frame.__guards\"\n    artifact_log_qnames: set[str] = field(default_factory=set)\n\n    # child logs of registered logs if specified via open\n    # registration by the user (ie placing \"torch._dynamo.output_graph\" in the env var)\n    # these need to be tracked so their levels can be reset properly\n    # e.g. \"torch._dynamo.output_graph\"\n    child_log_qnames: set[str] = field(default_factory=set)\n\n    # artifact names, populated by register_artifact\n    # e.g. \"guards\"\n    artifact_names: set[str] = field(default_factory=set)\n\n    # Artifacts that should be visible by default in the error message\n    visible_artifacts: set[str] = field(default_factory=set)\n\n    # A short description of each artifact\n    artifact_descriptions: dict[str, str] = field(default_factory=dict)\n\n    # artifacts which are not displayed unless explicitly named in the\n    # settings. Ex. output_code is NOT displayed even if the inductor\n    # log level is set to DEBUG. It must be explicitly named in the settings\n    off_by_default_artifact_names: set[str] = field(default_factory=set)\n\n    # logging format string for artifacts\n    artifact_log_formatters: dict[str, logging.Formatter] = field(default_factory=dict)\n\n    def is_artifact(self, name):\n        return name in self.artifact_names\n\n    def is_log(self, alias):\n        return alias in self.log_alias_to_log_qnames\n\n    # register a log with an alias\n    def register_log(self, alias, log_qnames: Union[str, list[str]]) -> None:\n        if isinstance(log_qnames, str):\n            log_qnames = [log_qnames]\n        self.log_alias_to_log_qnames[alias] = log_qnames\n\n    # register an artifact name\n    def register_artifact_name(\n        self, name, description, visible, off_by_default, log_format\n    ) -> None:\n        self.artifact_names.add(name)\n        if visible:\n            self.visible_artifacts.add(name)\n        self.artifact_descriptions[name] = description\n\n        # if off by default, don't enable it\n        # when log_name's log_level is set to DEBUG\n        if off_by_default:\n            self.off_by_default_artifact_names.add(name)\n\n        if log_format is not None:\n            self.artifact_log_formatters[name] = logging.Formatter(log_format)\n\n    # register the qualified name of an artifact log\n    # this is needed to know which logs need to be reset\n    # whenever the log_state is changed\n    def register_artifact_log(self, artifact_log_qname) -> None:\n        self.artifact_log_qnames.add(artifact_log_qname)\n\n    def register_child_log(self, log_qname) -> None:\n        self.child_log_qnames.add(log_qname)\n\n    # flattens all the qnames together (TODO: consider memoizing?)\n    def get_log_qnames(self) -> set[str]:\n        return set(itertools.chain.from_iterable(self.log_alias_to_log_qnames.values()))\n\n    def get_artifact_log_qnames(self):\n        return set(self.artifact_log_qnames)\n\n    def get_child_log_qnames(self):\n        return set(self.child_log_qnames)\n\n    def is_off_by_default(self, artifact_qname):\n        return artifact_qname in self.off_by_default_artifact_names\n\n\n@dataclass\nclass LogState:\n    # qualified log names -> currently set log level\n    log_qname_to_level: dict[str, str] = field(default_factory=dict)\n\n    # the set of currently enabled artifacts\n    artifact_names: set[str] = field(default_factory=set)\n\n    def enable_artifact(self, artifact_name) -> None:\n        self.artifact_names.add(artifact_name)\n\n    def is_artifact_enabled(self, name):\n        return name in self.artifact_names\n\n    def enable_log(self, log_qnames, log_level) -> None:\n        if isinstance(log_qnames, str):\n            log_qnames = [log_qnames]\n        for log_qname in log_qnames:\n            self.log_qname_to_level[log_qname] = log_level\n\n    def get_log_level_pairs(self):\n        \"\"\"Returns all qualified module names for which the user requested\n        explicit logging settings.\n\n        .. warning:\n\n            This function used to return all loggers, regardless of whether\n            or not the user specified them or not; it now only returns logs\n            which were explicitly mentioned by the user (and torch, which\n            always is implicitly requested when we initialize our logging\n            subsystem.)\n        \"\"\"\n        return self.log_qname_to_level.items()\n\n    def clear(self) -> None:\n        self.log_qname_to_level.clear()\n        self.artifact_names.clear()\n\n\nlog_registry = LogRegistry()\nlog_state = LogState()\n\n# sample usage: torch._logging.set_logs(**torch._logging.DEFAULT_LOGGING)\nDEFAULT_LOGGING = {\n    \"dynamo\": logging.INFO,\n    \"aot\": logging.INFO,\n    \"inductor\": logging.INFO,\n    \"fsdp\": logging.INFO,\n    \"ddp_graphs\": True,\n    \"graph_breaks\": True,\n    \"guards\": True,\n    \"recompiles\": True,\n    \"dynamic\": logging.INFO,\n}\n\n\ndef set_logs(\n    *,\n    all: Optional[int] = None,\n    dynamo: Optional[int] = None,\n    aot: Optional[int] = None,\n    autograd: Optional[int] = None,\n    dynamic: Optional[int] = None,\n    inductor: Optional[int] = None,\n    distributed: Optional[int] = None,\n    c10d: Optional[int] = None,\n    ddp: Optional[int] = None,\n    fsdp: Optional[int] = None,\n    dtensor: Optional[int] = None,\n    onnx: Optional[int] = None,\n    bytecode: bool = False,\n    aot_graphs: bool = False,\n    aot_joint_graph: bool = False,\n    ddp_graphs: bool = False,\n    graph: bool = False,\n    graph_code: bool = False,\n    graph_code_verbose: bool = False,\n    graph_breaks: bool = False,\n    graph_sizes: bool = False,\n    guards: bool = False,\n    recompiles: bool = False,\n    recompiles_verbose: bool = False,\n    trace_source: bool = False,\n    trace_call: bool = False,\n    trace_bytecode: bool = False,\n    output_code: bool = False,\n    kernel_code: bool = False,\n    schedule: bool = False,\n    perf_hints: bool = False,\n    pre_grad_graphs: bool = False,\n    post_grad_graphs: bool = False,\n    ir_pre_fusion: bool = False,\n    ir_post_fusion: bool = False,\n    onnx_diagnostics: bool = False,\n    fusion: bool = False,\n    overlap: bool = False,\n    export: Optional[int] = None,\n    modules: Optional[dict[str, Union[int, bool]]] = None,\n    cudagraphs: bool = False,\n    sym_node: bool = False,\n    compiled_autograd: bool = False,\n    compiled_autograd_verbose: bool = False,\n    cudagraph_static_inputs: bool = False,\n    benchmarking: bool = False,\n    autotuning: bool = False,\n    graph_region_expansion: bool = False,\n    inductor_metrics: bool = False,\n    hierarchical_compile: bool = False,\n    compute_dependencies: bool = False,\n) -> None:\n    \"\"\"\n    Sets the log level for individual components and toggles individual log\n    artifact types.\n\n    .. warning:: This feature is a prototype and may have compatibility\n        breaking changes in the future.\n\n    .. note:: The ``TORCH_LOGS`` environment variable has complete precedence\n        over this function, so if it was set, this function does nothing.\n\n    A component is a set of related features in PyTorch. All of the log\n    messages emitted from a given component have their own log levels. If the\n    log level of a particular message has priority greater than or equal to its\n    component's log level setting, it is emitted. Otherwise, it is suppressed.\n    This allows you to, for instance, silence large groups of log messages that\n    are not relevant to you and increase verbosity of logs for components that\n    are relevant. The expected log level values, ordered from highest to lowest\n    priority, are:\n\n        * ``logging.CRITICAL``\n        * ``logging.ERROR``\n        * ``logging.WARNING``\n        * ``logging.INFO``\n        * ``logging.DEBUG``\n        * ``logging.NOTSET``\n\n    See documentation for the Python ``logging`` module for more information on\n    log levels: `<https://docs.python.org/3/library/logging.html#logging-levels>`_\n\n    An artifact is a particular type of log message. Each artifact is assigned\n    to a parent component. A component can emit many different kinds of\n    artifacts. In general, an artifact is emitted if either its corresponding\n    setting in the argument list below is turned on or if its parent component\n    is set to a log level less than or equal to the log level of the artifact.\n\n    Keyword args:\n        all (:class:`Optional[int]`):\n            The default log level for all components. Default: ``logging.WARN``\n\n        dynamo (:class:`Optional[int]`):\n            The log level for the TorchDynamo component. Default: ``logging.WARN``\n\n        aot (:class:`Optional[int]`):\n            The log level for the AOTAutograd component. Default: ``logging.WARN``\n\n        autograd (:class:`Optional[int]`):\n            The log level for autograd. Default: ``logging.WARN``\n\n        inductor (:class:`Optional[int]`):\n            The log level for the TorchInductor component. Default: ``logging.WARN``\n\n        dynamic (:class:`Optional[int]`):\n            The log level for dynamic shapes. Default: ``logging.WARN``\n\n        distributed (:class:`Optional[int]`):\n            Whether to log c10d communication operations and other debug info from PyTorch Distributed components.\n            Default: ``logging.WARN``\n\n        c10d (:class:`Optional[int]`):\n            Whether to log c10d communication operations related debug info in PyTorch Distributed components.\n            Default: ``logging.WARN``\n\n        ddp (:class:`Optional[int]`):\n            Whether to log debug info related to ``DistributedDataParallel``(DDP) from PyTorch Distributed components.\n            Default: ``logging.WARN``\n\n        fsdp (:class:`Optional[int]`):\n            Whether to log debug info related to ``FullyShardedDataParallel``(FSDP) in PyTorch Distributed components.\n            Default: ``logging.WARN``\n\n        dtensor (:class:`Optional[int]`):\n            Whether to log debug info related to ``DTensor``(DTensor) in PyTorch Distributed components.\n            Default: ``logging.WARN``\n\n        onnx (:class:`Optional[int]`):\n            The log level for the ONNX exporter component. Default: ``logging.WARN``\n\n        bytecode (:class:`bool`):\n            Whether to emit the original and generated bytecode from TorchDynamo.\n            Default: ``False``\n\n        aot_graphs (:class:`bool`):\n            Whether to emit the graphs generated by AOTAutograd. Default: ``False``\n\n        aot_joint_graph (:class:`bool`):\n            Whether to emit the joint forward-backward graph generated by AOTAutograd. Default: ``False``\n\n        ddp_graphs (:class:`bool`):\n            Whether to emit graphs generated by DDPOptimizer. Default: ``False``\n\n        graph (:class:`bool`):\n            Whether to emit the graph captured by TorchDynamo in tabular format.\n            Default: ``False``\n\n        graph_code (:class:`bool`):\n            Whether to emit the python source of the graph captured by TorchDynamo.\n            Default: ``False``\n\n        graph_code_verbose (:class:`bool`):\n            Whether to emit verbose/intermediate FX pass logs for graph code. Default: ``False``\n\n        graph_breaks (:class:`bool`):\n            Whether to emit the graph breaks encountered by TorchDynamo.\n            Default: ``False``\n\n        graph_sizes (:class:`bool`):\n            Whether to emit tensor sizes of the graph captured by TorchDynamo.\n            Default: ``False``\n\n        guards (:class:`bool`):\n            Whether to emit the guards generated by TorchDynamo for each compiled\n            function. Default: ``False``\n\n        recompiles (:class:`bool`):\n            Whether to emit a guard failure reason and message every time\n            TorchDynamo recompiles a function. Default: ``False``\n\n        recompiles_verbose (:class:`bool`):\n            Whether to emit all guard failure reasons when TorchDynamo recompiles\n            a function, even those that are not actually run. Default: ``False``\n\n        trace_source (:class:`bool`):\n            Whether to emit when TorchDynamo begins tracing a new line. Default: ``False``\n\n        trace_call (:class:`bool`):\n            Whether to emit detailed line location when TorchDynamo creates an FX node\n            corresponding to function call. Python 3.11+ only. Default: ``False``\n\n        trace_bytecode (:class:`bool`):\n            Whether to emit bytecode instructions and traced stack state as TorchDynamo\n            traces bytecode. Default: ``False``\n\n        output_code (:class:`bool`):\n            Whether to emit the TorchInductor output code on a per-graph basis. Default: ``False``\n\n        kernel_code (:class:`bool`):\n            Whether to emit the TorchInductor output code on a per-kernel bases. Default: ``False``\n\n        schedule (:class:`bool`):\n            Whether to emit the TorchInductor schedule. Default: ``False``\n\n        perf_hints (:class:`bool`):\n            Whether to emit the TorchInductor perf hints. Default: ``False``\n\n        pre_grad_graphs (:class:`bool`):\n            Whether to emit the graphs before inductor grad passes. Default: ``False``\n\n        post_grad_graphs (:class:`bool`):\n            Whether to emit the graphs generated by after post grad passes. Default: ``False``\n\n        ir_pre_fusion (:class:`bool`):\n            Whether to emit the graphs before inductor fusion passes. Default: ``False``\n\n        ir_post_fusion (:class:`bool`):\n            Whether to emit the graphs after inductor fusion passes. Default: ``False``\n\n        onnx_diagnostics (:class:`bool`):\n            Whether to emit the ONNX exporter diagnostics in logging. Default: ``False``\n\n        fusion (:class:`bool`):\n            Whether to emit detailed Inductor fusion decisions. Default: ``False``\n\n        overlap (:class:`bool`):\n            Whether to emit detailed Inductor compute/comm overlap decisions. Default: ``False``\n\n        sym_node (:class:`bool`):\n            Whether to emit debug info for various SymNode opterations. Default: ``False``\n\n        export (:class:`Optional[int]`):\n            The log level for export. Default: ``logging.WARN``\n\n        benchmarking (:class:`bool`):\n            Whether to emit detailed Inductor benchmarking information. Default: ``False``\n\n        modules (dict):\n            This argument provides an alternate way to specify the above log\n            component and artifact settings, in the format of a keyword args\n            dictionary given as a single argument. There are two cases\n            where this is useful (1) if a new log component or artifact has\n            been registered but a keyword argument for it has not been added\n            to this function and (2) if the log level for an unregistered module\n            needs to be set. This can be done by providing the fully-qualified module\n            name as the key, with the log level as the value. Default: ``None``\n\n        cudagraph_static_inputs (:class:`bool`):\n            Whether to emit debug info for cudagraph static input detection. Default: ``False``\n\n        autotuning (:class:`bool`):\n            Autotuning choice logs, such as kernel source, perf, and tuning parameters. Default: ``False``\n\n        graph_region_expansion (:class:`bool`):\n            Whether to emit the detailed steps of the duplicate graph region tracker expansion algorithm. Default: ``False``\n\n        inductor_metrics (:class:`bool`):\n            Whether to estimate the runtimes of the nodes in a graph and log them to the metrics table. Default: ``False``\n\n        hierarchical_compile (:class:`bool`):\n            Whether to emit debug info for hierarchical compilation. Default: ``False``\n\n    Example::\n\n        >>> # xdoctest: +SKIP\n        >>> import logging\n\n        # The following changes the \"dynamo\" component to emit DEBUG-level\n        # logs, and to emit \"graph_code\" artifacts.\n\n        >>> torch._logging.set_logs(dynamo=logging.DEBUG, graph_code=True)\n\n        # The following enables the logs for a different module\n\n        >>> torch._logging.set_logs(modules={\"unregistered.module.name\": logging.DEBUG})\n    \"\"\"\n    # ignore if env var is set\n    if LOG_ENV_VAR in os.environ:\n        log.warning(\n            \"Using TORCH_LOGS environment variable for log settings, ignoring call to set_logs\"\n        )\n        return\n\n    log_state.clear()\n\n    modules = modules or {}\n\n    def _set_logs(**kwargs) -> None:\n        for alias, val in itertools.chain(kwargs.items(), modules.items()):  # type: ignore[union-attr]\n            if val is None:\n                continue\n\n            if log_registry.is_artifact(alias):\n                if not isinstance(val, bool):\n                    raise ValueError(\n                        f\"Expected bool to enable artifact {alias}, received {val}\"\n                    )\n\n                if val:\n                    log_state.enable_artifact(alias)\n            elif log_registry.is_log(alias) or alias in log_registry.child_log_qnames:\n                if val not in logging._levelToName:\n                    raise ValueError(\n                        f\"Unrecognized log level for log {alias}: {val}, valid level values \"\n                        f\"are: {','.join([str(k) for k in logging._levelToName.keys()])}\"\n                    )\n\n                log_state.enable_log(\n                    log_registry.log_alias_to_log_qnames.get(alias, alias), val\n                )\n            elif _is_valid_module(alias):\n                if not _has_registered_parent(alias):\n                    log_registry.register_log(alias, alias)\n                else:\n                    log_registry.register_child_log(alias)\n                log_state.enable_log(\n                    log_registry.log_alias_to_log_qnames.get(alias, alias), val\n                )\n            else:\n                raise ValueError(\n                    f\"Unrecognized log or artifact name passed to set_logs: {alias}\"\n                )\n\n        _init_logs()\n\n    _set_logs(\n        torch=all,\n        dynamo=dynamo,\n        aot=aot,\n        autograd=autograd,\n        inductor=inductor,\n        dynamic=dynamic,\n        bytecode=bytecode,\n        aot_graphs=aot_graphs,\n        aot_joint_graph=aot_joint_graph,\n        ddp_graphs=ddp_graphs,\n        distributed=distributed,\n        c10d=c10d,\n        ddp=ddp,\n        fsdp=fsdp,\n        dtensor=dtensor,\n        graph=graph,\n        graph_code=graph_code,\n        graph_code_verbose=graph_code_verbose,\n        graph_breaks=graph_breaks,\n        graph_sizes=graph_sizes,\n        guards=guards,\n        recompiles=recompiles,\n        recompiles_verbose=recompiles_verbose,\n        trace_source=trace_source,\n        trace_call=trace_call,\n        trace_bytecode=trace_bytecode,\n        output_code=output_code,\n        kernel_code=kernel_code,\n        schedule=schedule,\n        perf_hints=perf_hints,\n        pre_grad_graphs=pre_grad_graphs,\n        post_grad_graphs=post_grad_graphs,\n        ir_pre_fusion=ir_pre_fusion,\n        ir_post_fusion=ir_post_fusion,\n        onnx=onnx,\n        onnx_diagnostics=onnx_diagnostics,\n        fusion=fusion,\n        overlap=overlap,\n        sym_node=sym_node,\n        export=export,\n        cudagraphs=cudagraphs,\n        compiled_autograd=compiled_autograd,\n        compiled_autograd_verbose=compiled_autograd_verbose,\n        cudagraph_static_inputs=cudagraph_static_inputs,\n        benchmarking=benchmarking,\n        autotuning=autotuning,\n        graph_region_expansion=graph_region_expansion,\n        inductor_metrics=inductor_metrics,\n        hierarchical_compile=hierarchical_compile,\n        compute_dependencies=compute_dependencies,\n    )\n\n\ndef get_loggers() -> list[logging.Logger]:\n    \"\"\"\n    Returns: a list of all registered loggers\n    \"\"\"\n    return [logging.getLogger(qname) for qname in log_registry.get_log_qnames()]\n\n\ndef register_log(setting_name, log_name) -> None:\n    \"\"\"\n    Enables a log to be controlled by the env var and user API with the setting_name\n    Args:\n        setting_name:  the shorthand name used in the env var and user API\n        log_name:  the log name that the setting_name is associated with\n    \"\"\"\n    log_registry.register_log(setting_name, log_name)\n\n\ndef register_artifact(\n    setting_name, description, visible=False, off_by_default=False, log_format=None\n) -> None:\n    \"\"\"\n    Enables an artifact to be controlled by the env var and user API with name\n    Args:\n        setting_name: the shorthand name used in the env var and user API\n        description: A description of what this outputs\n        visible: Whether it gets suggested to users by default\n        off_by_default: whether this artifact should be logged when the ancestor loggers\n            are enabled at level DEBUG\n    \"\"\"\n    log_registry.register_artifact_name(\n        setting_name, description, visible, off_by_default, log_format\n    )\n\n\ndef getArtifactLogger(module_qname, artifact_name) -> logging.Logger:\n    if artifact_name not in log_registry.artifact_names:\n        raise ValueError(\n            f\"Artifact name: {repr(artifact_name)} not registered,\"\n            f\"please call register_artifact({repr(artifact_name)}) in torch._logging.registrations.\"\n        )\n    qname = module_qname + f\".__{artifact_name}\"\n    log = logging.getLogger(qname)\n    log.artifact_name = artifact_name  # type: ignore[attr-defined]\n    log_registry.register_artifact_log(qname)\n    configure_artifact_log(log)\n    return log\n\n\nINCR_VERBOSITY_CHAR = \"+\"\nDECR_VERBOSITY_CHAR = \"-\"\nVERBOSITY_REGEX = (\n    \"(\"\n    + \"|\".join([re.escape(INCR_VERBOSITY_CHAR), re.escape(DECR_VERBOSITY_CHAR)])\n    + \"?)\"\n)\n\n\ndef configure_artifact_log(log) -> None:\n    # If the artifact is off by default, then it should only be logged when explicitly\n    # enabled; set propagate to False so that this artifact is not propagated\n    # to its ancestor logger\n    if log_registry.is_off_by_default(log.artifact_name):\n        log.propagate = False\n\n    # enable artifact logging when explicitly enabled\n    if log_state.is_artifact_enabled(log.artifact_name):\n        log.setLevel(logging.DEBUG)\n        log.propagate = True\n\n\n# match a comma separated list of loggable names (whitespace allowed after commas)\ndef _gen_settings_regex():\n    return re.compile(r\"((\\+|-)?[\\w\\.]+,\\s*)*(\\+|-)?[\\w\\.]+?\")\n\n\ndef _validate_settings(settings):\n    return re.fullmatch(_gen_settings_regex(), settings) is not None\n\n\ndef help_message(verbose=False):\n    def pad_to(s, length=30):\n        assert len(s) <= length\n        return s + \" \" * (length - len(s))\n\n    if verbose:\n        printed_artifacts = log_registry.artifact_names\n    else:\n        printed_artifacts = log_registry.visible_artifacts\n    if verbose:\n        heading = \"All registered names\"\n    else:\n        heading = \"Visible registered names (use TORCH_LOGS='+help' for full list)\"\n    lines = (\n        [\"all\"]\n        + sorted(log_registry.log_alias_to_log_qnames.keys())\n        + sorted(\n            [\n                f\"{pad_to(name)}\\t{log_registry.artifact_descriptions[name]}\"\n                for name in printed_artifacts\n            ]\n        )\n    )\n    setting_info = \"  \" + \"\\n  \".join(lines)\n    examples = \"\"\"\nExamples:\n  TORCH_LOGS=\"+dynamo,aot\" will set the log level of TorchDynamo to\n  logging.DEBUG and AOT to logging.INFO\n\n  TORCH_LOGS=\"-dynamo,+inductor\" will set the log level of TorchDynamo to\n  logging.ERROR and TorchInductor to logging.DEBUG\n\n  TORCH_LOGS=\"aot_graphs\" will enable the aot_graphs artifact\n\n  TORCH_LOGS=\"+dynamo,schedule\" will enable set the log level of TorchDynamo\n  to logging.DEBUG and enable the schedule artifact\n\n  TORCH_LOGS=\"+some.random.module,schedule\" will set the log level of\n  some.random.module to logging.DEBUG and enable the schedule artifact\n\n  TORCH_LOGS_FORMAT=\"%(levelname)s: %(message)s\" or any provided format\n  string will set the output format\n  Valid keys are \"levelname\", \"message\", \"pathname\", \"levelno\", \"lineno\",\n  \"filename\" and \"name\".\n\n  TORCH_LOGS_OUT=/tmp/output.txt will output the logs to /tmp/output.txt as\n  well. This is useful when the output is long.\n\"\"\"  # flake8: noqa: B950\n    msg = f\"\"\"\nTORCH_LOGS Info\n{examples}\n\n{heading}\n{setting_info}\n\"\"\"\n    return msg\n\n\ndef _invalid_settings_err_msg(settings, verbose=False):\n    valid_settings = (\n        [\"all\"]\n        + list(log_registry.log_alias_to_log_qnames.keys())\n        + list(log_registry.artifact_names)\n    )\n    valid_settings = \", \".join(sorted(valid_settings))\n    msg = f\"\"\"\nInvalid log settings: {settings}, must be a comma separated list of fully\nqualified module names, registered log names or registered artifact names.\nFor more info on various settings, try TORCH_LOGS=\"help\"\nValid settings:\n{valid_settings}\n\"\"\"\n    return msg\n\n\n@functools.lru_cache\ndef _parse_log_settings(settings):\n    if settings == \"\":\n        return {}\n\n    if settings == \"help\":\n        raise ValueError(help_message(verbose=False))\n    elif settings == \"+help\":\n        raise ValueError(help_message(verbose=True))\n    if not _validate_settings(settings):\n        raise ValueError(_invalid_settings_err_msg(settings))\n\n    settings = re.sub(r\"\\s+\", \"\", settings)\n    log_names = settings.split(\",\")\n\n    def get_name_level_pair(name):\n        clean_name = name.replace(INCR_VERBOSITY_CHAR, \"\")\n        clean_name = clean_name.replace(DECR_VERBOSITY_CHAR, \"\")\n\n        if name[0] == INCR_VERBOSITY_CHAR:\n            level = logging.DEBUG\n        elif name[0] == DECR_VERBOSITY_CHAR:\n            level = logging.ERROR\n        else:\n            level = logging.INFO\n\n        return clean_name, level\n\n    log_state = LogState()\n\n    for name in log_names:\n        name, level = get_name_level_pair(name)\n\n        if name == \"all\":\n            name = \"torch\"\n\n        if log_registry.is_log(name):\n            assert level is not None\n            log_qnames = log_registry.log_alias_to_log_qnames[name]\n            log_state.enable_log(log_qnames, level)\n        elif log_registry.is_artifact(name):\n            log_state.enable_artifact(name)\n        elif _is_valid_module(name):\n            if not _has_registered_parent(name):\n                log_registry.register_log(name, name)\n            else:\n                log_registry.register_child_log(name)\n            log_state.enable_log(name, level)\n        else:\n            raise ValueError(_invalid_settings_err_msg(settings))\n\n    return log_state\n\n\ndef _is_valid_module(qname):\n    spec = importlib.util.find_spec(qname)\n    return spec is not None\n\n\ndef _update_log_state_from_env() -> None:\n    global log_state\n    log_setting = os.environ.get(LOG_ENV_VAR, None)\n    if log_setting is not None:\n        log_state = _parse_log_settings(log_setting)\n\n\ndef _has_registered_parent(log_qname) -> bool:\n    cur_log = logging.getLogger(log_qname)\n\n    registered_log_qnames = log_registry.get_log_qnames()\n\n    while cur_log.parent:\n        if cur_log.name in registered_log_qnames:\n            return True\n        cur_log = cur_log.parent\n\n    return False\n\n\ndef make_module_path_relative(abs_path):\n    \"\"\"\n    Given an absolute filepath corresponding to a Python module which was\n    loaded via normal import mechanisms using sys.path, convert it into\n    a relative path relative to one of the Python search paths.\n    \"\"\"\n\n    abs_path = pathlib.Path(abs_path).resolve()\n\n    for path in sys.path:\n        try:\n            rel_path = abs_path.relative_to(path)\n        except ValueError:\n            continue\n        else:\n            return str(rel_path)\n\n    return str(abs_path)\n\n\n# apply custom formats to artifacts when necessary\nclass TorchLogsFormatter(logging.Formatter):\n    def __init__(\n        self, *, trace: bool = False, trace_id_filter: Optional[set[str]] = None\n    ) -> None:\n        super().__init__()\n        self._is_trace = trace\n        self._trace_id_filter = trace_id_filter\n\n    def format(self, record):\n        artifact_name = getattr(logging.getLogger(record.name), \"artifact_name\", None)\n        if artifact_name is not None:\n            artifact_formatter = log_registry.artifact_log_formatters.get(\n                artifact_name, None\n            )\n            if artifact_formatter is not None:\n                return artifact_formatter.format(record)\n\n        record.message = record.getMessage()\n        record.asctime = self.formatTime(record, \"%m%d %H:%M:%S\")\n\n        # exception handling - copied from logging.Formatter.format\n        s = record.message\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + record.exc_text\n        if record.stack_info:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + self.formatStack(record.stack_info)\n\n        record.rankprefix = \"\"\n        if not self._is_trace and dist.is_available() and dist.is_initialized():\n            record.rankprefix = f\"[rank{dist.get_rank()}]:\"\n\n        record.traceid = \"\"\n        if (\n            not self._is_trace\n            and (trace_id := torch._guards.CompileContext.current_trace_id())\n            is not None\n        ):\n            record.traceid = f\" [{trace_id}]\"\n\n        glog_level_to_abbr = {\n            \"DEBUG\": \"V\",  # V is for VERBOSE in glog\n            \"INFO\": \"I\",\n            \"WARNING\": \"W\",\n            \"ERROR\": \"E\",\n            \"CRITICAL\": \"C\",\n        }\n\n        shortlevel = glog_level_to_abbr.get(record.levelname, record.levelname)\n\n        record.artifactprefix = \"\"\n        if artifact_name is not None:\n            record.artifactprefix = f\" [__{artifact_name}]\"\n\n        filepath = make_module_path_relative(record.pathname)\n\n        if (\n            self._trace_id_filter\n            and record.traceid.strip() not in self._trace_id_filter\n        ):\n            return \"\"\n\n        prefix = (\n            f\"{record.rankprefix}{shortlevel}{record.asctime}.{int(record.msecs * 1000):06d} {record.process} \"\n            f\"{filepath}:\"\n            f\"{record.lineno}]{record.traceid}{record.artifactprefix}\"\n        )\n        if self._is_trace:\n            assert s == \"\"\n            try:\n                r = f\"{prefix} {json.dumps(record.metadata)}\"\n            except TypeError:\n                log.warning(\"failing metadata: %r\", record.metadata)\n                raise\n            if record.payload is not None:\n                r += \"\".join(f\"\\n\\t{l}\" for l in record.payload.split(\"\\n\"))\n            return r\n        else:\n            lines = s.split(\"\\n\")\n            return \"\\n\".join(f\"{prefix} {l}\" for l in lines)\n\n\ndef _default_formatter():\n    fmt = os.environ.get(LOG_FORMAT_ENV_VAR, None)\n    trace_id_filter = {\n        item.strip()\n        for item in os.environ.get(LOG_TRACE_ID_FILTER, \"\").split(\",\")\n        if item.strip()\n    }\n    if fmt is None:\n        return TorchLogsFormatter(trace_id_filter=trace_id_filter)\n    else:\n        if fmt in (\"short\", \"basic\"):\n            fmt = logging.BASIC_FORMAT\n        return logging.Formatter(fmt)\n\n\nDEFAULT_FORMATTER = _default_formatter()\n\n\ndef _setup_handlers(create_handler_fn, log) -> None:\n    debug_handler = _track_handler(create_handler_fn())\n    debug_handler.setFormatter(DEFAULT_FORMATTER)\n    debug_handler.setLevel(logging.DEBUG)\n    log.addHandler(debug_handler)\n\n\nhandlers = WeakSet()  # type: ignore[var-annotated]\n\n\n# mark handlers that we've created\n# so we don't modify user handlers\ndef _track_handler(handler):\n    handlers.add(handler)\n    return handler\n\n\ndef _is_torch_handler(handler):\n    return handler in handlers\n\n\n# clears all torch handlers on specified loggers\ndef _clear_handlers(log) -> None:\n    to_remove = [handler for handler in log.handlers if _is_torch_handler(handler)]\n    for handler in to_remove:\n        log.removeHandler(handler)\n\n\ndef _reset_logs() -> None:\n    # reset all registered logs\n    for log_qname in log_registry.get_log_qnames():\n        log = logging.getLogger(log_qname)\n        log.setLevel(logging.WARNING)\n        log.propagate = False\n        _clear_handlers(log)\n\n    # reset all artifact and child logs\n    for artifact_log_qname in itertools.chain(\n        log_registry.get_artifact_log_qnames(), log_registry.get_child_log_qnames()\n    ):\n        log = logging.getLogger(artifact_log_qname)\n        log.setLevel(logging.NOTSET)\n        log.propagate = True\n\n    trace_log.propagate = False\n    _clear_handlers(trace_log)\n\n\ndef _get_log_state():\n    return log_state\n\n\ndef _set_log_state(state) -> None:\n    global log_state\n    log_state = state\n\n\ndef _init_logs(log_file_name=None) -> None:\n    global GET_DTRACE_STRUCTURED\n\n    _reset_logs()\n    _update_log_state_from_env()\n\n    out = os.environ.get(LOG_OUT_ENV_VAR, None)\n    if out is not None:\n        log_file_name = out\n\n    # First, reset all known (registered) loggers to NOTSET, so that they\n    # respect their parent log level\n    for log_qname in log_registry.get_log_qnames():\n        # But not the top level torch level: this defaults to WARNING so\n        # that our log messages don't leak to the lower levels\n        if log_qname == \"torch\":\n            continue\n        log = logging.getLogger(log_qname)\n        log.setLevel(logging.NOTSET)\n\n    # Now, for all loggers which the user requested to have non-standard\n    # logging behavior, modify their log levels\n    for log_qname, level in log_state.get_log_level_pairs():\n        log = logging.getLogger(log_qname)\n        log.setLevel(level)\n\n    # Finally, setup handlers for all registered loggers\n    for log_qname in log_registry.get_log_qnames():\n        log = logging.getLogger(log_qname)\n        _setup_handlers(\n            logging.StreamHandler,\n            log,\n        )\n\n        if log_file_name is not None:\n            _setup_handlers(\n                lambda: logging.FileHandler(log_file_name),\n                log,\n            )\n\n    # configure artifact loggers, note: this must happen last\n    # since the levels of ancestor loggers are taken into account\n    for artifact_log_qname in log_registry.get_artifact_log_qnames():\n        log = logging.getLogger(artifact_log_qname)\n        configure_artifact_log(log)\n\n    # Setup handler for the special trace_log, with different default\n    # configuration\n    trace_dir_name = os.environ.get(TRACE_ENV_VAR, None)\n\n    if dtrace_dir_name := os.environ.get(DTRACE_ENV_VAR, None):\n        GET_DTRACE_STRUCTURED = True\n        trace_dir_name = dtrace_dir_name\n\n    # This handler may remove itself if trace_dir_name is None and we are not\n    # actually in an FB environment.  This allows us to defer actually\n    # initializing it until we actually need to log anything.  This is\n    # important because JK initializes a C++ singleton, which will pork our\n    # process if we subsequently fork.\n    global LOG_TRACE_HANDLER\n    if LOG_TRACE_HANDLER is None:\n        LOG_TRACE_HANDLER = LazyTraceHandler(trace_dir_name)\n    # This log is ALWAYS at debug level.  We will additionally test if there\n    # are any handlers before deciding to actually call logging on this.  Do\n    # not manually call\n    trace_log.setLevel(logging.DEBUG)\n    trace_log_handler = _track_handler(LOG_TRACE_HANDLER)\n    trace_log_handler.setFormatter(TorchLogsFormatter(trace=True))\n    trace_log.addHandler(trace_log_handler)\n\n\nclass LazyTraceHandler(logging.StreamHandler):\n    \"\"\"Like FileHandler, but the file is allocated lazily only upon the first log message\"\"\"\n\n    def __init__(self, root_dir: Optional[str]) -> None:\n        # This is implemented in the same way that delay is implemented on\n        # FileHandler\n        self.root_dir = root_dir\n        logging.Handler.__init__(self)\n        self.stream = None\n        self._builtin_open = open\n\n    # cloned from FileHandler in cpython\n    def close(self) -> None:\n        self.acquire()\n        try:\n            try:\n                if self.stream:\n                    try:\n                        self.flush()\n                    finally:\n                        stream = self.stream\n                        self.stream = None\n                        if hasattr(stream, \"close\"):\n                            stream.close()\n            finally:\n                # Issue #19523: call unconditionally to\n                # prevent a handler leak when delay is set\n                # Also see Issue #42378: we also rely on\n                # self._closed being set to True there\n                logging.StreamHandler.close(self)\n        finally:\n            self.release()\n\n    def emit(self, record) -> None:\n        if self.stream is None:\n            if self.root_dir is None:\n                TRACE_LOG_DIR = \"/logs\"\n\n                import torch.version as torch_version\n\n                if (\n                    hasattr(torch_version, \"git_version\")\n                    and os.getenv(\"MAST_HPC_JOB_NAME\") is None\n                ):\n                    log.info(\n                        \"LazyTraceHandler: disabled because not fbcode or conda on mast\"\n                    )\n                elif not torch._utils_internal.justknobs_check(\"pytorch/trace:enable\"):\n                    log.info(\n                        \"LazyTraceHandler: disabled because justknobs_check('pytorch/trace:enable') returned False\"\n                    )\n                elif not os.path.exists(TRACE_LOG_DIR):\n                    log.info(\n                        \"LazyTraceHandler: disabled because %s does not exist\",\n                        TRACE_LOG_DIR,\n                    )\n                elif not os.access(TRACE_LOG_DIR, os.W_OK):\n                    log.info(\n                        \"LazyTraceHandler: disabled because %s is not writeable\",\n                        TRACE_LOG_DIR,\n                    )\n                else:\n                    self.root_dir = TRACE_LOG_DIR\n\n            if self.root_dir is not None:\n                os.makedirs(self.root_dir, exist_ok=True)\n                ranksuffix = \"\"\n                if dist.is_available() and dist.is_initialized():\n                    ranksuffix = f\"rank_{dist.get_rank()}_\"\n                self.stream = tempfile.NamedTemporaryFile(\n                    mode=\"w+\",\n                    suffix=\".log\",\n                    prefix=f\"dedicated_log_torch_trace_{ranksuffix}\",\n                    dir=self.root_dir,\n                    delete=False,\n                )\n                log.info(\"LazyTraceHandler: logging to %s\", self.stream.name)\n            else:\n                # We go poof, remove and no-op\n                trace_log.removeHandler(self)\n                return\n        if self.stream:\n            super().emit(record)\n\n\n@functools.cache\ndef warning_once(logger_obj, *args, **kwargs) -> None:\n    \"\"\"\n    This function is similar to `logger.warning()`, but will emit the warning with the same message only once\n    Note: The cache is for the function arguments, so 2 different callers using the same arguments will hit the cache.\n    The assumption here is that all warning messages are unique across the code. If they aren't then need to switch to\n    another type of cache that includes the caller frame information in the hashing function.\n    \"\"\"\n    logger_obj.warning(*args, **kwargs)\n\n\ndef safe_grad_filter(message, category, filename, lineno, file=None, line=None) -> bool:\n    return \"The .grad attribute of a Tensor\" not in str(message)\n\n\ndef user_warning_filter(\n    message, category, filename, lineno, file=None, line=None\n) -> bool:\n    return not category == UserWarning\n\n\n@contextlib.contextmanager\ndef hide_warnings(filter_fn=lambda *args, **kwargs: True):\n    \"\"\"\n    A context manager that temporarily suppresses warnings,\n    using public API: https://docs.python.org/3/library/warnings.html#warnings.showwarning.\n\n    Useful to hide warnings without mutating warnings module state, see:\n    https://github.com/pytorch/pytorch/issues/128427#issuecomment-2161496162.\n\n    NOTE: Warnings issued under this context will still be cached in the __warningregistry__\n    and count towards the once/default rule. So you should NEVER use this on a user-land function.\n\n    Filter must implement the showwarning API:\n    def filter_fn(message, category, filename, lineno, file=None, line=None) -> bool:\n        return True  # show this warning entry\n    \"\"\"\n    prior = warnings.showwarning\n\n    def _showwarning(*args, **kwargs):\n        if filter_fn(*args, **kwargs):\n            prior(*args, **kwargs)\n\n    try:\n        warnings.showwarning = _showwarning\n        yield\n    finally:\n        warnings.showwarning = prior\n\n\nclass LazyString(Generic[_P]):\n    def __init__(\n        self, func: Callable[_P, str], *args: _P.args, **kwargs: _P.kwargs\n    ) -> None:\n        self.func = func\n        self.args = args\n        self.kwargs = kwargs\n\n    def __str__(self) -> str:\n        return self.func(*self.args, **self.kwargs)\n\n\n# Logs the time it takes to do structured logging by frame/compile id\n# key is always {frame_id}_{frame_compile_id}\nstructured_logging_overhead: dict[str, float] = defaultdict(float)\n\n\ndef add_structured_logging_overhead(time_spent: float) -> None:\n    global structured_logging_overhead\n    key = None\n    if (trace_id := torch._guards.CompileContext.current_trace_id()) is not None:\n        frame_id = trace_id.compile_id.frame_id\n        frame_compile_id = trace_id.compile_id.frame_compile_id\n        # Why not trace_id.attempt, like structured logging?\n        # We aggregate across all attempts because\n        # a compilation metric is logged per successful attempt\n        key = f\"{frame_id}_{frame_compile_id}\"\n    # TODO: deal with structured logging that occurs outside of specific compile ids\n    # It's hard to figure out where we would log that if we want it in compilation metrics\n    # itself.\n    if key is not None:\n        key = str(key)\n        structured_logging_overhead[key] += time_spent\n\n\ndef get_structured_logging_overhead() -> Optional[float]:\n    key = None\n    if (trace_id := torch._guards.CompileContext.current_trace_id()) is not None:\n        frame_id = trace_id.compile_id.frame_id\n        frame_compile_id = trace_id.compile_id.frame_compile_id\n        key = f\"{frame_id}_{frame_compile_id}\"\n    if key is not None:\n        return structured_logging_overhead.get(key)\n    else:\n        return None\n\n\ndef trace_structured_artifact(\n    name: str,  # this will go in metadata\n    encoding: str,\n    payload_fn: Callable[[], Optional[Union[str, object]]] = lambda: None,\n) -> None:\n    trace_structured(\n        \"artifact\",\n        metadata_fn=lambda: {\n            \"name\": name,\n            \"encoding\": encoding,\n        },\n        payload_fn=payload_fn,\n    )\n\n\ndef trace_structured(\n    name: str,\n    # NB: metadata expected to be dict so adding more info is forward compatible\n    # Tuple[str, int] is a special case for string interning\n    metadata_fn: Callable[[], Union[dict[str, Any], tuple[str, int]]] = dict,\n    *,\n    payload_fn: Callable[[], Optional[Union[str, object]]] = lambda: None,\n    suppress_context: bool = False,\n    expect_trace_id: bool = True,  # Whether or not we expect to have a current trace id\n    record_logging_overhead: bool = True,  # Whether or not to record the time spent on structured logging\n    compile_id: Optional[CompileId] = None,  # Optional if unavailable in the trace\n) -> None:\n    \"\"\"\n    metadata is an arbitrary JSON compatible struct, but it's expected to not be\n    too long (e.g., less than 1MB)\n\n    payload is an arbitrary string, which can be arbitrarily long (but expected to have\n    newlines so no lines are too long)\n    \"\"\"\n    assert name not in [\n        \"rank\",\n        \"compiled_autograd_id\",\n        \"frame_id\",\n        \"frame_compile_id\",\n        \"attempt\",\n        \"severity\",\n        \"timestamp\",\n        \"pathname\",\n        \"thread\",\n    ]\n    assert callable(metadata_fn), (\n        f\"metadata_fn should be callable, but got {type(metadata_fn)}\"\n    )\n    assert callable(payload_fn), (\n        f\"payload_fn should be callable, but got {type(payload_fn)}\"\n    )\n    # trace_log never propagates and is ALWAYS DEBUG, so also check that there\n    # are handlers instead of checking the log level\n    if trace_log.handlers:\n        start_time = time.time_ns()\n        record: dict[str, object] = {}\n        record[name] = metadata_fn()\n        if not suppress_context:\n            # TODO: Actually, the rank probably should just be emitted once at\n            # the top, and not repeatedly spammed in all the logs, since it\n            # never changes and we assume no interleaving\n            if dist.is_available() and dist.is_initialized():\n                record[\"rank\"] = dist.get_rank()\n\n            trace_id = torch._guards.CompileContext.current_trace_id()\n            if expect_trace_id and trace_id is None and compile_id is None:\n                # Record the stack of the log call to better diagnose why we\n                # don't have a frame id for it\n                record[\"stack\"] = torch._logging.structured.from_traceback(\n                    CapturedTraceback.extract(skip=1).summary()\n                )\n            else:\n                cid = trace_id.compile_id if trace_id else compile_id\n                if cid is not None:\n                    if cid.compiled_autograd_id is not None:\n                        record[\"compiled_autograd_id\"] = cid.compiled_autograd_id\n                    if cid.frame_id is not None:\n                        record[\"frame_id\"] = cid.frame_id\n                    if cid.frame_compile_id is not None:\n                        record[\"frame_compile_id\"] = cid.frame_compile_id\n                if trace_id:\n                    record[\"attempt\"] = trace_id.attempt\n\n        payload = payload_fn()\n        if payload is not None:\n            if not isinstance(payload, str):\n                if isinstance(payload, list):\n                    # special case to look better\n                    payload = \"[\\n\" + \",\\n\".join(json.dumps(i) for i in payload) + \"\\n]\"\n                else:\n\n                    def json_default(obj):\n                        # Sets aren't json serializable\n                        if isinstance(obj, set):\n                            return list(obj)\n                        raise TypeError(\n                            f\"Object of type {type(obj)} is not JSON serializable\"\n                        )\n\n                    # force newlines so we are unlikely to overflow line limit\n                    payload = json.dumps(payload, default=json_default, indent=0)\n            h = hashlib.md5(usedforsecurity=False)\n            h.update(payload.encode(\"utf-8\"))\n            record[\"has_payload\"] = h.hexdigest()\n        trace_log.debug(\n            \"\", extra={\"metadata\": record, \"payload\": payload}, stacklevel=2\n        )\n        log_trace_structured_event(name, record)\n\n        if record_logging_overhead:\n            # Convert to seconds from nanoseconds, add it to the frame compile total\n            structured_logging_overhead_s = (time.time_ns() - start_time) / 1e9\n            add_structured_logging_overhead(structured_logging_overhead_s)\n\n\ndef dtrace_structured(\n    name: str,\n    # NB: metadata expected to be dict so adding more info is forward compatible\n    # Tuple[str, int] is a special case for string interning\n    metadata_fn: Callable[[], Union[dict[str, Any], tuple[str, int]]] = dict,\n    *,\n    payload_fn: Callable[[], Optional[Union[str, object]]] = lambda: None,\n    suppress_context: bool = False,\n    expect_trace_id: bool = False,  # Whether or not we expect to have a current trace id\n    record_logging_overhead: bool = True,  # Whether or not to record the time spent on structured logging\n) -> None:\n    \"\"\"\n    For logging more detailed information used for debugging. This may result in\n    the program becoming slow.\n    \"\"\"\n    if GET_DTRACE_STRUCTURED:\n        trace_structured(\n            name,\n            metadata_fn,\n            payload_fn=payload_fn,\n            suppress_context=suppress_context,\n            expect_trace_id=expect_trace_id,\n            record_logging_overhead=record_logging_overhead,\n        )\n\n\nimport torch._guards\nimport torch._utils_internal\nimport torch.distributed as dist\n", 1390], "/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py": ["#\n# Module providing various facilities to other parts of the package\n#\n# multiprocessing/util.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# Licensed to PSF under a Contributor Agreement.\n#\n\nimport os\nimport itertools\nimport sys\nimport weakref\nimport atexit\nimport threading        # we want threading to install it's\n                        # cleanup function before multiprocessing does\nfrom subprocess import _args_from_interpreter_flags\n\nfrom . import process\n\n__all__ = [\n    'sub_debug', 'debug', 'info', 'sub_warning', 'get_logger',\n    'log_to_stderr', 'get_temp_dir', 'register_after_fork',\n    'is_exiting', 'Finalize', 'ForkAwareThreadLock', 'ForkAwareLocal',\n    'close_all_fds_except', 'SUBDEBUG', 'SUBWARNING',\n    ]\n\n#\n# Logging\n#\n\nNOTSET = 0\nSUBDEBUG = 5\nDEBUG = 10\nINFO = 20\nSUBWARNING = 25\n\nLOGGER_NAME = 'multiprocessing'\nDEFAULT_LOGGING_FORMAT = '[%(levelname)s/%(processName)s] %(message)s'\n\n_logger = None\n_log_to_stderr = False\n\ndef sub_debug(msg, *args):\n    if _logger:\n        _logger.log(SUBDEBUG, msg, *args, stacklevel=2)\n\ndef debug(msg, *args):\n    if _logger:\n        _logger.log(DEBUG, msg, *args, stacklevel=2)\n\ndef info(msg, *args):\n    if _logger:\n        _logger.log(INFO, msg, *args, stacklevel=2)\n\ndef sub_warning(msg, *args):\n    if _logger:\n        _logger.log(SUBWARNING, msg, *args, stacklevel=2)\n\ndef get_logger():\n    '''\n    Returns logger used by multiprocessing\n    '''\n    global _logger\n    import logging\n\n    logging._acquireLock()\n    try:\n        if not _logger:\n\n            _logger = logging.getLogger(LOGGER_NAME)\n            _logger.propagate = 0\n\n            # XXX multiprocessing should cleanup before logging\n            if hasattr(atexit, 'unregister'):\n                atexit.unregister(_exit_function)\n                atexit.register(_exit_function)\n            else:\n                atexit._exithandlers.remove((_exit_function, (), {}))\n                atexit._exithandlers.append((_exit_function, (), {}))\n\n    finally:\n        logging._releaseLock()\n\n    return _logger\n\ndef log_to_stderr(level=None):\n    '''\n    Turn on logging and add a handler which prints to stderr\n    '''\n    global _log_to_stderr\n    import logging\n\n    logger = get_logger()\n    formatter = logging.Formatter(DEFAULT_LOGGING_FORMAT)\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    if level:\n        logger.setLevel(level)\n    _log_to_stderr = True\n    return _logger\n\n\n# Abstract socket support\n\ndef _platform_supports_abstract_sockets():\n    if sys.platform == \"linux\":\n        return True\n    if hasattr(sys, 'getandroidapilevel'):\n        return True\n    return False\n\n\ndef is_abstract_socket_namespace(address):\n    if not address:\n        return False\n    if isinstance(address, bytes):\n        return address[0] == 0\n    elif isinstance(address, str):\n        return address[0] == \"\\0\"\n    raise TypeError(f'address type of {address!r} unrecognized')\n\n\nabstract_sockets_supported = _platform_supports_abstract_sockets()\n\n#\n# Function returning a temp directory which will be removed on exit\n#\n\ndef _remove_temp_dir(rmtree, tempdir):\n    def onerror(func, path, err_info):\n        if not issubclass(err_info[0], FileNotFoundError):\n            raise\n    rmtree(tempdir, onerror=onerror)\n\n    current_process = process.current_process()\n    # current_process() can be None if the finalizer is called\n    # late during Python finalization\n    if current_process is not None:\n        current_process._config['tempdir'] = None\n\ndef get_temp_dir():\n    # get name of a temp directory which will be automatically cleaned up\n    tempdir = process.current_process()._config.get('tempdir')\n    if tempdir is None:\n        import shutil, tempfile\n        tempdir = tempfile.mkdtemp(prefix='pymp-')\n        info('created temp directory %s', tempdir)\n        # keep a strong reference to shutil.rmtree(), since the finalizer\n        # can be called late during Python shutdown\n        Finalize(None, _remove_temp_dir, args=(shutil.rmtree, tempdir),\n                 exitpriority=-100)\n        process.current_process()._config['tempdir'] = tempdir\n    return tempdir\n\n#\n# Support for reinitialization of objects when bootstrapping a child process\n#\n\n_afterfork_registry = weakref.WeakValueDictionary()\n_afterfork_counter = itertools.count()\n\ndef _run_after_forkers():\n    items = list(_afterfork_registry.items())\n    items.sort()\n    for (index, ident, func), obj in items:\n        try:\n            func(obj)\n        except Exception as e:\n            info('after forker raised exception %s', e)\n\ndef register_after_fork(obj, func):\n    _afterfork_registry[(next(_afterfork_counter), id(obj), func)] = obj\n\n#\n# Finalization using weakrefs\n#\n\n_finalizer_registry = {}\n_finalizer_counter = itertools.count()\n\n\nclass Finalize(object):\n    '''\n    Class which supports object finalization using weakrefs\n    '''\n    def __init__(self, obj, callback, args=(), kwargs=None, exitpriority=None):\n        if (exitpriority is not None) and not isinstance(exitpriority,int):\n            raise TypeError(\n                \"Exitpriority ({0!r}) must be None or int, not {1!s}\".format(\n                    exitpriority, type(exitpriority)))\n\n        if obj is not None:\n            self._weakref = weakref.ref(obj, self)\n        elif exitpriority is None:\n            raise ValueError(\"Without object, exitpriority cannot be None\")\n\n        self._callback = callback\n        self._args = args\n        self._kwargs = kwargs or {}\n        self._key = (exitpriority, next(_finalizer_counter))\n        self._pid = os.getpid()\n\n        _finalizer_registry[self._key] = self\n\n    def __call__(self, wr=None,\n                 # Need to bind these locally because the globals can have\n                 # been cleared at shutdown\n                 _finalizer_registry=_finalizer_registry,\n                 sub_debug=sub_debug, getpid=os.getpid):\n        '''\n        Run the callback unless it has already been called or cancelled\n        '''\n        try:\n            del _finalizer_registry[self._key]\n        except KeyError:\n            sub_debug('finalizer no longer registered')\n        else:\n            if self._pid != getpid():\n                sub_debug('finalizer ignored because different process')\n                res = None\n            else:\n                sub_debug('finalizer calling %s with args %s and kwargs %s',\n                          self._callback, self._args, self._kwargs)\n                res = self._callback(*self._args, **self._kwargs)\n            self._weakref = self._callback = self._args = \\\n                            self._kwargs = self._key = None\n            return res\n\n    def cancel(self):\n        '''\n        Cancel finalization of the object\n        '''\n        try:\n            del _finalizer_registry[self._key]\n        except KeyError:\n            pass\n        else:\n            self._weakref = self._callback = self._args = \\\n                            self._kwargs = self._key = None\n\n    def still_active(self):\n        '''\n        Return whether this finalizer is still waiting to invoke callback\n        '''\n        return self._key in _finalizer_registry\n\n    def __repr__(self):\n        try:\n            obj = self._weakref()\n        except (AttributeError, TypeError):\n            obj = None\n\n        if obj is None:\n            return '<%s object, dead>' % self.__class__.__name__\n\n        x = '<%s object, callback=%s' % (\n                self.__class__.__name__,\n                getattr(self._callback, '__name__', self._callback))\n        if self._args:\n            x += ', args=' + str(self._args)\n        if self._kwargs:\n            x += ', kwargs=' + str(self._kwargs)\n        if self._key[0] is not None:\n            x += ', exitpriority=' + str(self._key[0])\n        return x + '>'\n\n\ndef _run_finalizers(minpriority=None):\n    '''\n    Run all finalizers whose exit priority is not None and at least minpriority\n\n    Finalizers with highest priority are called first; finalizers with\n    the same priority will be called in reverse order of creation.\n    '''\n    if _finalizer_registry is None:\n        # This function may be called after this module's globals are\n        # destroyed.  See the _exit_function function in this module for more\n        # notes.\n        return\n\n    if minpriority is None:\n        f = lambda p : p[0] is not None\n    else:\n        f = lambda p : p[0] is not None and p[0] >= minpriority\n\n    # Careful: _finalizer_registry may be mutated while this function\n    # is running (either by a GC run or by another thread).\n\n    # list(_finalizer_registry) should be atomic, while\n    # list(_finalizer_registry.items()) is not.\n    keys = [key for key in list(_finalizer_registry) if f(key)]\n    keys.sort(reverse=True)\n\n    for key in keys:\n        finalizer = _finalizer_registry.get(key)\n        # key may have been removed from the registry\n        if finalizer is not None:\n            sub_debug('calling %s', finalizer)\n            try:\n                finalizer()\n            except Exception:\n                import traceback\n                traceback.print_exc()\n\n    if minpriority is None:\n        _finalizer_registry.clear()\n\n#\n# Clean up on exit\n#\n\ndef is_exiting():\n    '''\n    Returns true if the process is shutting down\n    '''\n    return _exiting or _exiting is None\n\n_exiting = False\n\ndef _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n                   active_children=process.active_children,\n                   current_process=process.current_process):\n    # We hold on to references to functions in the arglist due to the\n    # situation described below, where this function is called after this\n    # module's globals are destroyed.\n\n    global _exiting\n\n    if not _exiting:\n        _exiting = True\n\n        info('process shutting down')\n        debug('running all \"atexit\" finalizers with priority >= 0')\n        _run_finalizers(0)\n\n        if current_process() is not None:\n            # We check if the current process is None here because if\n            # it's None, any call to ``active_children()`` will raise\n            # an AttributeError (active_children winds up trying to\n            # get attributes from util._current_process).  One\n            # situation where this can happen is if someone has\n            # manipulated sys.modules, causing this module to be\n            # garbage collected.  The destructor for the module type\n            # then replaces all values in the module dict with None.\n            # For instance, after setuptools runs a test it replaces\n            # sys.modules with a copy created earlier.  See issues\n            # #9775 and #15881.  Also related: #4106, #9205, and\n            # #9207.\n\n            for p in active_children():\n                if p.daemon:\n                    info('calling terminate() for daemon %s', p.name)\n                    p._popen.terminate()\n\n            for p in active_children():\n                info('calling join() for process %s', p.name)\n                p.join()\n\n        debug('running the remaining \"atexit\" finalizers')\n        _run_finalizers()\n\natexit.register(_exit_function)\n\n#\n# Some fork aware types\n#\n\nclass ForkAwareThreadLock(object):\n    def __init__(self):\n        self._lock = threading.Lock()\n        self.acquire = self._lock.acquire\n        self.release = self._lock.release\n        register_after_fork(self, ForkAwareThreadLock._at_fork_reinit)\n\n    def _at_fork_reinit(self):\n        self._lock._at_fork_reinit()\n\n    def __enter__(self):\n        return self._lock.__enter__()\n\n    def __exit__(self, *args):\n        return self._lock.__exit__(*args)\n\n\nclass ForkAwareLocal(threading.local):\n    def __init__(self):\n        register_after_fork(self, lambda obj : obj.__dict__.clear())\n    def __reduce__(self):\n        return type(self), ()\n\n#\n# Close fds except those specified\n#\n\ntry:\n    MAXFD = os.sysconf(\"SC_OPEN_MAX\")\nexcept Exception:\n    MAXFD = 256\n\ndef close_all_fds_except(fds):\n    fds = list(fds) + [-1, MAXFD]\n    fds.sort()\n    assert fds[-1] == MAXFD, 'fd too large'\n    for i in range(len(fds) - 1):\n        os.closerange(fds[i]+1, fds[i+1])\n#\n# Close sys.stdin and replace stdin with os.devnull\n#\n\ndef _close_stdin():\n    if sys.stdin is None:\n        return\n\n    try:\n        sys.stdin.close()\n    except (OSError, ValueError):\n        pass\n\n    try:\n        fd = os.open(os.devnull, os.O_RDONLY)\n        try:\n            sys.stdin = open(fd, encoding=\"utf-8\", closefd=False)\n        except:\n            os.close(fd)\n            raise\n    except (OSError, ValueError):\n        pass\n\n#\n# Flush standard streams, if any\n#\n\ndef _flush_std_streams():\n    try:\n        sys.stdout.flush()\n    except (AttributeError, ValueError):\n        pass\n    try:\n        sys.stderr.flush()\n    except (AttributeError, ValueError):\n        pass\n\n#\n# Start a program with only specified fds kept open\n#\n\ndef spawnv_passfds(path, args, passfds):\n    import _posixsubprocess\n    import subprocess\n    passfds = tuple(sorted(map(int, passfds)))\n    errpipe_read, errpipe_write = os.pipe()\n    try:\n        return _posixsubprocess.fork_exec(\n            args, [path], True, passfds, None, None,\n            -1, -1, -1, -1, -1, -1, errpipe_read, errpipe_write,\n            False, False, -1, None, None, None, -1, None,\n            subprocess._USE_VFORK)\n    finally:\n        os.close(errpipe_read)\n        os.close(errpipe_write)\n\n\ndef close_fds(*fds):\n    \"\"\"Close each file descriptor given as an argument\"\"\"\n    for fd in fds:\n        os.close(fd)\n\n\ndef _cleanup_tests():\n    \"\"\"Cleanup multiprocessing resources when multiprocessing tests\n    completed.\"\"\"\n\n    from test import support\n\n    # cleanup multiprocessing\n    process._cleanup()\n\n    # Stop the ForkServer process if it's running\n    from multiprocessing import forkserver\n    forkserver._forkserver._stop()\n\n    # Stop the ResourceTracker process if it's running\n    from multiprocessing import resource_tracker\n    resource_tracker._resource_tracker._stop()\n\n    # bpo-37421: Explicitly call _run_finalizers() to remove immediately\n    # temporary directories created by multiprocessing.util.get_temp_dir().\n    _run_finalizers()\n    support.gc_collect()\n\n    support.reap_children()\n", 494], "/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/process.py": ["#\n# Module providing the `Process` class which emulates `threading.Thread`\n#\n# multiprocessing/process.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# Licensed to PSF under a Contributor Agreement.\n#\n\n__all__ = ['BaseProcess', 'current_process', 'active_children',\n           'parent_process']\n\n#\n# Imports\n#\n\nimport os\nimport sys\nimport signal\nimport itertools\nimport threading\nfrom _weakrefset import WeakSet\n\n#\n#\n#\n\ntry:\n    ORIGINAL_DIR = os.path.abspath(os.getcwd())\nexcept OSError:\n    ORIGINAL_DIR = None\n\n#\n# Public functions\n#\n\ndef current_process():\n    '''\n    Return process object representing the current process\n    '''\n    return _current_process\n\ndef active_children():\n    '''\n    Return list of process objects corresponding to live child processes\n    '''\n    _cleanup()\n    return list(_children)\n\n\ndef parent_process():\n    '''\n    Return process object representing the parent process\n    '''\n    return _parent_process\n\n#\n#\n#\n\ndef _cleanup():\n    # check for processes which have finished\n    for p in list(_children):\n        if (child_popen := p._popen) and child_popen.poll() is not None:\n            _children.discard(p)\n\n#\n# The `Process` class\n#\n\nclass BaseProcess(object):\n    '''\n    Process objects represent activity that is run in a separate process\n\n    The class is analogous to `threading.Thread`\n    '''\n    def _Popen(self):\n        raise NotImplementedError\n\n    def __init__(self, group=None, target=None, name=None, args=(), kwargs={},\n                 *, daemon=None):\n        assert group is None, 'group argument must be None for now'\n        count = next(_process_counter)\n        self._identity = _current_process._identity + (count,)\n        self._config = _current_process._config.copy()\n        self._parent_pid = os.getpid()\n        self._parent_name = _current_process.name\n        self._popen = None\n        self._closed = False\n        self._target = target\n        self._args = tuple(args)\n        self._kwargs = dict(kwargs)\n        self._name = name or type(self).__name__ + '-' + \\\n                     ':'.join(str(i) for i in self._identity)\n        if daemon is not None:\n            self.daemon = daemon\n        _dangling.add(self)\n\n    def _check_closed(self):\n        if self._closed:\n            raise ValueError(\"process object is closed\")\n\n    def run(self):\n        '''\n        Method to be run in sub-process; can be overridden in sub-class\n        '''\n        if self._target:\n            self._target(*self._args, **self._kwargs)\n\n    def start(self):\n        '''\n        Start child process\n        '''\n        self._check_closed()\n        assert self._popen is None, 'cannot start a process twice'\n        assert self._parent_pid == os.getpid(), \\\n               'can only start a process object created by current process'\n        assert not _current_process._config.get('daemon'), \\\n               'daemonic processes are not allowed to have children'\n        _cleanup()\n        self._popen = self._Popen(self)\n        self._sentinel = self._popen.sentinel\n        # Avoid a refcycle if the target function holds an indirect\n        # reference to the process object (see bpo-30775)\n        del self._target, self._args, self._kwargs\n        _children.add(self)\n\n    def terminate(self):\n        '''\n        Terminate process; sends SIGTERM signal or uses TerminateProcess()\n        '''\n        self._check_closed()\n        self._popen.terminate()\n\n    def kill(self):\n        '''\n        Terminate process; sends SIGKILL signal or uses TerminateProcess()\n        '''\n        self._check_closed()\n        self._popen.kill()\n\n    def join(self, timeout=None):\n        '''\n        Wait until child process terminates\n        '''\n        self._check_closed()\n        assert self._parent_pid == os.getpid(), 'can only join a child process'\n        assert self._popen is not None, 'can only join a started process'\n        res = self._popen.wait(timeout)\n        if res is not None:\n            _children.discard(self)\n\n    def is_alive(self):\n        '''\n        Return whether process is alive\n        '''\n        self._check_closed()\n        if self is _current_process:\n            return True\n        assert self._parent_pid == os.getpid(), 'can only test a child process'\n\n        if self._popen is None:\n            return False\n\n        returncode = self._popen.poll()\n        if returncode is None:\n            return True\n        else:\n            _children.discard(self)\n            return False\n\n    def close(self):\n        '''\n        Close the Process object.\n\n        This method releases resources held by the Process object.  It is\n        an error to call this method if the child process is still running.\n        '''\n        if self._popen is not None:\n            if self._popen.poll() is None:\n                raise ValueError(\"Cannot close a process while it is still running. \"\n                                 \"You should first call join() or terminate().\")\n            self._popen.close()\n            self._popen = None\n            del self._sentinel\n            _children.discard(self)\n        self._closed = True\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert isinstance(name, str), 'name must be a string'\n        self._name = name\n\n    @property\n    def daemon(self):\n        '''\n        Return whether process is a daemon\n        '''\n        return self._config.get('daemon', False)\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        '''\n        Set whether process is a daemon\n        '''\n        assert self._popen is None, 'process has already started'\n        self._config['daemon'] = daemonic\n\n    @property\n    def authkey(self):\n        return self._config['authkey']\n\n    @authkey.setter\n    def authkey(self, authkey):\n        '''\n        Set authorization key of process\n        '''\n        self._config['authkey'] = AuthenticationString(authkey)\n\n    @property\n    def exitcode(self):\n        '''\n        Return exit code of process or `None` if it has yet to stop\n        '''\n        self._check_closed()\n        if self._popen is None:\n            return self._popen\n        return self._popen.poll()\n\n    @property\n    def ident(self):\n        '''\n        Return identifier (PID) of process or `None` if it has yet to start\n        '''\n        self._check_closed()\n        if self is _current_process:\n            return os.getpid()\n        else:\n            return self._popen and self._popen.pid\n\n    pid = ident\n\n    @property\n    def sentinel(self):\n        '''\n        Return a file descriptor (Unix) or handle (Windows) suitable for\n        waiting for process termination.\n        '''\n        self._check_closed()\n        try:\n            return self._sentinel\n        except AttributeError:\n            raise ValueError(\"process not started\") from None\n\n    def __repr__(self):\n        exitcode = None\n        if self is _current_process:\n            status = 'started'\n        elif self._closed:\n            status = 'closed'\n        elif self._parent_pid != os.getpid():\n            status = 'unknown'\n        elif self._popen is None:\n            status = 'initial'\n        else:\n            exitcode = self._popen.poll()\n            if exitcode is not None:\n                status = 'stopped'\n            else:\n                status = 'started'\n\n        info = [type(self).__name__, 'name=%r' % self._name]\n        if self._popen is not None:\n            info.append('pid=%s' % self._popen.pid)\n        info.append('parent=%s' % self._parent_pid)\n        info.append(status)\n        if exitcode is not None:\n            exitcode = _exitcode_to_name.get(exitcode, exitcode)\n            info.append('exitcode=%s' % exitcode)\n        if self.daemon:\n            info.append('daemon')\n        return '<%s>' % ' '.join(info)\n\n    ##\n\n    def _bootstrap(self, parent_sentinel=None):\n        from . import util, context\n        global _current_process, _parent_process, _process_counter, _children\n\n        try:\n            if self._start_method is not None:\n                context._force_start_method(self._start_method)\n            _process_counter = itertools.count(1)\n            _children = set()\n            util._close_stdin()\n            old_process = _current_process\n            _current_process = self\n            _parent_process = _ParentProcess(\n                self._parent_name, self._parent_pid, parent_sentinel)\n            if threading._HAVE_THREAD_NATIVE_ID:\n                threading.main_thread()._set_native_id()\n            try:\n                self._after_fork()\n            finally:\n                # delay finalization of the old process object until after\n                # _run_after_forkers() is executed\n                del old_process\n            util.info('child process calling self.run()')\n            try:\n                self.run()\n                exitcode = 0\n            finally:\n                util._exit_function()\n        except SystemExit as e:\n            if e.code is None:\n                exitcode = 0\n            elif isinstance(e.code, int):\n                exitcode = e.code\n            else:\n                sys.stderr.write(str(e.code) + '\\n')\n                exitcode = 1\n        except:\n            exitcode = 1\n            import traceback\n            sys.stderr.write('Process %s:\\n' % self.name)\n            traceback.print_exc()\n        finally:\n            threading._shutdown()\n            util.info('process exiting with exitcode %d' % exitcode)\n            util._flush_std_streams()\n\n        return exitcode\n\n    @staticmethod\n    def _after_fork():\n        from . import util\n        util._finalizer_registry.clear()\n        util._run_after_forkers()\n\n\n#\n# We subclass bytes to avoid accidental transmission of auth keys over network\n#\n\nclass AuthenticationString(bytes):\n    def __reduce__(self):\n        from .context import get_spawning_popen\n        if get_spawning_popen() is None:\n            raise TypeError(\n                'Pickling an AuthenticationString object is '\n                'disallowed for security reasons'\n                )\n        return AuthenticationString, (bytes(self),)\n\n\n#\n# Create object representing the parent process\n#\n\nclass _ParentProcess(BaseProcess):\n\n    def __init__(self, name, pid, sentinel):\n        self._identity = ()\n        self._name = name\n        self._pid = pid\n        self._parent_pid = None\n        self._popen = None\n        self._closed = False\n        self._sentinel = sentinel\n        self._config = {}\n\n    def is_alive(self):\n        from multiprocessing.connection import wait\n        return not wait([self._sentinel], timeout=0)\n\n    @property\n    def ident(self):\n        return self._pid\n\n    def join(self, timeout=None):\n        '''\n        Wait until parent process terminates\n        '''\n        from multiprocessing.connection import wait\n        wait([self._sentinel], timeout=timeout)\n\n    pid = ident\n\n#\n# Create object representing the main process\n#\n\nclass _MainProcess(BaseProcess):\n\n    def __init__(self):\n        self._identity = ()\n        self._name = 'MainProcess'\n        self._parent_pid = None\n        self._popen = None\n        self._closed = False\n        self._config = {'authkey': AuthenticationString(os.urandom(32)),\n                        'semprefix': '/mp'}\n        # Note that some versions of FreeBSD only allow named\n        # semaphores to have names of up to 14 characters.  Therefore\n        # we choose a short prefix.\n        #\n        # On MacOSX in a sandbox it may be necessary to use a\n        # different prefix -- see #19478.\n        #\n        # Everything in self._config will be inherited by descendant\n        # processes.\n\n    def close(self):\n        pass\n\n\n_parent_process = None\n_current_process = _MainProcess()\n_process_counter = itertools.count(1)\n_children = set()\ndel _MainProcess\n\n#\n# Give names to some return codes\n#\n\n_exitcode_to_name = {}\n\nfor name, signum in list(signal.__dict__.items()):\n    if name[:3]=='SIG' and '_' not in name:\n        _exitcode_to_name[-signum] = f'-{name}'\ndel name, signum\n\n# For debug and leak testing\n_dangling = WeakSet()\n", 439]}, "functions": {"finalize.detach (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:592)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py", 592], "samestat (<frozen genericpath>:99)": ["<frozen genericpath>", 99], "_get_sep (<frozen posixpath>:41)": ["<frozen posixpath>", 41], "join (<frozen posixpath>:71)": ["<frozen posixpath>", 71], "_rmtree_safe_fd (/root/miniconda3/envs/gs-lightning/lib/python3.12/shutil.py:642)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/shutil.py", 642], "rmtree (/root/miniconda3/envs/gs-lightning/lib/python3.12/shutil.py:710)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/shutil.py", 710], "TemporaryDirectory._rmtree (/root/miniconda3/envs/gs-lightning/lib/python3.12/tempfile.py:894)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/tempfile.py", 894], "TemporaryDirectory.cleanup (/root/miniconda3/envs/gs-lightning/lib/python3.12/tempfile.py:952)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/tempfile.py", 952], "indent.<locals>.prefixed_lines (/root/miniconda3/envs/gs-lightning/lib/python3.12/textwrap.py:482)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/textwrap.py", 482], "indent (/root/miniconda3/envs/gs-lightning/lib/python3.12/textwrap.py:470)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/textwrap.py", 470], "_acquireLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:234)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py", 234], "Manager.disable (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1369)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py", 1369], "Logger.getEffectiveLevel (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1776)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py", 1776], "_releaseLock (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:243)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py", 243], "Logger.isEnabledFor (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1790)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py", 1790], "Logger.info (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1529)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py", 1529], "_log_traced_frames (/data/wangyingqi/code/pytorch/torch/_dynamo/eval_frame.py:551)": ["/data/wangyingqi/code/pytorch/torch/_dynamo/eval_frame.py", 551], "_ModuleLockManager.__init__ (<frozen importlib._bootstrap>:412)": ["<frozen importlib._bootstrap>", 412], "_ModuleLock.__init__ (<frozen importlib._bootstrap>:232)": ["<frozen importlib._bootstrap>", 232], "_get_module_lock (<frozen importlib._bootstrap>:426)": ["<frozen importlib._bootstrap>", 426], "_BlockingOnManager.__init__ (<frozen importlib._bootstrap>:158)": ["<frozen importlib._bootstrap>", 158], "_WeakValueDictionary.__init__.<locals>.KeyedRef.__new__ (<frozen importlib._bootstrap>:74)": ["<frozen importlib._bootstrap>", 74], "_WeakValueDictionary.__init__.<locals>.KeyedRef.__init__ (<frozen importlib._bootstrap>:79)": ["<frozen importlib._bootstrap>", 79], "_WeakValueDictionary.setdefault (<frozen importlib._bootstrap>:124)": ["<frozen importlib._bootstrap>", 124], "_BlockingOnManager.__enter__ (<frozen importlib._bootstrap>:162)": ["<frozen importlib._bootstrap>", 162], "_BlockingOnManager.__exit__ (<frozen importlib._bootstrap>:173)": ["<frozen importlib._bootstrap>", 173], "_WeakValueDictionary.__init__.<locals>.KeyedRef.remove (<frozen importlib._bootstrap>:82)": ["<frozen importlib._bootstrap>", 82], "_ModuleLock.acquire (<frozen importlib._bootstrap>:304)": ["<frozen importlib._bootstrap>", 304], "_ModuleLockManager.__enter__ (<frozen importlib._bootstrap>:416)": ["<frozen importlib._bootstrap>", 416], "_ImportLockContext.__enter__ (<frozen importlib._bootstrap>:1222)": ["<frozen importlib._bootstrap>", 1222], "DistutilsMetaFinder.find_spec.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/site-packages/_distutils_hack/__init__.py:108)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/site-packages/_distutils_hack/__init__.py", 108], "DistutilsMetaFinder.find_spec (/root/miniconda3/envs/gs-lightning/lib/python3.12/site-packages/_distutils_hack/__init__.py:101)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/site-packages/_distutils_hack/__init__.py", 101], "_ImportLockContext.__exit__ (<frozen importlib._bootstrap>:1226)": ["<frozen importlib._bootstrap>", 1226], "BuiltinImporter.find_spec (<frozen importlib._bootstrap>:982)": ["<frozen importlib._bootstrap>", 982], "_call_with_frames_removed (<frozen importlib._bootstrap>:480)": ["<frozen importlib._bootstrap>", 480], "FrozenImporter.find_spec (<frozen importlib._bootstrap>:1128)": ["<frozen importlib._bootstrap>", 1128], "PathFinder._path_importer_cache (<frozen importlib._bootstrap_external>:1473)": ["<frozen importlib._bootstrap_external>", 1473], "_path_stat (<frozen importlib._bootstrap_external>:140)": ["<frozen importlib._bootstrap_external>", 140], "_make_relax_case.<locals>._relax_case (<frozen importlib._bootstrap_external>:71)": ["<frozen importlib._bootstrap_external>", 71], "_path_join (<frozen importlib._bootstrap_external>:126)": ["<frozen importlib._bootstrap_external>", 126], "_verbose_message (<frozen importlib._bootstrap>:491)": ["<frozen importlib._bootstrap>", 491], "FileFinder.find_spec (<frozen importlib._bootstrap_external>:1597)": ["<frozen importlib._bootstrap_external>", 1597], "FileFinder._fill_cache (<frozen importlib._bootstrap_external>:1648)": ["<frozen importlib._bootstrap_external>", 1648], "ModuleSpec.__init__ (<frozen importlib._bootstrap>:599)": ["<frozen importlib._bootstrap>", 599], "PathFinder._get_spec (<frozen importlib._bootstrap_external>:1495)": ["<frozen importlib._bootstrap_external>", 1495], "PathFinder.find_spec (<frozen importlib._bootstrap_external>:1524)": ["<frozen importlib._bootstrap_external>", 1524], "_EditableFinder.find_spec (/root/miniconda3/envs/gs-lightning/lib/python3.12/site-packages/__editable___torch_2_9_0a0_git7cc5d03_finder.py:15)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/site-packages/__editable___torch_2_9_0a0_git7cc5d03_finder.py", 15], "_find_spec (<frozen importlib._bootstrap>:1240)": ["<frozen importlib._bootstrap>", 1240], "_find_and_load_unlocked (<frozen importlib._bootstrap>:1304)": ["<frozen importlib._bootstrap>", 1304], "_ModuleLock.release (<frozen importlib._bootstrap>:372)": ["<frozen importlib._bootstrap>", 372], "_ModuleLockManager.__exit__ (<frozen importlib._bootstrap>:420)": ["<frozen importlib._bootstrap>", 420], "_get_module_lock.<locals>.cb (<frozen importlib._bootstrap>:445)": ["<frozen importlib._bootstrap>", 445], "_find_and_load (<frozen importlib._bootstrap>:1349)": ["<frozen importlib._bootstrap>", 1349], "tabulate.<locals>.<genexpr> (/data/wangyingqi/code/pytorch/torch/_dynamo/utils.py:230)": ["/data/wangyingqi/code/pytorch/torch/_dynamo/utils.py", 230], "tabulate (/data/wangyingqi/code/pytorch/torch/_dynamo/utils.py:221)": ["/data/wangyingqi/code/pytorch/torch/_dynamo/utils.py", 221], "compile_times (/data/wangyingqi/code/pytorch/torch/_dynamo/utils.py:793)": ["/data/wangyingqi/code/pytorch/torch/_dynamo/utils.py", 793], "dump_compile_times (/data/wangyingqi/code/pytorch/torch/_dynamo/utils.py:830)": ["/data/wangyingqi/code/pytorch/torch/_dynamo/utils.py", 830], "finalize._select_for_exit.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:638)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py", 638], "finalize._select_for_exit (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:634)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py", 634], "FakeImplHolder.register.<locals>.deregister_fake_kernel (/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py:78)": ["/data/wangyingqi/code/pytorch/torch/_library/fake_impl.py", 78], "RegistrationHandle.destroy (/data/wangyingqi/code/pytorch/torch/_library/utils.py:31)": ["/data/wangyingqi/code/pytorch/torch/_library/utils.py", 31], "_del_library (/data/wangyingqi/code/pytorch/torch/library.py:435)": ["/data/wangyingqi/code/pytorch/torch/library.py", 435], "finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:585)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py", 585], "finalize._exitfunc (/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py:641)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/weakref.py", 641], "dump_cache_stats (/data/wangyingqi/code/pytorch/torch/_subclasses/fake_tensor.py:3178)": ["/data/wangyingqi/code/pytorch/torch/_subclasses/fake_tensor.py", 3178], "_set_python_exit_flag (/data/wangyingqi/code/pytorch/torch/utils/data/_utils/__init__.py:45)": ["/data/wangyingqi/code/pytorch/torch/utils/data/_utils/__init__.py", 45], "Handler.acquire (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:968)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py", 968], "Handler.release (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:975)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py", 975], "StreamHandler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1137)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py", 1137], "Handler.close (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1048)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py", 1048], "LazyTraceHandler.close (/data/wangyingqi/code/pytorch/torch/_logging/_internal.py:1077)": ["/data/wangyingqi/code/pytorch/torch/_logging/_internal.py", 1077], "Handler.flush (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1039)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py", 1039], "_StderrHandler.stream (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:1299)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py", 1299], "shutdown (/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py:2245)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/logging/__init__.py", 2245], "info (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:52)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py", 52], "debug (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:48)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py", 48], "_run_finalizers.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:287)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py", 287], "_run_finalizers (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:271)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py", 271], "current_process (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/process.py:37)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/process.py", 37], "_cleanup (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/process.py:61)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/process.py", 61], "active_children (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/process.py:43)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/process.py", 43], "_run_finalizers.<locals>.<lambda> (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:285)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py", 285], "sub_debug (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:44)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py", 44], "Finalize.__call__ (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:208)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py", 208], "_exit_function (/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py:323)": ["/root/miniconda3/envs/gs-lightning/lib/python3.12/multiprocessing/util.py", 323]}}}